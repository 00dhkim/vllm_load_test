Attaching to vllm_load_test_llm_server_1
[36mllm_server_1  |[0m INFO 07-21 18:40:53 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:40:57 [api_server.py:1395] vLLM API server version 0.9.2
[36mllm_server_1  |[0m INFO 07-21 18:40:57 [cli_args.py:325] non-default args: {'model': '/llm', 'max_model_len': 32768, 'served_model_name': ['qwen3'], 'data_parallel_size': 4}
[36mllm_server_1  |[0m INFO 07-21 18:41:04 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
[36mllm_server_1  |[0m INFO 07-21 18:41:04 [config.py:1472] Using max model len 32768
[36mllm_server_1  |[0m INFO 07-21 18:41:04 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36mllm_server_1  |[0m INFO 07-21 18:41:05 [utils.py:364] Started DP Coordinator process (PID: 203)
[36mllm_server_1  |[0m INFO 07-21 18:41:10 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:41:10 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:41:10 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:41:10 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:41:10 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:12 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:12 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:12 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:12 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:13 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:13 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:13 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:13 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:13 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:13 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:13 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:13 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:16 [parallel_state.py:935] Adjusting world_size=4 rank=2 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:16 [parallel_state.py:935] Adjusting world_size=4 rank=3 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:16 [parallel_state.py:935] Adjusting world_size=4 rank=0 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:16 [parallel_state.py:935] Adjusting world_size=4 rank=1 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:16 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:16 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:16 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:16 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:16 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:16 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:16 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:16 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:17 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:17 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:17 [parallel_state.py:1076] rank 3 in world size 4 is assigned as DP rank 3, PP rank 0, TP rank 0, EP rank 3
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:17 [parallel_state.py:1076] rank 1 in world size 4 is assigned as DP rank 1, PP rank 0, TP rank 0, EP rank 1
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:17 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:17 [parallel_state.py:1076] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:17 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:17 [parallel_state.py:1076] rank 2 in world size 4 is assigned as DP rank 2, PP rank 0, TP rank 0, EP rank 2
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:17 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:17 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:17 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:17 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:17 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:17 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:17 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:17 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:17 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:17 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:17 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:17 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:17 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:17 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:17 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:17 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:06,  1.02it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:02<00:06,  1.02s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:03<00:05,  1.05s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.16it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.04it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:05<00:02,  1.03s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:07<00:01,  1.07s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:25 [default_loader.py:272] Loading weights took 7.72 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:26 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 7.986110 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:26 [default_loader.py:272] Loading weights took 8.25 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:08<00:00,  1.09s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:08<00:00,  1.03s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m 
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:26 [default_loader.py:272] Loading weights took 8.33 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:26 [default_loader.py:272] Loading weights took 8.34 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:26 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 8.519463 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:26 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 8.578622 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:26 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 8.606891 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:39 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_3/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:39 [backends.py:519] Dynamo bytecode transform time: 11.76 s
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:39 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_0/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:39 [backends.py:519] Dynamo bytecode transform time: 11.77 s
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:39 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_1/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:39 [backends.py:519] Dynamo bytecode transform time: 11.93 s
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:39 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_2/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:39 [backends.py:519] Dynamo bytecode transform time: 12.03 s
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:41:44 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:41:45 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:41:45 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:41:45 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:42:29 [backends.py:193] Compiling a graph for general shape takes 50.08 s
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:42:29 [backends.py:193] Compiling a graph for general shape takes 50.23 s
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:42:29 [backends.py:193] Compiling a graph for general shape takes 50.12 s
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:42:31 [backends.py:193] Compiling a graph for general shape takes 51.86 s
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:42:52 [monitor.py:34] torch.compile takes 62.00 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:42:53 [monitor.py:34] torch.compile takes 61.84 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:42:53 [monitor.py:34] torch.compile takes 62.15 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:42:53 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:42:54 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:42:54 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:42:54 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:42:54 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:42:55 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:42:55 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:42:55 [monitor.py:34] torch.compile takes 63.80 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:42:55 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:42:55 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_1 pid=207)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:42:56 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:42:56 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:42:56 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:43:36 [gpu_model_runner.py:2326] Graph capturing finished in 41 secs, took 0.71 GiB
[36mllm_server_1  |[0m Capturing CUDA graph shapes:   1%|▏         | 1/67 [00:02<03:15,  2.96s/it]Capturing CUDA graph shapes:   3%|▎         | 2/67 [00:03<01:47,  1.65s/it]Capturing CUDA graph shapes:   4%|▍         | 3/67 [00:04<01:14,  1.17s/it]Capturing CUDA graph shapes:   6%|▌         | 4/67 [00:04<00:59,  1.06it/s]Capturing CUDA graph shapes:   7%|▋         | 5/67 [00:05<00:50,  1.22it/s]Capturing CUDA graph shapes:   9%|▉         | 6/67 [00:06<00:45,  1.35it/s]Capturing CUDA graph shapes:  10%|█         | 7/67 [00:06<00:41,  1.43it/s]Capturing CUDA graph shapes:  12%|█▏        | 8/67 [00:07<00:39,  1.48it/s]Capturing CUDA graph shapes:  13%|█▎        | 9/67 [00:08<00:39,  1.46it/s]Capturing CUDA graph shapes:  15%|█▍        | 10/67 [00:08<00:38,  1.47it/s]Capturing CUDA graph shapes:  16%|█▋        | 11/67 [00:09<00:36,  1.55it/s]Capturing CUDA graph shapes:  18%|█▊        | 12/67 [00:09<00:34,  1.58it/s]Capturing CUDA graph shapes:  19%|█▉        | 13/67 [00:10<00:33,  1.60it/s]Capturing CUDA graph shapes:  21%|██        | 14/67 [00:11<00:32,  1.61it/s]Capturing CUDA graph shapes:  22%|██▏       | 15/67 [00:11<00:33,  1.54it/s]Capturing CUDA graph shapes:  24%|██▍       | 16/67 [00:12<00:32,  1.56it/s]Capturing CUDA graph shapes:  25%|██▌       | 17/67 [00:13<00:33,  1.47it/s]Capturing CUDA graph shapes:  27%|██▋       | 18/67 [00:13<00:32,  1.51it/s]Capturing CUDA graph shapes:  28%|██▊       | 19/67 [00:14<00:34,  1.40it/s]Capturing CUDA graph shapes:  30%|██▉       | 20/67 [00:15<00:32,  1.44it/s]Capturing CUDA graph shapes:  31%|███▏      | 21/67 [00:15<00:30,  1.50it/s]Capturing CUDA graph shapes:  33%|███▎      | 22/67 [00:16<00:29,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 23/67 [00:17<00:27,  1.61it/s]Capturing CUDA graph shapes:  36%|███▌      | 24/67 [00:17<00:26,  1.64it/s]Capturing CUDA graph shapes:  37%|███▋      | 25/67 [00:18<00:25,  1.66it/s]Capturing CUDA graph shapes:  39%|███▉      | 26/67 [00:18<00:24,  1.67it/s]Capturing CUDA graph shapes:  40%|████      | 27/67 [00:19<00:23,  1.72it/s]Capturing CUDA graph shapes:  42%|████▏     | 28/67 [00:19<00:22,  1.72it/s]Capturing CUDA graph shapes:  43%|████▎     | 29/67 [00:20<00:21,  1.73it/s]Capturing CUDA graph shapes:  45%|████▍     | 30/67 [00:21<00:21,  1.74it/s]Capturing CUDA graph shapes:  46%|████▋     | 31/67 [00:21<00:20,  1.73it/s]Capturing CUDA graph shapes:  48%|████▊     | 32/67 [00:22<00:20,  1.70it/s]Capturing CUDA graph shapes:  49%|████▉     | 33/67 [00:22<00:19,  1.74it/s]Capturing CUDA graph shapes:  51%|█████     | 34/67 [00:23<00:19,  1.72it/s]Capturing CUDA graph shapes:  52%|█████▏    | 35/67 [00:23<00:18,  1.72it/s]Capturing CUDA graph shapes:  54%|█████▎    | 36/67 [00:24<00:17,  1.74it/s]Capturing CUDA graph shapes:  55%|█████▌    | 37/67 [00:25<00:16,  1.78it/s]Capturing CUDA graph shapes:  57%|█████▋    | 38/67 [00:25<00:16,  1.78it/s]Capturing CUDA graph shapes:  58%|█████▊    | 39/67 [00:26<00:15,  1.80it/s]Capturing CUDA graph shapes:  60%|█████▉    | 40/67 [00:26<00:15,  1.79it/s]Capturing CUDA graph shapes:  61%|██████    | 41/67 [00:27<00:14,  1.82it/s]Capturing CUDA graph shapes:  63%|██████▎   | 42/67 [00:27<00:14,  1.78it/s]Capturing CUDA graph shapes:  64%|██████▍   | 43/67 [00:28<00:13,  1.81it/s]Capturing CUDA graph shapes:  66%|██████▌   | 44/67 [00:28<00:12,  1.80it/s]Capturing CUDA graph shapes:  67%|██████▋   | 45/67 [00:29<00:12,  1.76it/s]Capturing CUDA graph shapes:  69%|██████▊   | 46/67 [00:30<00:11,  1.78it/s]Capturing CUDA graph shapes:  70%|███████   | 47/67 [00:30<00:11,  1.80it/s]Capturing CUDA graph shapes:  72%|███████▏  | 48/67 [00:31<00:10,  1.79it/s]Capturing CUDA graph shapes:  73%|███████▎  | 49/67 [00:31<00:10,  1.76it/s]Capturing CUDA graph shapes:  75%|███████▍  | 50/67 [00:32<00:09,  1.76it/s]Capturing CUDA graph shapes:  76%|███████▌  | 51/67 [00:32<00:08,  1.81it/s]Capturing CUDA graph shapes:  78%|███████▊  | 52/67 [00:33<00:08,  1.82it/s]Capturing CUDA graph shapes:  79%|███████▉  | 53/67 [00:33<00:07,  1.83it/s]Capturing CUDA graph shapes:  81%|████████  | 54/67 [00:34<00:07,  1.84it/s]Capturing CUDA graph shapes:  82%|████████▏ | 55/67 [00:35<00:06,  1.88it/s]Capturing CUDA graph shapes:  84%|████████▎ | 56/67 [00:35<00:05,  1.89it/s]Capturing CUDA graph shapes:  85%|████████▌ | 57/67 [00:36<00:05,  1.88it/s]Capturing CUDA graph shapes:  87%|████████▋ | 58/67 [00:36<00:04,  1.85it/s]Capturing CUDA graph shapes:  88%|████████▊ | 59/67 [00:37<00:04,  1.77it/s]Capturing CUDA graph shapes:  90%|████████▉ | 60/67 [00:37<00:03,  1.78it/s]Capturing CUDA graph shapes:  91%|█████████ | 61/67 [00:38<00:03,  1.80it/s]Capturing CUDA graph shapes:  93%|█████████▎| 62/67 [00:38<00:02,  1.78it/s]Capturing CUDA graph shapes:  94%|█████████▍| 63/67 [00:39<00:02,  1.79it/s]Capturing CUDA graph shapes:  96%|█████████▌| 64/67 [00:40<00:01,  1.82it/s]Capturing CUDA graph shapes:  97%|█████████▋| 65/67 [00:40<00:01,  1.85it/s]Capturing CUDA graph shapes:  99%|█████████▊| 66/67 [00:41<00:00,  1.84it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:41<00:00,  1.82it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:41<00:00,  1.61it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:43:36 [gpu_model_runner.py:2326] Graph capturing finished in 42 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:43:36 [gpu_model_runner.py:2326] Graph capturing finished in 39 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:43:36 [gpu_model_runner.py:2326] Graph capturing finished in 41 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:43:36 [core.py:172] init engine (profile, create kv cache, warmup model) took 129.13 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:43:36 [core.py:172] init engine (profile, create kv cache, warmup model) took 129.17 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:43:36 [core.py:172] init engine (profile, create kv cache, warmup model) took 129.27 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:43:36 [core.py:172] init engine (profile, create kv cache, warmup model) took 129.78 seconds
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 11764
[36mllm_server_1  |[0m WARNING 07-21 18:43:36 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:29] Available routes are:
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /docs, Methods: GET, HEAD
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /redoc, Methods: GET, HEAD
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /health, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /load, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /ping, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /ping, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /tokenize, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /detokenize, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /v1/models, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /version, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /v1/completions, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /v1/embeddings, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /pooling, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /classify, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /score, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /v1/score, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /v1/audio/translations, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /v1/rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /v2/rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /invocations, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:43:36 [launcher.py:37] Route: /metrics, Methods: GET
[36mllm_server_1  |[0m INFO:     Started server process [7]
[36mllm_server_1  |[0m INFO:     Waiting for application startup.
[36mllm_server_1  |[0m INFO:     Application startup complete.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49420 - "GET /health HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:52 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36mllm_server_1  |[0m INFO 07-21 18:43:52 [logger.py:43] Received request chatcmpl-727396158c104d9dac44ba1ee95faf1b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:52 [async_llm.py:270] Added request chatcmpl-727396158c104d9dac44ba1ee95faf1b.
[36mllm_server_1  |[0m INFO 07-21 18:43:57 [loggers.py:118] Engine 000: Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 17.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-6009fe2f525d421b975c848c3a48e0f7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-6009fe2f525d421b975c848c3a48e0f7.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-9a76a3212e6c4f68b79ed6bd2f544a87: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-9a76a3212e6c4f68b79ed6bd2f544a87.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-6b5fe6d2c2a04240b4d6711afad55e9d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-6b5fe6d2c2a04240b4d6711afad55e9d.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-fb61d63c25f54c6e8e6c30b157dcdb0c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-84c32cc3e0554d379d8820b57769b4c5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-fb61d63c25f54c6e8e6c30b157dcdb0c.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-a4458983c59e4ad49857004f927f3be4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-84c32cc3e0554d379d8820b57769b4c5.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-a4458983c59e4ad49857004f927f3be4.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-dd95dcc1c88e4d74b7d1ce39e3010303: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-dd95dcc1c88e4d74b7d1ce39e3010303.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-06eb501e1f574e5aa2dca6b2038c3503: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-06eb501e1f574e5aa2dca6b2038c3503.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-dee5719406f046a5996ea0b57297fe2f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-dee5719406f046a5996ea0b57297fe2f.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-0f60a0b955e545d0a2e93d48062daee1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-0f60a0b955e545d0a2e93d48062daee1.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-2709586f34854a699c0a2a5459b11c04: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-2709586f34854a699c0a2a5459b11c04.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-8bc8f16f701c4212902e779fbda33875: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-8bc8f16f701c4212902e779fbda33875.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-9b12308fe5574c3c8949b672972edd64: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-9b12308fe5574c3c8949b672972edd64.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-51b58a59142840c2812dcbf9e099d4e9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-51b58a59142840c2812dcbf9e099d4e9.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-a2a74ba2272d4a37ad5289ccd71d41b6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-a2a74ba2272d4a37ad5289ccd71d41b6.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-dbe00dd322f94f39a2e2d986fe8a3802: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-c41bf02823db486f807784208d19f346: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-dbe00dd322f94f39a2e2d986fe8a3802.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-c41bf02823db486f807784208d19f346.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-16618abc74d94c939ea513769bfaa747: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-16618abc74d94c939ea513769bfaa747.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-c5317e59743c4ebd9fd9d06e9f9d23c6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-c5317e59743c4ebd9fd9d06e9f9d23c6.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-d9e408667cf540429ac4bb912e5fe3a6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-d9e408667cf540429ac4bb912e5fe3a6.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-3a9b234ccb0f43299e80d80dea61f3c4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-3a9b234ccb0f43299e80d80dea61f3c4.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-c00a8065ac1d4bfaa4bc6788565c0dbf: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-c00a8065ac1d4bfaa4bc6788565c0dbf.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-4f14aa04fa1e4ef08624dec84cb3d76e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-4f14aa04fa1e4ef08624dec84cb3d76e.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-226f6d26673844879ec47d849180edd7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-226f6d26673844879ec47d849180edd7.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-98c7c4c713f9447b83191a02be2ad329: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-98c7c4c713f9447b83191a02be2ad329.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-52a3e47f5e9b425694d4dc81c234c727: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-52a3e47f5e9b425694d4dc81c234c727.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-14f3e020cedf4f4c81d06ee6e7862e2c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35632 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-14f3e020cedf4f4c81d06ee6e7862e2c.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-e4e6e348a7a043e1a03a42ae52b92b35: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-e4e6e348a7a043e1a03a42ae52b92b35.
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [logger.py:43] Received request chatcmpl-321efa58b97a41cebb9bf6a135dd21f3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:43:59 [async_llm.py:270] Added request chatcmpl-321efa58b97a41cebb9bf6a135dd21f3.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-2945bc058c384688a9ebefc5a73948c4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-2945bc058c384688a9ebefc5a73948c4.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-15792b0b5ec24da59913e21185f37b44: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-15792b0b5ec24da59913e21185f37b44.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-e6ec0e15c3c94535805eabe58a72a1b1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-e6ec0e15c3c94535805eabe58a72a1b1.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-dd412bdd0ba74f189266e794ebd30a47: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-dd412bdd0ba74f189266e794ebd30a47.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-93f0a3cf3e5b4a08940de5d48a531da7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-93f0a3cf3e5b4a08940de5d48a531da7.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-98716cd5834a40228113b6142bc8e73d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-f48d938c4eba4455a06735e6d17f6482: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-98716cd5834a40228113b6142bc8e73d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-f48d938c4eba4455a06735e6d17f6482.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-dd3ebfc139b84faab0f05d803b607c31: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-dd3ebfc139b84faab0f05d803b607c31.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-078bdb11f09e4487b9acc54290b6f1f7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-078bdb11f09e4487b9acc54290b6f1f7.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-2be2f513789b4ba89eaf757588cf59ce: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-2be2f513789b4ba89eaf757588cf59ce.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-5f9deb3dc83547ff88752677e53d1ca8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-5f9deb3dc83547ff88752677e53d1ca8.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-f596a7450e2c46fd9a213597d07dcfd4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-f596a7450e2c46fd9a213597d07dcfd4.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-2d871b41fddf4ea483a2a1a855fc5df6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-e26d14ad37dc42d3aa7b8abf8a5ce8c9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-2d871b41fddf4ea483a2a1a855fc5df6.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-658edb77359d486a8674eb38a3b37bf6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-5e5562af4421410c8f252e6c13aabbab: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-e26d14ad37dc42d3aa7b8abf8a5ce8c9.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-658edb77359d486a8674eb38a3b37bf6.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-5e5562af4421410c8f252e6c13aabbab.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-db2b0fd556384829851c5b074a9c5f5a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-db2b0fd556384829851c5b074a9c5f5a.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-5dde6d2f8d05437f83a21f7b274403a7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-5dde6d2f8d05437f83a21f7b274403a7.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-bb3881abf2eb4f4480466397b57277d6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-bb3881abf2eb4f4480466397b57277d6.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-a65ea8d5dd084ffc90378ce1b9739e35: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-a65ea8d5dd084ffc90378ce1b9739e35.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-e14ac31c47d542c0b2cd60464fc2f84b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-e14ac31c47d542c0b2cd60464fc2f84b.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-b25facb596f844f78f3c2198b0a67cbd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-b25facb596f844f78f3c2198b0a67cbd.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-4e30bdeed8314ad482b0933bddefbd3a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-4e30bdeed8314ad482b0933bddefbd3a.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-4fe1669e5b2944b0bd71c636ed81c95d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-4fe1669e5b2944b0bd71c636ed81c95d.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-d96876e1999c469fb10114dcc54fe344: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-63c27779f7444b26ad8d0d9fdb1cfb0b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-d96876e1999c469fb10114dcc54fe344.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-04d0e88218e848d18e2f36e25e47de8e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-d89e1574c52045b9a36fe61be8799986: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-63c27779f7444b26ad8d0d9fdb1cfb0b.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-04d0e88218e848d18e2f36e25e47de8e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-d89e1574c52045b9a36fe61be8799986.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-cd9caca227004957a64470e5941f0311: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-64b4e342e5f94d8e933ad89dbb6cc513: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35908 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-cd9caca227004957a64470e5941f0311.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-64b4e342e5f94d8e933ad89dbb6cc513.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-fc5e86790e7e4a6d88ebf8f28b83d9b9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-fc5e86790e7e4a6d88ebf8f28b83d9b9.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-ff9bf40e04ff4478872fd863dc8b3b2b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-ff9bf40e04ff4478872fd863dc8b3b2b.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-74e9b0ff7fc241db8d937d448f2d3fb0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-6dbecbb92e8f471e9d54f91d6821ef7e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-74e9b0ff7fc241db8d937d448f2d3fb0.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-6dbecbb92e8f471e9d54f91d6821ef7e.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-2f72c520b2fe47a3a19477ad2a371ba1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-2f72c520b2fe47a3a19477ad2a371ba1.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-9c090d161b254bab96b1e6313e542281: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-9c090d161b254bab96b1e6313e542281.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-d4347ea6545b432faa784d9924c7f30f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35980 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-d4347ea6545b432faa784d9924c7f30f.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-11a0987a1c72430c81c9dcc63f5b1ab3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-11a0987a1c72430c81c9dcc63f5b1ab3.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-e501507f016240749f86f707bf8d9d8d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-a32b5a2366124c97a255516e16867d54: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-bc62d70d69c049d6bfd235c711846185: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36004 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-e501507f016240749f86f707bf8d9d8d.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-afebda950e354c2d8de3ea94825e735b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-a32b5a2366124c97a255516e16867d54.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-bc62d70d69c049d6bfd235c711846185.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-afebda950e354c2d8de3ea94825e735b.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-78ddb227e0de4ee4834584352dc18e3f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-78ddb227e0de4ee4834584352dc18e3f.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-32a4a50a331c42cbac6d662eba188687: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-32a4a50a331c42cbac6d662eba188687.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-2947c7b013b44e8bb38b6e7a15b1b1b3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-2947c7b013b44e8bb38b6e7a15b1b1b3.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-74f151aa52b840f693c8b16b05eddc98: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36074 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-74f151aa52b840f693c8b16b05eddc98.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-95be7e27c33d4c3e90c6ed5fe8d2b456: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-4979bb242bb84fc596760f7fc1c9bcc0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-95be7e27c33d4c3e90c6ed5fe8d2b456.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-4979bb242bb84fc596760f7fc1c9bcc0.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-bd579d3216c145fd8d0075033f139ad0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-639a8742013f4ba6a2c45b5d1b3dbc03: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-bd579d3216c145fd8d0075033f139ad0.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-639a8742013f4ba6a2c45b5d1b3dbc03.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-b2e06fc9ffb4472980e91ce5b0695f52: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-b2e06fc9ffb4472980e91ce5b0695f52.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-5bdbd1c2279f491ea7ae59943bbc11e6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-85a09896256145cb8eb4a35186bbf02d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-5bdbd1c2279f491ea7ae59943bbc11e6.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36108 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-85a09896256145cb8eb4a35186bbf02d.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-726eb5c641034024853f8660b9139e7e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-a07949df7fd64ce99bca1013d5b3d42d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-726eb5c641034024853f8660b9139e7e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-a07949df7fd64ce99bca1013d5b3d42d.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-cb4c0e0758e741b585af5a8605d536c9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-b594426090a847be8fdb7e267924c507: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-cb4c0e0758e741b585af5a8605d536c9.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-8f0245d26e8045d1a70b106b3c1b03bb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-b594426090a847be8fdb7e267924c507.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-8f0245d26e8045d1a70b106b3c1b03bb.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-167c7c6d19434456b45ddc60547e22cd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-d1705799561a481eba632afb214907d3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-167c7c6d19434456b45ddc60547e22cd.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-d1705799561a481eba632afb214907d3.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-beb9eabc7926467da3357a0f73ec70a0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-beb9eabc7926467da3357a0f73ec70a0.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-38ee0ea6c39a40548ad494c4c502cf8d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-38ee0ea6c39a40548ad494c4c502cf8d.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-b966cc1e98ed430bb2c7b3b2973573f4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-b966cc1e98ed430bb2c7b3b2973573f4.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-c5c1fb1daf7a4302be5064fec3439388: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-60baf8fb7db845ce8408dd815cf1b7ac: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-c5c1fb1daf7a4302be5064fec3439388.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-60baf8fb7db845ce8408dd815cf1b7ac.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-cbf66c1bdb2d47e7b9bd4b80ea2723fe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-cbf66c1bdb2d47e7b9bd4b80ea2723fe.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-aa1414388252431b981c931c149742ad: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-ddaca257d0ca495c81c97d4921c5f3fd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-aa1414388252431b981c931c149742ad.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-ddaca257d0ca495c81c97d4921c5f3fd.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-d566884994644af9ae9e65f542dd25f1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-c4a916e778ef4538a66c7c267abff830: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-d566884994644af9ae9e65f542dd25f1.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-c4a916e778ef4538a66c7c267abff830.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-614657e6c643413db4274e84018d9586: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [logger.py:43] Received request chatcmpl-31140207e6fa409f8a2af72efed3b2a5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:00 [async_llm.py:270] Added request chatcmpl-614657e6c643413db4274e84018d9586.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-31140207e6fa409f8a2af72efed3b2a5.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-ce749bcec3d6447783bd1371b7775924: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-a15c74898908484c894eaa68adee2981: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-ce749bcec3d6447783bd1371b7775924.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-a15c74898908484c894eaa68adee2981.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-2fd722c5a2f1473d94d556c562be1323: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-2fd722c5a2f1473d94d556c562be1323.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-1d3508ca6e0e402384bcee71eb4a1bbd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36314 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-1d3508ca6e0e402384bcee71eb4a1bbd.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-11d957c04b6040868386ed2384e6f425: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-11d957c04b6040868386ed2384e6f425.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-1dfc5c3de7134e8c80af636a19e5ff95: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-1dfc5c3de7134e8c80af636a19e5ff95.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-360016faa6774174bfcac05b73335bd0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-360016faa6774174bfcac05b73335bd0.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-a86eea4f7c5c48e59b5acb8106ea0e3c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-926f72dd8f7b4e70941e62bd49d6a03e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-a86eea4f7c5c48e59b5acb8106ea0e3c.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-926f72dd8f7b4e70941e62bd49d6a03e.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-ea70860d495c458aa3f9d9a3b87f75c2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-ea70860d495c458aa3f9d9a3b87f75c2.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-28cc61503eab440da03be238a4f61e5c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-28cc61503eab440da03be238a4f61e5c.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-15dcc135e4554303a5860813250d36f3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-15dcc135e4554303a5860813250d36f3.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-cca4d504c03c42638305d5b8b5f19306: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-cca4d504c03c42638305d5b8b5f19306.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-08e26bcedbfd49f58a7bc16cfd9ea701: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-08e26bcedbfd49f58a7bc16cfd9ea701.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-8d35a7161e8c445e9d79f65d4876d0f2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-f9d50866cc554de4a722eaa4e81c288a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-8d35a7161e8c445e9d79f65d4876d0f2.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-f9d50866cc554de4a722eaa4e81c288a.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-0adac03d510a4b79b8191807f609859b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-0adac03d510a4b79b8191807f609859b.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-0d66502ba7074008b25ecaba8c2281d9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-0d66502ba7074008b25ecaba8c2281d9.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-fe6a621410f3492a8a58508f9074715b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-fe6a621410f3492a8a58508f9074715b.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-384add95bf7d4fe1af667da0e48a1a80: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-384add95bf7d4fe1af667da0e48a1a80.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-c114429cb21a4873af27a16825d1aa90: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-c114429cb21a4873af27a16825d1aa90.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-8621baa9625245d8b35259f61f5bee9e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-8621baa9625245d8b35259f61f5bee9e.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-11770fb03f3e432caeb8fa6f17b810b8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-11770fb03f3e432caeb8fa6f17b810b8.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-ea9f0abb867e421f976467503a7d0e02: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-ea9f0abb867e421f976467503a7d0e02.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-4b69c55ec4ad44818e84379784b275e5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-4b69c55ec4ad44818e84379784b275e5.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-d91e961a175c4f4f885f4234fdc17710: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-c85754a6697e4e3489b38a6b1f6ea9eb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-d91e961a175c4f4f885f4234fdc17710.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-c85754a6697e4e3489b38a6b1f6ea9eb.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-b7e5f8ba87c34305a590961e21d6ad86: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-b7e5f8ba87c34305a590961e21d6ad86.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-80f65aca9d724a05bc6378320c572fb2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-80f65aca9d724a05bc6378320c572fb2.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-9af156382f3940dda52fc9d1a3c909ad: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-9af156382f3940dda52fc9d1a3c909ad.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-9e092ea3817e476e8de753f54c8c589f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-9e092ea3817e476e8de753f54c8c589f.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-297eb214859348d7a2665ac9bcffd75e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-bbf86d229aed48bca483cb720832e547: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36534 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-297eb214859348d7a2665ac9bcffd75e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-bbf86d229aed48bca483cb720832e547.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-0925a132b0e042cd9d4a3c383da338e2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36552 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-0925a132b0e042cd9d4a3c383da338e2.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-24efe048bad14fc1b7029a9c3b63115e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-24efe048bad14fc1b7029a9c3b63115e.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-1d5f4e4779b64680aceda89cb6bbf7b5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-1d5f4e4779b64680aceda89cb6bbf7b5.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-76abb580b26b4a10b669b8399f2d672d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-76abb580b26b4a10b669b8399f2d672d.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-ac1573c756194d2dbf7aaaf16e8a0944: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-ac1573c756194d2dbf7aaaf16e8a0944.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-fbece34fd8ad472ca1d97f54c79afacf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-fbece34fd8ad472ca1d97f54c79afacf.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-958da738e6fd4794a2721713f1811299: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-958da738e6fd4794a2721713f1811299.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-b3a329c7bbc5448784b4c97e6a48a532: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-b3a329c7bbc5448784b4c97e6a48a532.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-ccbb069ac8344875aebc6f6eb385890b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-ccbb069ac8344875aebc6f6eb385890b.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-118cbf4820dd4ceba09796e6a8990731: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-118cbf4820dd4ceba09796e6a8990731.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-e9a8ca44caeb4b009176dcfafd48efbc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36656 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-e9a8ca44caeb4b009176dcfafd48efbc.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-07aa777ac288459b92c3b2695d5e1896: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-07aa777ac288459b92c3b2695d5e1896.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-546f0859237c4ddfa2dfdc1cae139c3e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-900037f681de4f1289b823d1c989a459: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-546f0859237c4ddfa2dfdc1cae139c3e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-900037f681de4f1289b823d1c989a459.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-89531ab6f8634998a9c000115fcd7cd7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-89531ab6f8634998a9c000115fcd7cd7.
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [logger.py:43] Received request chatcmpl-a572da861c3f4367a4bf026058e87c6f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:01 [async_llm.py:270] Added request chatcmpl-a572da861c3f4367a4bf026058e87c6f.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-45dd79f040fb47cf9e22501d68d0e632: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-38ee06d9c0ef4ca484bc05b211d25d2f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-45dd79f040fb47cf9e22501d68d0e632.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-38ee06d9c0ef4ca484bc05b211d25d2f.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-8a12944e11a64f3f821bed5530ae0953: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-a339a826b3ab45b6ac544afe32e538b1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-8a12944e11a64f3f821bed5530ae0953.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-a339a826b3ab45b6ac544afe32e538b1.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-a0b61ac9a71644b8a2eafcf200d72007: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-a771797ece9845aa8aa38e0d3b94dd4d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-a0b61ac9a71644b8a2eafcf200d72007.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-a771797ece9845aa8aa38e0d3b94dd4d.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-9405c1242bb74c3ebf953d8541843a96: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-9405c1242bb74c3ebf953d8541843a96.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-23fc844aefb44e4484ae6f5aa77ce077: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-23fc844aefb44e4484ae6f5aa77ce077.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-c73967d4fdca487588ddb400ad61c827: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36778 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-c73967d4fdca487588ddb400ad61c827.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-2bd5aebd59d942b181c056c83039371c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-c2829f83656243b4ab537d2b13d34642: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-2bd5aebd59d942b181c056c83039371c.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-c2829f83656243b4ab537d2b13d34642.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-76767c7c6e154d7891664f70e051bd9d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-76767c7c6e154d7891664f70e051bd9d.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-2d154e666708468fa230deed521e1552: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-2d154e666708468fa230deed521e1552.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-ff59e57f56ae4a3badff02c35ac624e1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-ff59e57f56ae4a3badff02c35ac624e1.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-ddc75a79970b4f27905db8bfdd63dfc5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-ddc75a79970b4f27905db8bfdd63dfc5.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-3a1e68dad0644a7eb86d217f72eced6f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-3a1e68dad0644a7eb86d217f72eced6f.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-58f1870e00fe4a1db870a124358f05c3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-88616a9ace6245fe86314c7da01da2b8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-77bcb4a9a65f433a8470a3a1d9a02364: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-58f1870e00fe4a1db870a124358f05c3.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-88616a9ace6245fe86314c7da01da2b8.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-77bcb4a9a65f433a8470a3a1d9a02364.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-1db3151f6b6f4207965cd688109e893b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-b9e1f109e7324f4f8243f55fb20b6c92: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-028952c49dad4ad6829b008935373bec: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36888 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-1db3151f6b6f4207965cd688109e893b.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-2a9c53bb10994e14bc36b61c0e614d00: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-77b5ccc788ec4d5b8ec8410ec6999387: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-b9e1f109e7324f4f8243f55fb20b6c92.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-028952c49dad4ad6829b008935373bec.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-8e33520d30f048e5a49947b93f36b3be: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-2a9c53bb10994e14bc36b61c0e614d00.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-77b5ccc788ec4d5b8ec8410ec6999387.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-8e33520d30f048e5a49947b93f36b3be.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-e849090e895e442598fcba22e37c96c3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-e849090e895e442598fcba22e37c96c3.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-24e2eb0263664526b329a974dc75f3af: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-b4a8d241c7f24c96953f953d22849c12: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-24e2eb0263664526b329a974dc75f3af.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-b4a8d241c7f24c96953f953d22849c12.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-3e60c7566b5c4d9fa6ed66f898b9724a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-3e60c7566b5c4d9fa6ed66f898b9724a.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-9449480192f049c4b95abb6cd527c102: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-9449480192f049c4b95abb6cd527c102.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-b1a5fe7cf92749a7ab0325b58d859c40: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-a84716a2e87346b9b11a127c838a5dd2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-b1a5fe7cf92749a7ab0325b58d859c40.
[36mllm_server_1  |[0m INFO:     172.22.0.1:36994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-a84716a2e87346b9b11a127c838a5dd2.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-2bcbe115776349fda2aea5fa8a6e833f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-2bcbe115776349fda2aea5fa8a6e833f.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-57f7264aba9943db8b8b8e206a8f4c0d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-57f7264aba9943db8b8b8e206a8f4c0d.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-7c4ff15758a245ef8d65c9e68ef04f24: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-7c4ff15758a245ef8d65c9e68ef04f24.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-4df0b146311d457aa4c0f3c7f649d7ca: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-4df0b146311d457aa4c0f3c7f649d7ca.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-d9214fb2c70c4cd6a39a7d9f20956dbb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-d9214fb2c70c4cd6a39a7d9f20956dbb.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-2beba2cbe5414a71ac81a3dd52afb0f8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37040 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-2beba2cbe5414a71ac81a3dd52afb0f8.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-97c7ad5215464882a0f537d211edcedf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-97c7ad5215464882a0f537d211edcedf.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-1a97430f9ba1492a9f3e5e2cf66d6ec8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-1a97430f9ba1492a9f3e5e2cf66d6ec8.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-70b11ff3e5ec4d17924659a902642b1a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-70b11ff3e5ec4d17924659a902642b1a.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-d614f3de144549ab943cb408a8df0683: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-d614f3de144549ab943cb408a8df0683.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-717b48967a7141a0a207da97fd9575d0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-717b48967a7141a0a207da97fd9575d0.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-711fb2d97f454c8f872e3965fa1506b0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-711fb2d97f454c8f872e3965fa1506b0.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-3531e4a8a8834265a2818a0563ce6cc8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:37112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-3531e4a8a8834265a2818a0563ce6cc8.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-508fb9172b004ae38dcb3e7abe11ba3e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-53b4fedbcb0645e6b1b24901f12c5c9d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-4dbaba93b8b149fd9b63806201320cba: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-508fb9172b004ae38dcb3e7abe11ba3e.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-cce2957cf12a48c2bef40d03df6d8dc7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-53b4fedbcb0645e6b1b24901f12c5c9d.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-bd7f599007c742bcaea2f3dcb8493b58: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-4dbaba93b8b149fd9b63806201320cba.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-a052d81d50234cebbb8e3ca70c051af0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [logger.py:43] Received request chatcmpl-2eacb98503d84ce9bf602cd754681bfb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-cce2957cf12a48c2bef40d03df6d8dc7.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-bd7f599007c742bcaea2f3dcb8493b58.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-a052d81d50234cebbb8e3ca70c051af0.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:02 [async_llm.py:270] Added request chatcmpl-2eacb98503d84ce9bf602cd754681bfb.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-e8190285bec747e5857f77976e4122f7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-e8190285bec747e5857f77976e4122f7.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-f4955df05f594eb3ba475b2ad8c272e8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-ea163a6730d64da99e5fb3efef19755c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-f4955df05f594eb3ba475b2ad8c272e8.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-ea163a6730d64da99e5fb3efef19755c.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-e0c6df284c384979a4e2fa727bb948df: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-9e090d10c5e34e6d943eddb6ec52cd66: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-e0c6df284c384979a4e2fa727bb948df.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-9e090d10c5e34e6d943eddb6ec52cd66.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-1fd9a635e23140b489874abd8f4e9fcf: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-1fd9a635e23140b489874abd8f4e9fcf.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-2b456ee7cd274ce0be0c3dc856ee24bf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-2b456ee7cd274ce0be0c3dc856ee24bf.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-882a4707fb7b451a83fdd56cf9276dde: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-882a4707fb7b451a83fdd56cf9276dde.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-5c764a606c9d46739ac4fd056a58f1e1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-5c764a606c9d46739ac4fd056a58f1e1.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-30d511063cd54dd2a0fffcad9b63e11f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-30d511063cd54dd2a0fffcad9b63e11f.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-def09201a90f4130b1e74360eb85a3b3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-def09201a90f4130b1e74360eb85a3b3.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-3c7666faf32d41e8abb5039f767b6ad5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-3c7666faf32d41e8abb5039f767b6ad5.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-ea6bb51e65a44db18478b85e462ad60d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-ea6bb51e65a44db18478b85e462ad60d.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-072def65a1e941208d91c54d3704da08: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-072def65a1e941208d91c54d3704da08.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-c7da5b7916904eb68082900d62faa88f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-c7da5b7916904eb68082900d62faa88f.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-ed355f1f7f104e479cdfb42af7b32c26: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-ed355f1f7f104e479cdfb42af7b32c26.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-2222aa0b209942429448af8944528f7d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-2222aa0b209942429448af8944528f7d.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-6f5ecb066b3849a8808494512ac24381: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-6f5ecb066b3849a8808494512ac24381.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-d633c0982e4d41f0adbb5594d887d44d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-d633c0982e4d41f0adbb5594d887d44d.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-0199663e1e0c40e793f0fce4f08c476c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-0199663e1e0c40e793f0fce4f08c476c.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-aa3bebe00f844d0f93ca979295668a27: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-aa3bebe00f844d0f93ca979295668a27.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-cc33854ef1414deb8bc7960e777ee687: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-cc33854ef1414deb8bc7960e777ee687.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-fc471cf572144493be0f742716b7cfd0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-fc471cf572144493be0f742716b7cfd0.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-656835b224df4199a8409f9a1c674f11: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-656835b224df4199a8409f9a1c674f11.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-44370b7e50084c338769d4009090a937: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-44370b7e50084c338769d4009090a937.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-84329f6835874017bcb19e26181677e0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-84329f6835874017bcb19e26181677e0.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-05bbd16b204649f29e9698572efab424: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-05bbd16b204649f29e9698572efab424.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-c8f8fc200e024a169c69058faf06de3d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-c8f8fc200e024a169c69058faf06de3d.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-789f3467a9e4432db4da9238d4c5bc0a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-789f3467a9e4432db4da9238d4c5bc0a.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-b93868cc4f2d4ae09ad6675f5ca1a519: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-b93868cc4f2d4ae09ad6675f5ca1a519.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-0618feb9b0f04de883d53723bc2ffe61: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45632 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-0618feb9b0f04de883d53723bc2ffe61.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-840de05dcbb749df88c76adf0ccb8778: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-840de05dcbb749df88c76adf0ccb8778.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-aa929c7bd37643fc9b47df3f94427f31: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45642 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-aa929c7bd37643fc9b47df3f94427f31.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-b4c71072a2654545878d12c7386c4aaf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-dda3dd9da8c44d1881920de76151a17e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-b4c71072a2654545878d12c7386c4aaf.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45656 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-dda3dd9da8c44d1881920de76151a17e.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-456f8cebf074435c9cd553385ba5f754: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-157cfaf089fd49acb0d6920ae7d7f592: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45662 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-456f8cebf074435c9cd553385ba5f754.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-157cfaf089fd49acb0d6920ae7d7f592.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-e372de0da6a34fa1a2ebfbe3461a2855: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-e372de0da6a34fa1a2ebfbe3461a2855.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-367c403f73aa404c9944ab26ff684683: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45686 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-367c403f73aa404c9944ab26ff684683.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-76f6e466d678499b85af629d4cc460d9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-76f6e466d678499b85af629d4cc460d9.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-757e54c6d1b047729a76b621f7ae9593: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-757e54c6d1b047729a76b621f7ae9593.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-a18b7960743a4bdd8228b231dc1e9160: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-a18b7960743a4bdd8228b231dc1e9160.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-8a373bfd9d36421c809084822149267f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-8a373bfd9d36421c809084822149267f.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-28f00f6235494a349efc5ce2709a1ebe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-28f00f6235494a349efc5ce2709a1ebe.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-234b4b281000418ba0d22cd8513913b9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-234b4b281000418ba0d22cd8513913b9.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-5c0cd3de67854e739c30e3685aeb1193: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-5c0cd3de67854e739c30e3685aeb1193.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-fd59946bb6074e10947a78a8fec882b0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-fd59946bb6074e10947a78a8fec882b0.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-b784a941cbc54c8caa66093f878ced6c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-b784a941cbc54c8caa66093f878ced6c.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-207201195b9243ddb21cd6b3a75f18f0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-207201195b9243ddb21cd6b3a75f18f0.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-bdd7eaac7186466792e7899ffe8b354d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-bdd7eaac7186466792e7899ffe8b354d.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-bf6d8a07762d42308b83a11c648df03d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-bf6d8a07762d42308b83a11c648df03d.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-e5d7260540374413ad75df16f653fcd7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-e5d7260540374413ad75df16f653fcd7.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-5c58e68d513141df9af42ce8c325eb17: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-5c58e68d513141df9af42ce8c325eb17.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-4cb1a8addcd14b2fa27cc310f30c5a88: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-4cb1a8addcd14b2fa27cc310f30c5a88.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-35c2c14fde084db69eb3283bd9110e01: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-35c2c14fde084db69eb3283bd9110e01.
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [logger.py:43] Received request chatcmpl-41ee3683dbc64b1581b38a289df592f5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:03 [async_llm.py:270] Added request chatcmpl-41ee3683dbc64b1581b38a289df592f5.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-18bd79bd16bb4d64929b9b6a75f37a96: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-18bd79bd16bb4d64929b9b6a75f37a96.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-d6c25240527847d98f929a7324a7794e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-d6c25240527847d98f929a7324a7794e.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-cd83173f17014f8da5dc56160a117599: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-cd83173f17014f8da5dc56160a117599.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-aa0b83b2b44e4a62bc0dd4415974300c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-aa0b83b2b44e4a62bc0dd4415974300c.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-5343597d858e4e0387d976554515a4aa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-248c00cc9c81458bad492770ec9b62cd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45814 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-5343597d858e4e0387d976554515a4aa.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-248c00cc9c81458bad492770ec9b62cd.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-357f7f64c88b46518a49fc004a3dcd94: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-357f7f64c88b46518a49fc004a3dcd94.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-5bc3e02403084fe490b9acb96a971e49: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-5bc3e02403084fe490b9acb96a971e49.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-9c868c7d01e549e793bd9c9d93bf9c63: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-9c868c7d01e549e793bd9c9d93bf9c63.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-73b0715384aa41ffaa780223f7c66102: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-73b0715384aa41ffaa780223f7c66102.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-247d84d6ab5b4a028c176180ed0139f3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-3ecc6a67a3c24360b121d37d8001cdd8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-247d84d6ab5b4a028c176180ed0139f3.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-3ecc6a67a3c24360b121d37d8001cdd8.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-d564b5ac519648ca8c4eaf1321321b68: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-d564b5ac519648ca8c4eaf1321321b68.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-16a7e705db31433cb2a06ada22d6b779: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-16a7e705db31433cb2a06ada22d6b779.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-b9c998afcd3d4641ad603a50b217424c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-b9c998afcd3d4641ad603a50b217424c.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-f90332e96f67468cb475b49e3a4ee425: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-f90332e96f67468cb475b49e3a4ee425.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-50db14fc4c444ffd8780426474051456: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-50db14fc4c444ffd8780426474051456.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-5bf4add747d14a7cb905e9d601e6a1b0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-5bf4add747d14a7cb905e9d601e6a1b0.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-4dffe7aa044c43879f9c4e1e53928ae1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-4dffe7aa044c43879f9c4e1e53928ae1.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-465a4f88d28c4513a294d09b0d0b3853: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-465a4f88d28c4513a294d09b0d0b3853.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-8d8f27b7f8d242068c2b09b6bc35714b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-8d8f27b7f8d242068c2b09b6bc35714b.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-2908452db4e44addb43b1f80b0fff6d7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-2908452db4e44addb43b1f80b0fff6d7.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-a675834b177046ae851ca43ba2a3b8dd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-a675834b177046ae851ca43ba2a3b8dd.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-f10d0c7a04904d578b04f35c7be1c3fa: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-f10d0c7a04904d578b04f35c7be1c3fa.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-61f6e951431740d899954cce69b19412: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-61f6e951431740d899954cce69b19412.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-79d1b644babe48729d612fd7210d729b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-79d1b644babe48729d612fd7210d729b.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-447b94d82ea94437a64cb22c4b72fe1c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-447b94d82ea94437a64cb22c4b72fe1c.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-7a554150a77b40a3993848f467efd4ea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-7a554150a77b40a3993848f467efd4ea.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-0e4d856b34c548ccb803f7ebc40cf201: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-0e4d856b34c548ccb803f7ebc40cf201.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-274fe462646b48a18043ad6c91379339: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-274fe462646b48a18043ad6c91379339.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-825dfa7d42554e9fbb9a8b0eedc77e27: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-825dfa7d42554e9fbb9a8b0eedc77e27.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-7169d7969398428aa46879770b6aa32a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46074 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-7169d7969398428aa46879770b6aa32a.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-c564f6d321744ba38af4e5948c2406ca: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-c564f6d321744ba38af4e5948c2406ca.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-abe2af22e6964b0790bd3429e2e27d07: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-abe2af22e6964b0790bd3429e2e27d07.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-cbf4311906814e73abc3b1d4585c1099: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-cbf4311906814e73abc3b1d4585c1099.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-3c6ed4b2ad154e738064563656b57477: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-3c6ed4b2ad154e738064563656b57477.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-a69020f607714795b81e65c64ff9207f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-a69020f607714795b81e65c64ff9207f.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-b25224298b6245b68b3842f1c440d583: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-901c0891fe804c5f97567561439c3ae7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-b25224298b6245b68b3842f1c440d583.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-901c0891fe804c5f97567561439c3ae7.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-ea32fbde000e4d13a8ea48ca891a13d2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-ea32fbde000e4d13a8ea48ca891a13d2.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-c2eb77ed768c48c6b07a14c6db8a5a8b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-c2eb77ed768c48c6b07a14c6db8a5a8b.
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [logger.py:43] Received request chatcmpl-f1ca8797dc7644cbbdefc389e9491cf4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:04 [async_llm.py:270] Added request chatcmpl-f1ca8797dc7644cbbdefc389e9491cf4.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-8f10d57009c84673b04caf7e7a191d71: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-8f10d57009c84673b04caf7e7a191d71.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-72761ac1611b4c8cb72f3265d7d97e7f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-72761ac1611b4c8cb72f3265d7d97e7f.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-15e5cc676e7947a4976781b154f8e734: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-15e5cc676e7947a4976781b154f8e734.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-cfd8c057fe0e4df0922f8425243c50f5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-cfd8c057fe0e4df0922f8425243c50f5.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-2868f4a66b7c48368d9964b08d50c888: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-2868f4a66b7c48368d9964b08d50c888.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-9863fbdb9d8e4eb9ba595f0c33e211d2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-9863fbdb9d8e4eb9ba595f0c33e211d2.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-9e62f9e3aad04a15aaa8012ccc21a684: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-9e62f9e3aad04a15aaa8012ccc21a684.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-011cffa6aa444fd1868faf5d77b56d58: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-011cffa6aa444fd1868faf5d77b56d58.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-e745390ac3be434cbc68518e80092293: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-e745390ac3be434cbc68518e80092293.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-fab3cdbbcf5c441c840972099e0366ce: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-fab3cdbbcf5c441c840972099e0366ce.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-d07d799a24b54801a2dd92466c88a358: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-d07d799a24b54801a2dd92466c88a358.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-74249e98ab8e4b59a9aa156633eff479: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-74249e98ab8e4b59a9aa156633eff479.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-bd26a1b79e204e7197dc4cc6a94dbabd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-bd26a1b79e204e7197dc4cc6a94dbabd.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-748568e77e36415ca3eae64b975efa43: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-748568e77e36415ca3eae64b975efa43.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-d2221310158046b3a1684e57a1c6b366: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-d2221310158046b3a1684e57a1c6b366.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-09f653912d9b464db0a644c8343a50a0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-09f653912d9b464db0a644c8343a50a0.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-5644933773a44173bee94a8edfb38c1a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-5644933773a44173bee94a8edfb38c1a.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-8e3c0decc9e3428db4a36106f81bb3c0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-8e3c0decc9e3428db4a36106f81bb3c0.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-4bac1a0a9c484aedb141b150d15ecf9f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-4bac1a0a9c484aedb141b150d15ecf9f.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-0e71499bf467418b921c8a0113010482: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-0e71499bf467418b921c8a0113010482.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-3af9f2288451432bae4ef4275f877c3c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-3af9f2288451432bae4ef4275f877c3c.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-b75c2e8e8a094cb3ac22fc29ab98810b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-b75c2e8e8a094cb3ac22fc29ab98810b.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-d635338c7e5d4ab2873b0d33dcf2034e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-d635338c7e5d4ab2873b0d33dcf2034e.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-30f9a7574fe94e89b739c0c1d0bcb26f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-30f9a7574fe94e89b739c0c1d0bcb26f.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-e460705656ce4fb19d1e46172de1a766: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-e460705656ce4fb19d1e46172de1a766.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-2639ee81876a4256a06c71825dd94b08: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-2639ee81876a4256a06c71825dd94b08.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-04ed3d0c4341461bb51d06b723dce1d7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-04ed3d0c4341461bb51d06b723dce1d7.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-17a369811e324695942d04f7a97546be: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-17a369811e324695942d04f7a97546be.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-cc0c5755c7d94f5799f8e98744092d39: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-cc0c5755c7d94f5799f8e98744092d39.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-cb07ccc6ddef4d15a7fe1e7f974f3b10: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-cb07ccc6ddef4d15a7fe1e7f974f3b10.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-af217c87a5a243fa9843860899ea4933: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-af217c87a5a243fa9843860899ea4933.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-a14b97c27fe545e0ace04c912a25f688: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-a14b97c27fe545e0ace04c912a25f688.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-fb2241abfba44706aea12f5e0e2de1b9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-fb2241abfba44706aea12f5e0e2de1b9.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-665f359602494459bb9536526ffb4a5c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-665f359602494459bb9536526ffb4a5c.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-360fdb82d00a4465a1721523cd07f5de: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-360fdb82d00a4465a1721523cd07f5de.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-36cb729de79245f4a9fb1a54587468f5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-36cb729de79245f4a9fb1a54587468f5.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-16bd4f727eb244c5a771f78477f7d2be: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-16bd4f727eb244c5a771f78477f7d2be.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-60af3bdbcc504f89809f06706d2a7a5e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-60af3bdbcc504f89809f06706d2a7a5e.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-40685e3c8c344740ab97427df111cfcf: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-40685e3c8c344740ab97427df111cfcf.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-e44d90aaf2db487294f15087c8e6877e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-e44d90aaf2db487294f15087c8e6877e.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-d1313a48eed24e52aab4711d79af2d2f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-cc65230ffde3476386e3a6c30820a89b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-d1313a48eed24e52aab4711d79af2d2f.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-e408bdd60cb14b76b8d0b0a6467584f1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-3521ba37c29a4e34bc3dfc718809db5d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-cc65230ffde3476386e3a6c30820a89b.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-e408bdd60cb14b76b8d0b0a6467584f1.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-df4ab6404db84e8b88f699a2bcef129f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-3521ba37c29a4e34bc3dfc718809db5d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-df4ab6404db84e8b88f699a2bcef129f.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-78a7fdb24ff7461f92d8f575fe8bf78d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-78a7fdb24ff7461f92d8f575fe8bf78d.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-a556bfdef7d4447d85e609c53b3628e9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-a556bfdef7d4447d85e609c53b3628e9.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-489a24e8f5ca4ea6b5382b3ce7553f27: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-489a24e8f5ca4ea6b5382b3ce7553f27.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-1dd529d2f7b644e0af1afb39d9c2abdd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-1dd529d2f7b644e0af1afb39d9c2abdd.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-9fc8e62859d94939bbc3bdef662b4c6d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-9fc8e62859d94939bbc3bdef662b4c6d.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-ffd905909da74221bc0ad143bf84d82e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-ffd905909da74221bc0ad143bf84d82e.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-fd6bdeac719a49c68080c543f54f90c3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-fd6bdeac719a49c68080c543f54f90c3.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-e4f656e81daa4748946c65bb1bfd20b2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-e4f656e81daa4748946c65bb1bfd20b2.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-19a7c645be8143bd8318503e4beeeafe: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-19a7c645be8143bd8318503e4beeeafe.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-0b2e81c7cc75415f8dc07269baf56d9a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-0b2e81c7cc75415f8dc07269baf56d9a.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-ba88edc9720e425f8634f301c172df2d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-ba88edc9720e425f8634f301c172df2d.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-3cc6192381914f0e94bc808b414960c5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-3cc6192381914f0e94bc808b414960c5.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-04c4b239cd6d4d0fa89227be9613eaf0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46712 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-04c4b239cd6d4d0fa89227be9613eaf0.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-26b113cb427b4570a153ef9436c7b2fd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-26b113cb427b4570a153ef9436c7b2fd.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-616ff1b144ae4d3fa99a6c0e6934e0f2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-616ff1b144ae4d3fa99a6c0e6934e0f2.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-da9157b9ba894dcb82443b320762d9f6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-da9157b9ba894dcb82443b320762d9f6.
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [logger.py:43] Received request chatcmpl-9d2985f316a44991b302703da9b02aa1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:05 [async_llm.py:270] Added request chatcmpl-9d2985f316a44991b302703da9b02aa1.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-1ad7b09a201f4120a76a9becc8fc54f8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-1ad7b09a201f4120a76a9becc8fc54f8.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-b7f760a8d63146ac89114ebf442f60cb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-b7f760a8d63146ac89114ebf442f60cb.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-001cff51c19d47029780ff90d6afb313: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-001cff51c19d47029780ff90d6afb313.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-54d09a6e804441deac414aac34b8b9c7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-7158940e6a8b4542b7cd049cfb827f67: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-54d09a6e804441deac414aac34b8b9c7.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-df47ef33f335455dab881aa69df8769a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-7158940e6a8b4542b7cd049cfb827f67.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-eada0a55ddba425fa1fda5e0dc15a463: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-df47ef33f335455dab881aa69df8769a.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-eada0a55ddba425fa1fda5e0dc15a463.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-b2ae0c193093499e8153570cd147e4d5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-b2ae0c193093499e8153570cd147e4d5.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-4699fd4b56a34702a863ca88e59a94cc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-4699fd4b56a34702a863ca88e59a94cc.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-4750dfe32a074d68b11495010de90cc9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-a2e61874f4354332bd0392f194aaa3e1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-4750dfe32a074d68b11495010de90cc9.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-a2e61874f4354332bd0392f194aaa3e1.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-d38f26124d6044ca9f2af9a007fa0580: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-d38f26124d6044ca9f2af9a007fa0580.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-1f6f8e40c2084c3499e1ad1c03979ee3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-1f6f8e40c2084c3499e1ad1c03979ee3.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-8d28f2fd5a624c64b30ef06ebabcbcde: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-3a9448d9b11a424d85399d03c4e13ffd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46900 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-8d28f2fd5a624c64b30ef06ebabcbcde.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-3a9448d9b11a424d85399d03c4e13ffd.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-79ca84ba4d854ad1b608565bd0d2f028: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-79ca84ba4d854ad1b608565bd0d2f028.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-c638a43a82604e388ff74aeac53237d5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-c638a43a82604e388ff74aeac53237d5.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-9d3f24e8598a4b3ea1745e0630a2e504: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-9d3f24e8598a4b3ea1745e0630a2e504.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-e0d3758392314c3782d52b77c8c824c3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-e0d3758392314c3782d52b77c8c824c3.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-41bfd770f19b409a95a0dc1ece48bc76: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-41bfd770f19b409a95a0dc1ece48bc76.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-3a4a62b0c0d14b63a67835af2858408d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-3a4a62b0c0d14b63a67835af2858408d.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-afa524cd26ae4f879741beb32ca1dc65: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-afa524cd26ae4f879741beb32ca1dc65.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-86ab2c713a654fce82fbc54ff791e6df: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-86ab2c713a654fce82fbc54ff791e6df.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-756e7fd8807348d9a828b5a4c776df8e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-756e7fd8807348d9a828b5a4c776df8e.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-e99f95492d69426984f8ff06fb1ac300: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-e99f95492d69426984f8ff06fb1ac300.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-795d4a3d46894bad9f2bdfc2749cff69: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-795d4a3d46894bad9f2bdfc2749cff69.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-0373851034874658bd41712e2dc9f067: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-0373851034874658bd41712e2dc9f067.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-5c467ed5e333438c96779ef69ba8cc62: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-5c467ed5e333438c96779ef69ba8cc62.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-d9dbd47c47e242afaeead81776f86f00: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-cc1aa6b213c946758415e50e52fb8e72: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-d9dbd47c47e242afaeead81776f86f00.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-cc1aa6b213c946758415e50e52fb8e72.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-23f7b43d43514ca5a3c4fc7418b3f297: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-23f7b43d43514ca5a3c4fc7418b3f297.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-1a888821fd0a4efba16277e2ac44db14: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-1a888821fd0a4efba16277e2ac44db14.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-e31ad803d8664f3486377ca9f00bf26a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-e31ad803d8664f3486377ca9f00bf26a.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-125b48ba35a2419ba1c5d11546d6aca5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-125b48ba35a2419ba1c5d11546d6aca5.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-b58b08293a1b468bb503e7487db8f865: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-b58b08293a1b468bb503e7487db8f865.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-43ad85a13a2549708842f191d69c6cd5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-43ad85a13a2549708842f191d69c6cd5.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-b1df2e06429845fbb2614b72fc4339c1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-757cb4a14d894d98a24667f042fcb163: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-b1df2e06429845fbb2614b72fc4339c1.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-757cb4a14d894d98a24667f042fcb163.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-f8517566c1d7437fb4280c24ec07721f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-97b9ceae6bfb443ebb227f5ca9b83adf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-f8517566c1d7437fb4280c24ec07721f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-97b9ceae6bfb443ebb227f5ca9b83adf.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-6c79f84d44934e669cc1350bcf173dc6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-6c79f84d44934e669cc1350bcf173dc6.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-c5123c8ae1c046668ea0c6c0cf728a55: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [async_llm.py:270] Added request chatcmpl-c5123c8ae1c046668ea0c6c0cf728a55.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-0e7f4af9347744769549f8cbc55c9196: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:06 [logger.py:43] Received request chatcmpl-aa8aca4ebab748309632d68e28cb7461: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-0e7f4af9347744769549f8cbc55c9196.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-aa8aca4ebab748309632d68e28cb7461.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-b0ad409fe7b14a97be3ad0a6487991f0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-b0ad409fe7b14a97be3ad0a6487991f0.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-08ee9e06decb4d8080ce834d5084bbb1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-08ee9e06decb4d8080ce834d5084bbb1.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-6cc8f30ddf064bea9160fe90d63e4a40: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-6cc8f30ddf064bea9160fe90d63e4a40.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-8136edea16e94cfaa97d73de94390d68: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-8136edea16e94cfaa97d73de94390d68.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-a5ddb1dd5c51460593d4f04477fd7f52: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-a5ddb1dd5c51460593d4f04477fd7f52.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-ac4f4768fc944c1fb6783f2a73f397fe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-ac4f4768fc944c1fb6783f2a73f397fe.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-a1de7888efd549e496b1bb38ec2e66de: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-3d6e70ae2693427592fb590546823dfa: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-4461f13b7e05410daec4c22954ad6ee7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-a1de7888efd549e496b1bb38ec2e66de.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-3d6e70ae2693427592fb590546823dfa.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-53eb4499aa9b4adc9755b83efe9e9f73: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-4461f13b7e05410daec4c22954ad6ee7.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-e700ab6b728c4808825c22064f40a081: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-53eb4499aa9b4adc9755b83efe9e9f73.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-e700ab6b728c4808825c22064f40a081.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-58490e24cfed4e2abea81e5d163980d6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-b7a982a650a342ecae440866dd6fcdaf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-58490e24cfed4e2abea81e5d163980d6.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-b7a982a650a342ecae440866dd6fcdaf.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-58dc7c6c57f84ec2aea093529beb84f3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-58dc7c6c57f84ec2aea093529beb84f3.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-9facd64785694cc581cff35e219da127: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-9facd64785694cc581cff35e219da127.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-0ba7599fd43940f6bf03c72d2fd2c3f5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-0ba7599fd43940f6bf03c72d2fd2c3f5.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-ebcf94a3c9bf416fa3aa6376781ad4f3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-ebcf94a3c9bf416fa3aa6376781ad4f3.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-abc86a286e7a496d845d554a8aca7eb0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-abc86a286e7a496d845d554a8aca7eb0.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-64e264c469a849d69174979aa20c9f46: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-64e264c469a849d69174979aa20c9f46.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [loggers.py:118] Engine 000: Avg prompt throughput: 542.6 tokens/s, Avg generation throughput: 1161.2 tokens/s, Running: 105 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.0%, Prefix cache hit rate: 82.6%
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [loggers.py:118] Engine 001: Avg prompt throughput: 537.2 tokens/s, Avg generation throughput: 1145.9 tokens/s, Running: 105 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.8%, Prefix cache hit rate: 82.2%
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [loggers.py:118] Engine 002: Avg prompt throughput: 552.8 tokens/s, Avg generation throughput: 1141.2 tokens/s, Running: 106 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.7%, Prefix cache hit rate: 82.8%
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [loggers.py:118] Engine 003: Avg prompt throughput: 546.4 tokens/s, Avg generation throughput: 1135.2 tokens/s, Running: 106 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.6%, Prefix cache hit rate: 82.9%
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-643e97ae3c694ddaacec34ea962ecf6d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-643e97ae3c694ddaacec34ea962ecf6d.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-438c2c4cf4dc43dd9b8fb47a4b7a233d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-438c2c4cf4dc43dd9b8fb47a4b7a233d.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-cb71286cf8e64a4987c8f040c10b0ee3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-cb71286cf8e64a4987c8f040c10b0ee3.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-d8318c08ef4343ff9c75eff7ebbcbae1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-d8318c08ef4343ff9c75eff7ebbcbae1.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-ec5abf188c9445af818dc327486de04f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-ec5abf188c9445af818dc327486de04f.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-10d51771b9f54690b56de1b011495954: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-10d51771b9f54690b56de1b011495954.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-c967c7d47dee4166ba8b29fa3188126a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-c967c7d47dee4166ba8b29fa3188126a.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-7ee4236b3c2e44c88b85dc007e061f96: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-7ee4236b3c2e44c88b85dc007e061f96.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-2e43aad23b40484a8faa4a319f9b8167: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-2e43aad23b40484a8faa4a319f9b8167.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-f48f6a29ebc7432cbaa910365cd3aaa0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-f48f6a29ebc7432cbaa910365cd3aaa0.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-6698966e7e944af389898d03abc206bf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-6698966e7e944af389898d03abc206bf.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-1bc2a4c3b9194ffe88db6b84f3fb5536: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47418 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-1bc2a4c3b9194ffe88db6b84f3fb5536.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-be34e10241c24140b651af48a467c1b3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-be34e10241c24140b651af48a467c1b3.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-c3d3df4f114f4eedb3b02d00687b42d0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-c3d3df4f114f4eedb3b02d00687b42d0.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-52d7923206fe48f686ec4df69999818e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-52d7923206fe48f686ec4df69999818e.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-7a8ab7a40dab47448cdd7c8668abf3b0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-7a8ab7a40dab47448cdd7c8668abf3b0.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-cb2c29081afa425090985d3005612af2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-cb2c29081afa425090985d3005612af2.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-4194c5aed0be4f77998c831c5e1a28f5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-4194c5aed0be4f77998c831c5e1a28f5.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-a7ea8e777c0e4bf3b84c3313dbda69d5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-a7ea8e777c0e4bf3b84c3313dbda69d5.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-2774bd05cfab4f7cad75a6920869c116: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-2774bd05cfab4f7cad75a6920869c116.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-14cab6dbf37b4ce7b708d9a4b90efe38: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-14cab6dbf37b4ce7b708d9a4b90efe38.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-41a1576dc2d544839b4ac075dd0a4679: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-41a1576dc2d544839b4ac075dd0a4679.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-6c7988f9565243f7aa5908a7caae7e14: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47524 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-6c7988f9565243f7aa5908a7caae7e14.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-bc3b413a14ed4f7fa6e6e3d315617130: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47536 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-bc3b413a14ed4f7fa6e6e3d315617130.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-ced5daa85a704af0a2243242114cc89a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-ced5daa85a704af0a2243242114cc89a.
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [logger.py:43] Received request chatcmpl-0647f0ce2d824116aee31191245ae722: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:07 [async_llm.py:270] Added request chatcmpl-0647f0ce2d824116aee31191245ae722.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-50750a53daec4bcb99e8369a7d917f29: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47552 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-50750a53daec4bcb99e8369a7d917f29.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-49c9730d413b4a1181dcfc1e9a46fce8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-49c9730d413b4a1181dcfc1e9a46fce8.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-d346b2973922412298f68915e9fc8bad: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-d346b2973922412298f68915e9fc8bad.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-30ed6765cc5e48e2ac487554a99b6497: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-30ed6765cc5e48e2ac487554a99b6497.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-b4edec9d36b44d93a90849f73e40f7d9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-b4edec9d36b44d93a90849f73e40f7d9.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-1261973751c84278adb070a91256c17a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-1261973751c84278adb070a91256c17a.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-10fe091d2dee48648e817a56f3907421: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-10fe091d2dee48648e817a56f3907421.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-7bc9f9e933024e27918411161ca688ee: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-7bc9f9e933024e27918411161ca688ee.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-c4203bc2b5b34a0e884462a9276c114e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-c4203bc2b5b34a0e884462a9276c114e.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-8a2de6d73c814ed999a62f24becde538: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47616 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-8a2de6d73c814ed999a62f24becde538.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-174d649309b9455084b5145778c1c5da: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47632 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-174d649309b9455084b5145778c1c5da.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-5bcd637b011b441aa14f2cbf99ef02a5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-5bcd637b011b441aa14f2cbf99ef02a5.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-7e9af39ba4c245fc9421a63a19136de2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47642 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-7e9af39ba4c245fc9421a63a19136de2.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-cbaf3bf46e4848849784fbf3900a52af: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-cbaf3bf46e4848849784fbf3900a52af.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-8cab1a78e0674bb696cdeaf239be5862: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-8cab1a78e0674bb696cdeaf239be5862.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-40c64c6ff3724d86b14cd02eccc6aa94: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-40c64c6ff3724d86b14cd02eccc6aa94.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-0c52f762836642aea52ad757f326bf60: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-0c52f762836642aea52ad757f326bf60.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-706e520a0fe243ce8acd21be4db01665: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-706e520a0fe243ce8acd21be4db01665.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-03cc1c6a747e4e9ea29e38a68275c802: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47686 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-03cc1c6a747e4e9ea29e38a68275c802.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-43778b72d34347aabebf5dd277fa8d8a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-43778b72d34347aabebf5dd277fa8d8a.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-0960055b1ac04f1fa5ebb16509cfaf54: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-0960055b1ac04f1fa5ebb16509cfaf54.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-0da471fdb69b46c999fbbc8d015f9b18: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-0da471fdb69b46c999fbbc8d015f9b18.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-789f6bb4b5df472c8d9af7a77f9b6898: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-789f6bb4b5df472c8d9af7a77f9b6898.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-fa1bd6302b634cf086e622ee7532150f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-fa1bd6302b634cf086e622ee7532150f.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-06b2961e0848463e918831f8c7952b5a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-c4f606d0cbca4f1f983ed6ff6ac2ba1c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-06b2961e0848463e918831f8c7952b5a.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-c4f606d0cbca4f1f983ed6ff6ac2ba1c.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-eaaeeede4bcf44f1be6a808a4addf9ed: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-eaaeeede4bcf44f1be6a808a4addf9ed.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-fcd21735e55f4e8fa315ef7102576c1d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-fcd21735e55f4e8fa315ef7102576c1d.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-18b9f287058b4d468523a6fcc4274aa3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-18b9f287058b4d468523a6fcc4274aa3.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-78ccb3cd30824a54abaa2d6027240076: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47778 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-78ccb3cd30824a54abaa2d6027240076.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-5c92c1cb8d544028be5199bc0ce6ba98: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-5c92c1cb8d544028be5199bc0ce6ba98.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-2e09659ed6ca4133ab0d509dd63e8585: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-2e09659ed6ca4133ab0d509dd63e8585.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-9d22d86deb9141fca1455a510966ec6e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-9d22d86deb9141fca1455a510966ec6e.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-228128046242435585d4eb82b9965806: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-228128046242435585d4eb82b9965806.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-bd1c0ed85962429f8c789c9184df9b50: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-bd1c0ed85962429f8c789c9184df9b50.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-8b294275b01547ecbbf2c6d62ea4c4d6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-8b294275b01547ecbbf2c6d62ea4c4d6.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-a8073abc2a36483396768ed2304453d7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-a8073abc2a36483396768ed2304453d7.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-b55f2b8608ab43eb9b16e6f6ac44c7c1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-b55f2b8608ab43eb9b16e6f6ac44c7c1.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-218950a856cf492493f7bc21b2ce9a0f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-218950a856cf492493f7bc21b2ce9a0f.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-56e488c2e2d54918a7fffc0254d69f3b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47888 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-56e488c2e2d54918a7fffc0254d69f3b.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-5ff147f652d941d08b54d878ef33ca28: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-5ff147f652d941d08b54d878ef33ca28.
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [logger.py:43] Received request chatcmpl-8a78052924e142e09cdacd45a1c43a03: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:08 [async_llm.py:270] Added request chatcmpl-8a78052924e142e09cdacd45a1c43a03.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-1e7313f837cd4cb8b06fce4846e69741: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-1e7313f837cd4cb8b06fce4846e69741.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-2d2d36814f90497bb6a983585e1ae251: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-2d2d36814f90497bb6a983585e1ae251.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-1efed3e8019d4746a17fe455df6d306b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-1efed3e8019d4746a17fe455df6d306b.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-ba23a1a014bc43b0af640a1d08434591: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-ba23a1a014bc43b0af640a1d08434591.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-1c867fb44c194e2f902a98be0741e3c8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-1c867fb44c194e2f902a98be0741e3c8.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-d1126d3e6b5a433f90cd72ca5bccd575: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-d1126d3e6b5a433f90cd72ca5bccd575.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-a6577b4489754876a522fae1bba6ece9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-a6577b4489754876a522fae1bba6ece9.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-99e276902a7c4e838154735f00ca3733: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-99e276902a7c4e838154735f00ca3733.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-8fabf94253a447da8f92600d5ada2ec5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-8fabf94253a447da8f92600d5ada2ec5.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-14b60a8c3b7246a38c76e1dd3a537ab1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-14b60a8c3b7246a38c76e1dd3a537ab1.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-3045c3238a2343a5ad59572f852a50db: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-3045c3238a2343a5ad59572f852a50db.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-862fe623da44451992bd370acd1bfc18: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-862fe623da44451992bd370acd1bfc18.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-35537c3b4ae746b2bfd9a514cf78d3cf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-35537c3b4ae746b2bfd9a514cf78d3cf.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-214f9531f59a4e10a21afb191d04a492: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-214f9531f59a4e10a21afb191d04a492.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-5f9fa954707040bfadbd41f07a1becea: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-5f9fa954707040bfadbd41f07a1becea.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-91e8134e97c84320a7a9b540154caa27: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-91e8134e97c84320a7a9b540154caa27.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-f528da73fa5a4a14be82b9bd5db1f599: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-f528da73fa5a4a14be82b9bd5db1f599.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-b5427e9f528f4c42b87bedee622ab9d3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-b5427e9f528f4c42b87bedee622ab9d3.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-de38eae0c82e40408e07d09ff8db42d5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-de38eae0c82e40408e07d09ff8db42d5.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-929722d879314934a20c14b86ce6dcc6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-929722d879314934a20c14b86ce6dcc6.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-47941fb4168b46b4a565941f3eb53f9f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-47941fb4168b46b4a565941f3eb53f9f.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-137f67e0d58b4ab3ae61588fdcd8e8b3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-137f67e0d58b4ab3ae61588fdcd8e8b3.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-1c1fceaa57af467b9f4f6d86d327a56e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-1c1fceaa57af467b9f4f6d86d327a56e.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-be9d20c843af4dfe80b68eedfa3bf36e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-be9d20c843af4dfe80b68eedfa3bf36e.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-0387eefb97a94e0bb1f0dddecb7d53a7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-0387eefb97a94e0bb1f0dddecb7d53a7.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-9778448a533c45e9af0d23d22bab3b7c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-9778448a533c45e9af0d23d22bab3b7c.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-acbdfb374d814a0f903d327887d55527: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-acbdfb374d814a0f903d327887d55527.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-0b3028b14999459eaf5a24865c8748de: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-0b3028b14999459eaf5a24865c8748de.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-98e8fb61c783463f97493c5cd4a80885: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-98e8fb61c783463f97493c5cd4a80885.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-671478b305404e63b2b0b44b047e13e6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-671478b305404e63b2b0b44b047e13e6.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-25e09a1ff973426eb8ce967c5e736e0b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-25e09a1ff973426eb8ce967c5e736e0b.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-59da90d09078415cbdf4b9c00720b64d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-59da90d09078415cbdf4b9c00720b64d.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-a433f84bdaa84dffa31da94f5cdcf7aa: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-a433f84bdaa84dffa31da94f5cdcf7aa.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-e19bf4715512428086255086b2e41672: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48224 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-e19bf4715512428086255086b2e41672.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-4946265548674666803374a225aa9df9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-4946265548674666803374a225aa9df9.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-e3eb9d5b7a8b44ebb238475057afe626: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-e3eb9d5b7a8b44ebb238475057afe626.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-c90c92d9fce643669842715a93c08a4d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-c90c92d9fce643669842715a93c08a4d.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-44044aac9572432eb2a4db562c2834fe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-086d386866564b12a5973c553252c895: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-44044aac9572432eb2a4db562c2834fe.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-086d386866564b12a5973c553252c895.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-e0dd3ecf5d644c0cbc25a2e22a0c9a46: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-50384b5f16b343c3ac6127a2d68351ed: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-e0dd3ecf5d644c0cbc25a2e22a0c9a46.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-50384b5f16b343c3ac6127a2d68351ed.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-2d3a1b46f5d74726b428b9c2b33b1494: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-2d3a1b46f5d74726b428b9c2b33b1494.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-c0cf007eb24848dcbe1c885832e399bb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-c0cf007eb24848dcbe1c885832e399bb.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-103d9defb4dc4871a76493c23fbd2001: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-103d9defb4dc4871a76493c23fbd2001.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-c2b4cf7389d94ba4bb394f581c5ee8f9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-c2b4cf7389d94ba4bb394f581c5ee8f9.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-ccd435d6de48437b83f4c152f687b15a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-ccd435d6de48437b83f4c152f687b15a.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-993d2f294f3b478bbd382b68097d4a91: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-993d2f294f3b478bbd382b68097d4a91.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-022a26e26db4421c9ba1de780e0b16fe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-0ec17671dcd545b395dbc0821b189f6c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-022a26e26db4421c9ba1de780e0b16fe.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-0ec17671dcd545b395dbc0821b189f6c.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-436c8922c7d949f7b06ea20217fd8576: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-436c8922c7d949f7b06ea20217fd8576.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-a67296c8024e4598a7a280a07e2dce41: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-a67296c8024e4598a7a280a07e2dce41.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-3c92bacf16ff462caa9aedc5ef76c263: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-3c92bacf16ff462caa9aedc5ef76c263.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-f648c01d31ee41a4b2fb279767731d78: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-f648c01d31ee41a4b2fb279767731d78.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-f2ed0ecbb88d4e76afb1d4563ad3ada9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-f2ed0ecbb88d4e76afb1d4563ad3ada9.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-41414b6a074d489f8b0fd4b76a05f361: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-41414b6a074d489f8b0fd4b76a05f361.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-84a7b41e32f44c7597bc38523d7759bd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-84a7b41e32f44c7597bc38523d7759bd.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-05647b5b5c86490ab08497875246500d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-05647b5b5c86490ab08497875246500d.
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [logger.py:43] Received request chatcmpl-c7a0e93b1c5d4f4087de9ad890a6952e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:09 [async_llm.py:270] Added request chatcmpl-c7a0e93b1c5d4f4087de9ad890a6952e.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-001bc39e37b74eab95adee7382eae9ec: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-001bc39e37b74eab95adee7382eae9ec.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-3f6312bfce53439785184a181b93569a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-3f6312bfce53439785184a181b93569a.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-e720c4ed807b4531a72641a8b6c046c2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-e720c4ed807b4531a72641a8b6c046c2.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-7cac991db27d47e782a073c6129f62c7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-7cac991db27d47e782a073c6129f62c7.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-d651e69348844432abbe91c4f79e8ac2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-d651e69348844432abbe91c4f79e8ac2.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-99c88997a9f34596abe8e84c39208515: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-99c88997a9f34596abe8e84c39208515.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-694cb24366e1435fa3a6949c0c1ce168: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-694cb24366e1435fa3a6949c0c1ce168.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-f1e92683744b491eb747c613afd4f8b7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-f1e92683744b491eb747c613afd4f8b7.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-86cbc30915ff480da55bf32e76d77785: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-86cbc30915ff480da55bf32e76d77785.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-8af5b10947914b7cb64d6d5e54f7926a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-8af5b10947914b7cb64d6d5e54f7926a.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-902429a72d7f459cbc97e762cfcbc397: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-902429a72d7f459cbc97e762cfcbc397.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-c8e17e7d59484bba91b112a10b17cb15: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-c8e17e7d59484bba91b112a10b17cb15.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-d791358cb76f48fb8a351a6c6232d5ca: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-d791358cb76f48fb8a351a6c6232d5ca.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-b027a29ad25b481a916c539642e7cc96: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-b027a29ad25b481a916c539642e7cc96.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-57675992870f42a2978a1d5ded214fb9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-57675992870f42a2978a1d5ded214fb9.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-b9d301bac96d41508120105e79665e07: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-b9d301bac96d41508120105e79665e07.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-f408cf97d77e4448896c314afdb70f3c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-f408cf97d77e4448896c314afdb70f3c.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-3f05dcd88f1e4beeb783f6cf50d04042: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-3f05dcd88f1e4beeb783f6cf50d04042.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-d8884bbc99ab4318b797240d4125ee95: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-d8884bbc99ab4318b797240d4125ee95.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-de5cf11bc7b648e69e5ac3edf9bbcb31: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-de5cf11bc7b648e69e5ac3edf9bbcb31.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-1790140e6b96403d8b955f6bf63cb856: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-f1ae23b982ef4706b4ce1dbbc65bd7a0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-e80fc14dcc844466a13e41cabcb38060: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-1790140e6b96403d8b955f6bf63cb856.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-f1ae23b982ef4706b4ce1dbbc65bd7a0.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-e80fc14dcc844466a13e41cabcb38060.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-f6268f6298b243fd87e9df0f83a0f268: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-f6268f6298b243fd87e9df0f83a0f268.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-a753ca605d9c454ea08814da797abd81: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-a753ca605d9c454ea08814da797abd81.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-2bff300d2b394248977248054bdcf557: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-2bff300d2b394248977248054bdcf557.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-a3885f6af742448d919ce0dc123c324e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-a3885f6af742448d919ce0dc123c324e.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-7c74ed728e6c4e7b9f52ee7c95e06b9d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-7c74ed728e6c4e7b9f52ee7c95e06b9d.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-95d359094d2e4d248e7127749a4e5390: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-95d359094d2e4d248e7127749a4e5390.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-6f39ed1517ad4f5a984055751db409a2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-6f39ed1517ad4f5a984055751db409a2.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-ffdb44bd2a7c471a96bbaefa52020a7f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-ffdb44bd2a7c471a96bbaefa52020a7f.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-3465a6716b174546a7624bee1636a233: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-3465a6716b174546a7624bee1636a233.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-328c483ce49046e59acd4956a470a09d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-328c483ce49046e59acd4956a470a09d.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-8b23dd1ea37e4df4bf9ee8c99a98bef8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-8b23dd1ea37e4df4bf9ee8c99a98bef8.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-47f66f20a4b148d894daf36678b88f6e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-47f66f20a4b148d894daf36678b88f6e.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-2ce47234a1764df992caae4ff4214db6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-517b43a9ef3043e9aebc054d75243510: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-2ce47234a1764df992caae4ff4214db6.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-517b43a9ef3043e9aebc054d75243510.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-b8c1d73150c442afbb9c18462a4cae69: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-35243e53597f4a6ebd49415ef172ce3b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-b8c1d73150c442afbb9c18462a4cae69.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-35243e53597f4a6ebd49415ef172ce3b.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-88fe683dfdb241b0a481499a8df19204: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-88fe683dfdb241b0a481499a8df19204.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-66f3088b72aa42ab8a465d9dc40cf0f6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-66f3088b72aa42ab8a465d9dc40cf0f6.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-49acd5c227db4126af129bfaf74880c3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-49acd5c227db4126af129bfaf74880c3.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-8b7681e47a9c465ebe59851c1939c4c4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-8b7681e47a9c465ebe59851c1939c4c4.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-3ecfab26338843448cdc4d6d6c29d7c5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-3ecfab26338843448cdc4d6d6c29d7c5.
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [logger.py:43] Received request chatcmpl-40424b4476cd4463ba0aede48a9c98bc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:10 [async_llm.py:270] Added request chatcmpl-40424b4476cd4463ba0aede48a9c98bc.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-7c5210c1a2374978abb667aae58608e1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-7c5210c1a2374978abb667aae58608e1.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-85e689aa56cc4badaabf7e2ca27b5ac7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-85e689aa56cc4badaabf7e2ca27b5ac7.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-67bfd361196048898c748e6b174159ec: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-67bfd361196048898c748e6b174159ec.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-16a1644807ef48f08509b253658dd9df: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-16a1644807ef48f08509b253658dd9df.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-c9453e8c4f58489da5566fb196072792: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-c9453e8c4f58489da5566fb196072792.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-1a3df03688624673b9a6f902b8836d3a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-1a3df03688624673b9a6f902b8836d3a.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-bd229b72132b4078a97194393629d4a0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-bd229b72132b4078a97194393629d4a0.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-2c51bbee819040ef87703c20be82d46c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-2c51bbee819040ef87703c20be82d46c.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-2eec8a8ce6dc41d5813038ec5b9055ea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-2eec8a8ce6dc41d5813038ec5b9055ea.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-0752c0e56fda496e8e2f628310efcdd5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-0752c0e56fda496e8e2f628310efcdd5.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-3533eaf5ef7b4c6e9634d1fb720bc30b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-3533eaf5ef7b4c6e9634d1fb720bc30b.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-3401e9467bb241c8afad9241f9560feb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-3401e9467bb241c8afad9241f9560feb.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-dfa3baedcd494630bc598e04218f73c4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-dfa3baedcd494630bc598e04218f73c4.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-442660ad8f0941e1b9fe7a01aa424971: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-442660ad8f0941e1b9fe7a01aa424971.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-fef0492285444ec6a2b553a7419a793a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-fef0492285444ec6a2b553a7419a793a.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-7edb12f9371843b6ae5715b4a740a182: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-7edb12f9371843b6ae5715b4a740a182.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-78ca815d082f4581bc6e64deecf42a8b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-78ca815d082f4581bc6e64deecf42a8b.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-ca06b7d2eb354939b91d792fc2d6d288: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-ca06b7d2eb354939b91d792fc2d6d288.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-184a2b85165e4d58b299021692f9f0c1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-184a2b85165e4d58b299021692f9f0c1.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-0f9bc23da17e4d7981cc2f9041ae3472: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-0f9bc23da17e4d7981cc2f9041ae3472.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-60a726c683a14e869516ee64ce941661: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-60a726c683a14e869516ee64ce941661.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-32b159253b5349f08ff26e0a32743ddb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-32b159253b5349f08ff26e0a32743ddb.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-eb08d588dbc040e5b25c4f44c232131b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-eb08d588dbc040e5b25c4f44c232131b.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-b42a5d44455c4076b59f901509f12820: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-b42a5d44455c4076b59f901509f12820.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-0c9a24d53258425d90786b102448b082: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-0c9a24d53258425d90786b102448b082.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-b6d15f2994344c1e9a3e73d7e6104e64: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-b6d15f2994344c1e9a3e73d7e6104e64.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-60c93c8eb8784c5a878bb3afcf661e5d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-60c93c8eb8784c5a878bb3afcf661e5d.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-d6c51ff51f1c41ada41a5aa76ee5370c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-d6c51ff51f1c41ada41a5aa76ee5370c.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-08c1b625fd724314b08c561c73d012a9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-08c1b625fd724314b08c561c73d012a9.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-d506e4b01277462fb705720f7fff27cf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-d506e4b01277462fb705720f7fff27cf.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-e95cea311e4745a78641282c6a426ce1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-e95cea311e4745a78641282c6a426ce1.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-7a4631bd3e6c48759501fadd1846887f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-7a4631bd3e6c48759501fadd1846887f.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-12014b6b864b45c8a0c31eeb28e6b580: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-12014b6b864b45c8a0c31eeb28e6b580.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-646733cbefe244babfc933f2ce761f0d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-646733cbefe244babfc933f2ce761f0d.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-e49500e03fa24f398d611db27f3b921b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-99a5b3161dd94308a79f9b7bf3f57d4d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-e49500e03fa24f398d611db27f3b921b.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-99a5b3161dd94308a79f9b7bf3f57d4d.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-38311165b8ca40ef95d74d0c99a357fe: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-9d1435c1e30740dc8401d63c5966fec3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-38311165b8ca40ef95d74d0c99a357fe.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-9d1435c1e30740dc8401d63c5966fec3.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-e2777318a5b34b9d92dcb6b2e2ac4694: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-e2777318a5b34b9d92dcb6b2e2ac4694.
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [logger.py:43] Received request chatcmpl-cbb81ef3aaa746429e96aa8f280883f3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:11 [async_llm.py:270] Added request chatcmpl-cbb81ef3aaa746429e96aa8f280883f3.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-88e2a06943824c96a112a5463780a526: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-88e2a06943824c96a112a5463780a526.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-dac11a9ec63b47d2b2b891bd08681294: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-dac11a9ec63b47d2b2b891bd08681294.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-c24bbd83a7e047ecbddff4ef0e0f038d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-c24bbd83a7e047ecbddff4ef0e0f038d.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-5fd250a86ca349a88db3b211f7a7f70f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-5fd250a86ca349a88db3b211f7a7f70f.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-501579894d12447a97f285ef79628e8e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-501579894d12447a97f285ef79628e8e.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-c612e458e44f4e07bf1f2aa79f3e399c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-c612e458e44f4e07bf1f2aa79f3e399c.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-996b31870fa643b1a763cb0300e9ffd6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-996b31870fa643b1a763cb0300e9ffd6.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-97b0b4da1f1846b383969f729da4aa1a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-97b0b4da1f1846b383969f729da4aa1a.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-4f7824b040ac460ea487a7fc3c069a5c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-4f7824b040ac460ea487a7fc3c069a5c.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-25c5dd85a657414488b2f9ede09ed77b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-25c5dd85a657414488b2f9ede09ed77b.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-882aa264959f41b6b151d077766ba70c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-882aa264959f41b6b151d077766ba70c.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-5faa7b2545014be0bff89993e1353637: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-5faa7b2545014be0bff89993e1353637.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-bb37d26bcec242d7a85ce6c558601a6e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-bb37d26bcec242d7a85ce6c558601a6e.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-2ada162774ee42a6b2aa4e750eeb058b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-2ada162774ee42a6b2aa4e750eeb058b.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-0b33a23026bb489ca31c15e919ac8f58: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-0b33a23026bb489ca31c15e919ac8f58.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-474f5570dd744068bdd72de2dd071441: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-474f5570dd744068bdd72de2dd071441.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-5df47588210c4b5b8217990ba9cc27e0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-5df47588210c4b5b8217990ba9cc27e0.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-e3ec711d405240279e2da9336b04c9ef: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-e3ec711d405240279e2da9336b04c9ef.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-e1d38638385a4c4bb0070519f5955809: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-e1d38638385a4c4bb0070519f5955809.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-7f553ef740974593a1942f4830906fa3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-c9cdb5e727d3424693ff85f31de8dfdf: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-7f553ef740974593a1942f4830906fa3.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-c9cdb5e727d3424693ff85f31de8dfdf.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-e192de3dd2514c43bbd87e377b1bceef: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-049713ac67cf473698dd4a565cd18418: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-abcf1e6d875d48e6aea31de85acaf4d2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-e192de3dd2514c43bbd87e377b1bceef.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-049713ac67cf473698dd4a565cd18418.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-abcf1e6d875d48e6aea31de85acaf4d2.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-3a8ff4d55768455194c904d10170ae9b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-3a8ff4d55768455194c904d10170ae9b.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-8fbf1c70aa5848ad91c31cb0411093a7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49478 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-8fbf1c70aa5848ad91c31cb0411093a7.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-96950d39715d4dcbb7e5b82dd0fed8d9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-96950d39715d4dcbb7e5b82dd0fed8d9.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-a315126a07974ce88567578c00306242: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-a315126a07974ce88567578c00306242.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-dfb88c450b404a97a4f46ad14e326f79: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-7c6843bf435e455d9d1425e0a3b9c996: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-dfb88c450b404a97a4f46ad14e326f79.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-7c6843bf435e455d9d1425e0a3b9c996.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-79688d6cc27d4c9ab409840ac1bf9c59: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-79688d6cc27d4c9ab409840ac1bf9c59.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-ae8f23d0ac864efca8dc4388ff543df2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-ae8f23d0ac864efca8dc4388ff543df2.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-733052a37a5a4bf3877a16743ff7c869: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-733052a37a5a4bf3877a16743ff7c869.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-a860a6ecc609442893b33609a902d833: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-a860a6ecc609442893b33609a902d833.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-c5ac191476f6427e9bdca46943807e74: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-c5ac191476f6427e9bdca46943807e74.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-1f63a39b9a0846f391a57b66b173fba8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-1f63a39b9a0846f391a57b66b173fba8.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-f50f86ddfeb84a9d802cc488e1b79802: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-f50f86ddfeb84a9d802cc488e1b79802.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-114b800411ba4a42997e804bab6ac071: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-114b800411ba4a42997e804bab6ac071.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-b8d00666f9244ef79dcf835497245275: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-b8d00666f9244ef79dcf835497245275.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-3911c27014c3468c8e928dd44ec2017b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-3911c27014c3468c8e928dd44ec2017b.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-336cd3a708484cbaaaeefbbdb3e5fd6d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-336cd3a708484cbaaaeefbbdb3e5fd6d.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-eff8d0781e4d4b3191dd9b161917a038: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-eff8d0781e4d4b3191dd9b161917a038.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-902d894d3e1b48ddb7b145136240e305: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49616 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-902d894d3e1b48ddb7b145136240e305.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-7a5dc67cf5f2442b81e6a4275ec1f032: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-7a5dc67cf5f2442b81e6a4275ec1f032.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-b0796ac437274e3d813f14fa859edb7d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-b0796ac437274e3d813f14fa859edb7d.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-e21e6803d5354c298a2f7969fff59b18: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-e21e6803d5354c298a2f7969fff59b18.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-fd5fd5982edc4cd9b19b474678b7b43c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-fd5fd5982edc4cd9b19b474678b7b43c.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-7ffd8ef80705454c8e4f709ef80a3dcf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-7ffd8ef80705454c8e4f709ef80a3dcf.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-c375d45a2be04fc690b13fe79d6ba651: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-c375d45a2be04fc690b13fe79d6ba651.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-dfd5ea112c7a4622a41afb422c35ba69: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-dfd5ea112c7a4622a41afb422c35ba69.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-8485447e242643eca693b804630b4d18: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-8485447e242643eca693b804630b4d18.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-fc1be1d4b5a94b75be22fbce5cd8438c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-fc1be1d4b5a94b75be22fbce5cd8438c.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-3ced467ee82848fbb37cc11cc150ab55: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:49708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-3ced467ee82848fbb37cc11cc150ab55.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-9230e94865a848fc8efea4bd0ae70556: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-9230e94865a848fc8efea4bd0ae70556.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-7ced3943fca74bee8ecd9cab4187d6c2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-7ced3943fca74bee8ecd9cab4187d6c2.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-94fff9d50ffd4441826a47cc914a7562: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-94fff9d50ffd4441826a47cc914a7562.
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [logger.py:43] Received request chatcmpl-200bd1819e8049e1979ce9c9c0d6171f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:12 [async_llm.py:270] Added request chatcmpl-200bd1819e8049e1979ce9c9c0d6171f.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-2318141613a14e60bb9e1bf61c90b04c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-2318141613a14e60bb9e1bf61c90b04c.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-79cc90ead14b45f1a0920efe9750c627: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-79cc90ead14b45f1a0920efe9750c627.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-549a42e0bd154beb8ac9faef68d646eb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-549a42e0bd154beb8ac9faef68d646eb.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-4c48fca9cb5b4e03acd19607277e181f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-4c48fca9cb5b4e03acd19607277e181f.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-ea079b5c26564ac5b68e81802a3c3332: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-7b5dc611564b4e3f8e9c27a07a790435: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-ea079b5c26564ac5b68e81802a3c3332.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-7b5dc611564b4e3f8e9c27a07a790435.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-bf0f1765a48f45fd8854032d1aed3e8b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-bf0f1765a48f45fd8854032d1aed3e8b.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-637b8312c0254e1ba823d0229189ce7b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-637b8312c0254e1ba823d0229189ce7b.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-e389540a3482432c9a15f4da1b89d305: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-e389540a3482432c9a15f4da1b89d305.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-ac29c2b594d44de48c5559136c5dd510: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-8693624c737d424faf4640ccbfe92061: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-f54e6a9833694e82832045809a5d17ca: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-238b5ce3eb9f4c9ab462b893dfc13a2b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-ac29c2b594d44de48c5559136c5dd510.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-8693624c737d424faf4640ccbfe92061.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44900 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-f54e6a9833694e82832045809a5d17ca.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-238b5ce3eb9f4c9ab462b893dfc13a2b.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-540813c2c6184be5b137fefcc7bff6af: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-35ef40d7c703495fb7cf4a63963dc493: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-540813c2c6184be5b137fefcc7bff6af.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-35ef40d7c703495fb7cf4a63963dc493.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-b2f5bde5f3cd41de8c4c899f2fdfa8f8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-b2f5bde5f3cd41de8c4c899f2fdfa8f8.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-fe64cbfbd1fb4ad995a50329bcd3db43: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-fe64cbfbd1fb4ad995a50329bcd3db43.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-d4721338877745f785f9909e00d61229: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-d4721338877745f785f9909e00d61229.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-d11161f7a0a84790a3c1475152c2b1c0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-d11161f7a0a84790a3c1475152c2b1c0.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-bb65252c5334468996d17ccf67d8408a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-bb65252c5334468996d17ccf67d8408a.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-ea5016c0388c497a85a0b07cd99e3e15: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-ea5016c0388c497a85a0b07cd99e3e15.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-21cc71a89c1a499d8b74d38795eb83e9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-21cc71a89c1a499d8b74d38795eb83e9.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-f58d0028126b4dd480ba5146b60f39c6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-f58d0028126b4dd480ba5146b60f39c6.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-879efc8bd558403c8a5901fdfa3f4f6d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-879efc8bd558403c8a5901fdfa3f4f6d.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-15646503a1f6474bab9a62cb0aafdf77: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-15646503a1f6474bab9a62cb0aafdf77.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-852d5c4ab3ec4f509bdf021cd644852e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-852d5c4ab3ec4f509bdf021cd644852e.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-47502f7221ee453296e2f111e0fad40a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-47502f7221ee453296e2f111e0fad40a.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-f331a27c7fdf4542920e78c8a8f0e63a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-f331a27c7fdf4542920e78c8a8f0e63a.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-425a4b2b5cde41be9beb0acaf14254df: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45040 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-425a4b2b5cde41be9beb0acaf14254df.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-f282de7c96f444bb87b281c88d27bee3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-f282de7c96f444bb87b281c88d27bee3.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-1eeec156f2d04d2d877009974c7171ff: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-1eeec156f2d04d2d877009974c7171ff.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-14646192b4ca461ea16472c2c1bdcc64: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-14646192b4ca461ea16472c2c1bdcc64.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-5b383bab577f4d499b18786936917be1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-5b383bab577f4d499b18786936917be1.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-f78d6c0a54b248b1aad0e65c05503077: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-f78d6c0a54b248b1aad0e65c05503077.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-a4af9d0ea08d4f52ad2d5c2c903089a9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-a4af9d0ea08d4f52ad2d5c2c903089a9.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-d61db0dd84604533b7fe6454c5d1e580: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-d61db0dd84604533b7fe6454c5d1e580.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-3763a72bc0c64cc7b915cea8667634f7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-3763a72bc0c64cc7b915cea8667634f7.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-7d7ed9cc4dbf44f790a5bf96a32377fe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-7d7ed9cc4dbf44f790a5bf96a32377fe.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-55ca4abba785421289cb9331d12a8a01: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-55ca4abba785421289cb9331d12a8a01.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-8f2dd9e692d5477cb41b0673cb77226f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-8f2dd9e692d5477cb41b0673cb77226f.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-d0693b8cc2824aa980a3a519038689c5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-d0693b8cc2824aa980a3a519038689c5.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-2c2ac707f6514155a9aa0af138d17561: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-2c2ac707f6514155a9aa0af138d17561.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-20679770c4da495ea1623363799ff6bf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-20679770c4da495ea1623363799ff6bf.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-93f97f68795d41f3b70e7cacfc6ec137: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-93f97f68795d41f3b70e7cacfc6ec137.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-39780bde4dfb4ecaa5f2a7ca918274c1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-39780bde4dfb4ecaa5f2a7ca918274c1.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-59e475bbf9eb4c1b9b794ca52cb74253: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-f1acd56b37c84f93844d42a5a0013dfb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-59e475bbf9eb4c1b9b794ca52cb74253.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-f1acd56b37c84f93844d42a5a0013dfb.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-99aad17db40e4727bbf0f0bfcffef8d8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-99aad17db40e4727bbf0f0bfcffef8d8.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-544809233f6348ca829abf2cf572c8af: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-544809233f6348ca829abf2cf572c8af.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-dc445366f6374cfab22bbec8b1a20ce8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-dc445366f6374cfab22bbec8b1a20ce8.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-53fb46c2d4fd414bb18853ae497c6ab6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-53fb46c2d4fd414bb18853ae497c6ab6.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-cd39e42a8c704ffe8a8e7002f9416ceb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-cd39e42a8c704ffe8a8e7002f9416ceb.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-38607569226846e08ee870784bbe0a4e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-38607569226846e08ee870784bbe0a4e.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-958585749bd54d148197fdb9398bc48e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-958585749bd54d148197fdb9398bc48e.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-7bac18c8e9ee427fab384d2c9c9ec0c8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-7bac18c8e9ee427fab384d2c9c9ec0c8.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-fb30eb8801a44ef6901be7a86bf939e6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-fb30eb8801a44ef6901be7a86bf939e6.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-9f71ae7948dc4d148e725420cfe18677: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-9f71ae7948dc4d148e725420cfe18677.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-dbd1e8b9818b4a8797f800cd944c5999: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-dbd1e8b9818b4a8797f800cd944c5999.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-7e4352f54eb4438f84fb12d546201612: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-7e4352f54eb4438f84fb12d546201612.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-7f9b47dc473a4a96806ffadefab6fbd7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-7f9b47dc473a4a96806ffadefab6fbd7.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-3379f680b34448d6a5dd84d332b52d0b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-3379f680b34448d6a5dd84d332b52d0b.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-6cb06770dd544c538be6f8c13f4ae247: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-6cb06770dd544c538be6f8c13f4ae247.
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [logger.py:43] Received request chatcmpl-1f8b600d82564990a24e836eb2c10394: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:13 [async_llm.py:270] Added request chatcmpl-1f8b600d82564990a24e836eb2c10394.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-0a10bde5f4e94b758a67f2d204e598c4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-0a10bde5f4e94b758a67f2d204e598c4.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-b5b4a4a6d8cb4515bba8091db9797d1c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-b5b4a4a6d8cb4515bba8091db9797d1c.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-937ae98ac9024fd0b04cc302e4e7d9a3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-937ae98ac9024fd0b04cc302e4e7d9a3.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-f479da69419340d3af77b8d55e8c76e3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45418 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-f479da69419340d3af77b8d55e8c76e3.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-b481969c2cea40d2aba6ace0f81d9edc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-b481969c2cea40d2aba6ace0f81d9edc.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-848e1aff26a64f34b7fbcd0eeb2c0372: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-848e1aff26a64f34b7fbcd0eeb2c0372.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-c74d54a813ad4216b36222d7e541cbd0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-c74d54a813ad4216b36222d7e541cbd0.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-7f03d737e5fa45729d838fb82dc0bb91: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-7f03d737e5fa45729d838fb82dc0bb91.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-b10840083669401f87cfe07a4e4d4929: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-b10840083669401f87cfe07a4e4d4929.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-b662efe7d89446c2ab68cf02a9eb447e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-b662efe7d89446c2ab68cf02a9eb447e.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-3150b61b61e94e61bf4937f60164b209: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-3150b61b61e94e61bf4937f60164b209.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-cea099f2eb9c4f44bf0cfe6af98c27be: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-cea099f2eb9c4f44bf0cfe6af98c27be.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-4b1ba9287de44ea88dbcb9a41ea8aa57: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-4b1ba9287de44ea88dbcb9a41ea8aa57.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-283e7cbbc1e8477ea3a9a8eb2517df09: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-283e7cbbc1e8477ea3a9a8eb2517df09.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-1a6280126c9948899dade6dac6ba8c11: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-1a6280126c9948899dade6dac6ba8c11.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-616e0a0e1f604f849185e7569cddb41d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-616e0a0e1f604f849185e7569cddb41d.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-4c4ca61189c6452aae4a40c02f5f56be: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-4c4ca61189c6452aae4a40c02f5f56be.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-57c6b084e1874dbd84b69c346f5eda56: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-57c6b084e1874dbd84b69c346f5eda56.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-a57becff4881482796ba425341f2ea31: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-a57becff4881482796ba425341f2ea31.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-4b14fd2a0eeb442ead298b8012538811: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-4b14fd2a0eeb442ead298b8012538811.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-3474dc20845548c28ed3951a1646e5b1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45552 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-3474dc20845548c28ed3951a1646e5b1.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-f72559b0e5a145fc9a9050f5f2d7aaa6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-f72559b0e5a145fc9a9050f5f2d7aaa6.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-3f79c832f3ac4e288dadf44b7993d433: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-3f79c832f3ac4e288dadf44b7993d433.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-5afe9509b2e6453e80bc6d42f92cc62a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-5afe9509b2e6453e80bc6d42f92cc62a.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-82c3693b46c8413f9eb71e675d9b5507: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-82c3693b46c8413f9eb71e675d9b5507.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-0a3b0a9382d740ab83ed1611f8346dae: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-0a3b0a9382d740ab83ed1611f8346dae.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-83b4258ca25744f792e5b097dd3ef9fb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-83b4258ca25744f792e5b097dd3ef9fb.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-6dd9f64f69794911906cfae55552487d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-6dd9f64f69794911906cfae55552487d.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-6d37ace21a874928abd7a3c92a99b2fb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-dd0789c738de46c18a22b545cd61b107: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-6d37ace21a874928abd7a3c92a99b2fb.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-dd0789c738de46c18a22b545cd61b107.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-bb439e45fead4ed0b2fa68629f1a2d95: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-8b45304fe68844af86d206ef9236ef15: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-bb439e45fead4ed0b2fa68629f1a2d95.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-8b45304fe68844af86d206ef9236ef15.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-4679f4be1e314e888c622b1b031267bb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-4679f4be1e314e888c622b1b031267bb.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-68a09a965d02499885396bc2039ee010: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-68a09a965d02499885396bc2039ee010.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-0cf07e9ec7be4fa197dc59b49a426abd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-0cf07e9ec7be4fa197dc59b49a426abd.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-52668879b6d4485d9b43c07a8df049c3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-52668879b6d4485d9b43c07a8df049c3.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-746170faeb81498b8e4279d8c282dda9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-cbef24065d184dbb83f5364a1611c051: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-746170faeb81498b8e4279d8c282dda9.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-cbef24065d184dbb83f5364a1611c051.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-2faf1f8be9cb47e28421ac411b31c43f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-2faf1f8be9cb47e28421ac411b31c43f.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-ec976b4eb12148e0a5110b9cc3ad76c8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-ec976b4eb12148e0a5110b9cc3ad76c8.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-b996d6526d934be98277902b54faa6ae: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-b996d6526d934be98277902b54faa6ae.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-1ff6341891694fda90168898de8712c8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-1ff6341891694fda90168898de8712c8.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-199a2dcde0ac490cb22ffbc77789ce09: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-199a2dcde0ac490cb22ffbc77789ce09.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-42a0fb19a5e846c78814f09c81db5387: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-42a0fb19a5e846c78814f09c81db5387.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-a4501dde62f6440fabee835bc9b795bc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-a4501dde62f6440fabee835bc9b795bc.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-8b98ad9104964e8ea0383f06967be064: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-8b98ad9104964e8ea0383f06967be064.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-35d75bcffb654798b669611358832376: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-35d75bcffb654798b669611358832376.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-434f195ecd324ee791dd8e8725e6926f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45778 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-434f195ecd324ee791dd8e8725e6926f.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-d93472057ad9443aa60181eddd99c97f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-d93472057ad9443aa60181eddd99c97f.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-c1f550106c9b43e4bf95abe0ef8dfd14: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-c1f550106c9b43e4bf95abe0ef8dfd14.
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [logger.py:43] Received request chatcmpl-c1fc998daf0142e1999ed8de03168200: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:14 [async_llm.py:270] Added request chatcmpl-c1fc998daf0142e1999ed8de03168200.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-98a222926ae34eb1966b956e70ccc4ab: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-98a222926ae34eb1966b956e70ccc4ab.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-ae74e63d06434c38bb1d4f50d0982ac4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-ae74e63d06434c38bb1d4f50d0982ac4.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-70edc07f5ca54e6da40ae6a3b54d9b1a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45830 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-70edc07f5ca54e6da40ae6a3b54d9b1a.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-e6627d8e6448491e98e8ef91ab0f9b56: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-e6627d8e6448491e98e8ef91ab0f9b56.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-2e65831e86a84b92ab18c397ce221fe9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-2e65831e86a84b92ab18c397ce221fe9.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-20e16c1e8b1a4beb9bf6c20ad9d66b6d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-20e16c1e8b1a4beb9bf6c20ad9d66b6d.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-a706d2f2b81446e096da6263eb7d2200: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-a706d2f2b81446e096da6263eb7d2200.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-01707c6ebe9240888451a77cd7a3127d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-01707c6ebe9240888451a77cd7a3127d.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-985823cb644f4f46bda88566220879a7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-985823cb644f4f46bda88566220879a7.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-498ab6ab61544faba7ef8ed2fece8340: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-498ab6ab61544faba7ef8ed2fece8340.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-157f062ede6948658eb727c1a3fbc236: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45908 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-157f062ede6948658eb727c1a3fbc236.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-e19195feec1e4f9e93265051dd513b8a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-e19195feec1e4f9e93265051dd513b8a.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-49f547728c5f420fafc6ae6c96cec4eb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-49f547728c5f420fafc6ae6c96cec4eb.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-ff00f1b02f7c4db5827273f4d0ccd888: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-ff00f1b02f7c4db5827273f4d0ccd888.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-26d96603000a44cf85282b8ef33e9cec: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-26d96603000a44cf85282b8ef33e9cec.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-c8bb05f628f34967b71d272c608be726: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-c8bb05f628f34967b71d272c608be726.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-52ef8c1204c9492d9ae1d066d5d9f434: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-52ef8c1204c9492d9ae1d066d5d9f434.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-3f65d2ef26034707917489c8a95e34a9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-3f65d2ef26034707917489c8a95e34a9.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-72d4335555f84717be896439e63af8fa: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-72d4335555f84717be896439e63af8fa.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-1cb951aa3dd84925b078bd99639bff0c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45980 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-1cb951aa3dd84925b078bd99639bff0c.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-31c60375d1884a23ad3d47d4fef2de04: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-31c60375d1884a23ad3d47d4fef2de04.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-ca0662496d474a75881b591a6860444b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-ca0662496d474a75881b591a6860444b.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-fc7519b8e99e4cbd9868bcc4e07d3dc0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-fc7519b8e99e4cbd9868bcc4e07d3dc0.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-3168ac33ab3b4944897a41d9aa98f0f9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-3168ac33ab3b4944897a41d9aa98f0f9.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-f71444d38d06407e920414cfce4c1ad2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-f71444d38d06407e920414cfce4c1ad2.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-2317c4928d5c4ae382dab500ce2449b1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-2317c4928d5c4ae382dab500ce2449b1.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-1aff04e76fde40fba1b3653c3758172b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46040 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-1aff04e76fde40fba1b3653c3758172b.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-97e0a28bcffd443f80425a0831a933aa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-97e0a28bcffd443f80425a0831a933aa.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-e34614aee33647eab39ba30a9e9ad945: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-e34614aee33647eab39ba30a9e9ad945.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-454530fed4bc48c0ac9081727e5595fb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-454530fed4bc48c0ac9081727e5595fb.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-a84d7a7231144bf79e03073c0f73636b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-a84d7a7231144bf79e03073c0f73636b.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-711f396dde814b0a929f143d35016b68: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-711f396dde814b0a929f143d35016b68.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-8808af24a01944a0b6ff6236f4a17f4d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-8808af24a01944a0b6ff6236f4a17f4d.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-df8114929bff4cee9eadfefb1d80879c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-df8114929bff4cee9eadfefb1d80879c.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-126ff7e712f94f18b9de0250f72b79ae: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-126ff7e712f94f18b9de0250f72b79ae.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-0127fd92aac545219592b974c035fbfb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-59f059fe888e4fa7999934abcd7c5df9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-0127fd92aac545219592b974c035fbfb.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-59f059fe888e4fa7999934abcd7c5df9.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-7df538c3086b4b8c959396c12ac7f4b4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-7df538c3086b4b8c959396c12ac7f4b4.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-08afa99d2ac24c0ebbeb9ac4c29ab260: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-08afa99d2ac24c0ebbeb9ac4c29ab260.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-223ec5d9079d47c496972f015c5c5cf3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-223ec5d9079d47c496972f015c5c5cf3.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-70c399a11318468386d718daeb7fdc8e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-70c399a11318468386d718daeb7fdc8e.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-b8753c073a794e0aa6fce42782831d55: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-b8753c073a794e0aa6fce42782831d55.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-2062c4dc735548aba737e500e3ba00f1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-2062c4dc735548aba737e500e3ba00f1.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-6980ca1eff4b4e509174eaf168bc6a9a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-6980ca1eff4b4e509174eaf168bc6a9a.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-ae1d986ccebd4094ab8875283c3b8a48: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-ae1d986ccebd4094ab8875283c3b8a48.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-66ffb639796a484cb38aa0402c3384db: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-66ffb639796a484cb38aa0402c3384db.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-7739b5cd99264ba08e0bcd0d9326bff9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-7739b5cd99264ba08e0bcd0d9326bff9.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-7b1f5b85b73a4635b2cce639b9658a04: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-7b1f5b85b73a4635b2cce639b9658a04.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-066418a8f0f049d59b128b576b9ce104: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-066418a8f0f049d59b128b576b9ce104.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-997929d4c6974f2c8eb7fd4d2184507f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46260 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-997929d4c6974f2c8eb7fd4d2184507f.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-8bf8bc5c944d437e9152f56aa820a86b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-8bf8bc5c944d437e9152f56aa820a86b.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-bc82ad4b17a2400e8ffb074dda23475d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-f67fb20e210e487aa1ee5ba181c3d9bc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-bc82ad4b17a2400e8ffb074dda23475d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-f67fb20e210e487aa1ee5ba181c3d9bc.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-33abb1167dc845598db408c1938f738c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-33abb1167dc845598db408c1938f738c.
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [logger.py:43] Received request chatcmpl-e0d1734ec8734cdcbe58a1cebd64f50b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:15 [async_llm.py:270] Added request chatcmpl-e0d1734ec8734cdcbe58a1cebd64f50b.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-e0c7b1de2a4d49c99915686a4354e94a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-e0c7b1de2a4d49c99915686a4354e94a.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-d883bd8067a74ea188df49d6b1fc2855: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-d883bd8067a74ea188df49d6b1fc2855.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-ae2048f733804bceb949519bbe4a62dd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-ae2048f733804bceb949519bbe4a62dd.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-8d2f240ee07345bcad9b96ecab4a2497: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-8d2f240ee07345bcad9b96ecab4a2497.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-4a57f2518633453fb7ce9bb895ec70f3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-4a57f2518633453fb7ce9bb895ec70f3.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-220a81bfcd7341c3ac53161946899c7c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-220a81bfcd7341c3ac53161946899c7c.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-637857ee37684f9a8d6a1dba02163129: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-637857ee37684f9a8d6a1dba02163129.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-db08cacf20a34431a4086e9d106ea899: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-db08cacf20a34431a4086e9d106ea899.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-c8f74a8ef30142c28e6c9c2f74b264fb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-c8f74a8ef30142c28e6c9c2f74b264fb.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-c7d2b20a83814b53b8e84617e3f07ac5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-c7d2b20a83814b53b8e84617e3f07ac5.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-7582280eecbe4adb942d9230559c7c91: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-7582280eecbe4adb942d9230559c7c91.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-af7d091bdfe8431d890aaba723059db4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-af7d091bdfe8431d890aaba723059db4.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-ba7b65c40b2543c3bdeb7e339f549e9b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-ba7b65c40b2543c3bdeb7e339f549e9b.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-9d3c7eec34f34441bd0034617e4d7949: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-9d3c7eec34f34441bd0034617e4d7949.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-0d51c1bf686a429d9c2afef998f44f59: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-0d51c1bf686a429d9c2afef998f44f59.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-f0f6aa0f0392456c92760f61e69f6843: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-f0f6aa0f0392456c92760f61e69f6843.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-707301ab92be46038c639855ce4df476: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-707301ab92be46038c639855ce4df476.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-a6ccf1ef34994647965e5070ede1a167: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-a6ccf1ef34994647965e5070ede1a167.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-fdadc6b6fc74418fb51de0f69ea17273: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-fdadc6b6fc74418fb51de0f69ea17273.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-e7c3e4cbc10c4c63a325060e2565c96c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-e7c3e4cbc10c4c63a325060e2565c96c.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-90bffd0d9ed84e0d954cca81ade136cc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-90bffd0d9ed84e0d954cca81ade136cc.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-96bef6127eae43c39395c84b1fce5eaf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-96bef6127eae43c39395c84b1fce5eaf.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-d4485cd10f184576ade0c6f078ea1540: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-d4485cd10f184576ade0c6f078ea1540.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-d20c4b79f43c436b8b130dba3f5c84be: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-d20c4b79f43c436b8b130dba3f5c84be.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-bfbf264fc55040f7bff9f321e49aa3f3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-bfbf264fc55040f7bff9f321e49aa3f3.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-28e683d8cd4e4d08a5e2abcf0b0b5802: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-28e683d8cd4e4d08a5e2abcf0b0b5802.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-d8c94fad14094f8ab59d7f851e79708e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-d8c94fad14094f8ab59d7f851e79708e.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-1102cfcff29e40d0844914a8c964a330: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-1102cfcff29e40d0844914a8c964a330.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-fa92175ec9924c3daf66be0e5b88c888: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-fa92175ec9924c3daf66be0e5b88c888.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-ef53b73da29344ea93ac2443dd2ce654: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-ef53b73da29344ea93ac2443dd2ce654.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-b8af309f13ab4ed8840e19a6e742c319: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-b8af309f13ab4ed8840e19a6e742c319.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-75dc8260a34648e784505e1b32cdb45a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-75dc8260a34648e784505e1b32cdb45a.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-bb3c2462f1ed4eb7beb4a29b2d51ed4c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-bb3c2462f1ed4eb7beb4a29b2d51ed4c.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-b39c5fd46a714797be6e9e89d328eb9d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-b39c5fd46a714797be6e9e89d328eb9d.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-b677b884b73b42f6a2ac453205730c48: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-b677b884b73b42f6a2ac453205730c48.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-364f55f86f034d489a92ac2cfe357351: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-364f55f86f034d489a92ac2cfe357351.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-e8632bab417d423db5790622ff56fcfc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-e8632bab417d423db5790622ff56fcfc.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-07a9c4ce6e594f3bbd4bbd1c14ff86e6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-07a9c4ce6e594f3bbd4bbd1c14ff86e6.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-71cdfeee55f94640ba86e8d9a448416b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46662 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-71cdfeee55f94640ba86e8d9a448416b.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-5cafd922c9f14571b51da5ee89e7b166: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46666 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-5cafd922c9f14571b51da5ee89e7b166.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-7d375e972fd8491b9ef09f81d7e923da: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-fd79f3433d354b22a145d3e246cd7c7d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-7d375e972fd8491b9ef09f81d7e923da.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-6c8d7a8d170b47e8802a76aa9f921aff: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-fd79f3433d354b22a145d3e246cd7c7d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-6c8d7a8d170b47e8802a76aa9f921aff.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-f48a7f11bee6478fb2f242c561530932: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-f48a7f11bee6478fb2f242c561530932.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-9e71a988569445359251a126db4fdce9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-9e71a988569445359251a126db4fdce9.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-3144dc349ecb463d9aa75f723f8f2e06: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-3144dc349ecb463d9aa75f723f8f2e06.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-43b82739e2a0472c9b80ee39807f0a66: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-43b82739e2a0472c9b80ee39807f0a66.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-4bb1646734e3436fb370903a284348dd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-4bb1646734e3436fb370903a284348dd.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-7f651c8c365d41d48886b4df089a89cc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-7f651c8c365d41d48886b4df089a89cc.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-192495b82de644e7b6a2aaae97714284: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-192495b82de644e7b6a2aaae97714284.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-c5382bf4186e48dda3c0c05ff44e2b12: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-c5382bf4186e48dda3c0c05ff44e2b12.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-88bca5cfe89848e28f83f72cbba5edee: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46778 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-88bca5cfe89848e28f83f72cbba5edee.
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [logger.py:43] Received request chatcmpl-56c2aed225fa42ab8f8d14f1ef6e43bf: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:16 [async_llm.py:270] Added request chatcmpl-56c2aed225fa42ab8f8d14f1ef6e43bf.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-f44ca5199eb9480fb419203abb59da1a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-f44ca5199eb9480fb419203abb59da1a.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-567f6c5dfa934ece8353ee3a74053285: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-567f6c5dfa934ece8353ee3a74053285.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-3f4c0dce58e54c3d8358114653ca547f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-3f4c0dce58e54c3d8358114653ca547f.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-ca9787cba0114439a1ddab6704f72681: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-ca9787cba0114439a1ddab6704f72681.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-64a3948135074af3afe14b99fb00c2bb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-64a3948135074af3afe14b99fb00c2bb.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-3beb07593ee2473c9835e6e8a637f8c8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-3beb07593ee2473c9835e6e8a637f8c8.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-4fb48ba72dc04a018489c74f532ee6b7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-8d900bd965104946bfff48d80b88ddff: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-4fb48ba72dc04a018489c74f532ee6b7.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-8d900bd965104946bfff48d80b88ddff.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-fc7f72a2e45c4599a8b8b147aebcbba1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-470b48348f6a42a5871d134ca98a2620: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-fc7f72a2e45c4599a8b8b147aebcbba1.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-07c7a3cdf15a4e47963f172afc122b2d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-470b48348f6a42a5871d134ca98a2620.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-07c7a3cdf15a4e47963f172afc122b2d.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-3dc851cb4eb74bc68c3a5c6fbe5c262b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-3dc851cb4eb74bc68c3a5c6fbe5c262b.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-9f013b83f64f4a7cb21b62ce86d49dda: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-c058e22db52540d69aeac9e6b8ce3ec0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [loggers.py:118] Engine 000: Avg prompt throughput: 633.6 tokens/s, Avg generation throughput: 2507.8 tokens/s, Running: 160 reqs, Waiting: 0 reqs, GPU KV cache usage: 44.9%, Prefix cache hit rate: 84.5%
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [loggers.py:118] Engine 001: Avg prompt throughput: 631.3 tokens/s, Avg generation throughput: 2505.6 tokens/s, Running: 159 reqs, Waiting: 0 reqs, GPU KV cache usage: 44.8%, Prefix cache hit rate: 84.0%
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [loggers.py:118] Engine 002: Avg prompt throughput: 618.5 tokens/s, Avg generation throughput: 2501.7 tokens/s, Running: 162 reqs, Waiting: 0 reqs, GPU KV cache usage: 45.0%, Prefix cache hit rate: 84.6%
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [loggers.py:118] Engine 003: Avg prompt throughput: 653.5 tokens/s, Avg generation throughput: 2477.9 tokens/s, Running: 163 reqs, Waiting: 0 reqs, GPU KV cache usage: 45.3%, Prefix cache hit rate: 84.3%
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-ab797de6e38e4768958859c4c5af5256: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-9f013b83f64f4a7cb21b62ce86d49dda.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-c058e22db52540d69aeac9e6b8ce3ec0.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-ab797de6e38e4768958859c4c5af5256.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-e234bf8a32aa48a98bcb01d21388ac21: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-e234bf8a32aa48a98bcb01d21388ac21.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-4c89260aedb24b92a786b7cab628a242: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-b148668e51644ae7a908189ae9482135: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-4c49498f72f3402eb6050506b254669b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-4c89260aedb24b92a786b7cab628a242.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-b148668e51644ae7a908189ae9482135.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-2390d1fe268b482d8ce4da785365e039: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-4c49498f72f3402eb6050506b254669b.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-2390d1fe268b482d8ce4da785365e039.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-17490875040b46c3a0d975d6be7053d6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-17490875040b46c3a0d975d6be7053d6.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-ca97f724941c4465a1134d1919895b53: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-ca97f724941c4465a1134d1919895b53.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-f5615dca1c85455891c0cf555ca7d6c7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-f5615dca1c85455891c0cf555ca7d6c7.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-039f46430ed44e909223fe286e4472e1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-039f46430ed44e909223fe286e4472e1.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-3fc0d916b7934303b37ffa1477a9e574: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-3fc0d916b7934303b37ffa1477a9e574.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-b09d69533bda43b3b50c78840295eada: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-b09d69533bda43b3b50c78840295eada.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-e875a9605e1e40b482e53edc85466e6b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-e875a9605e1e40b482e53edc85466e6b.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-18ccb0ba682e4b3f87e87047ff1f0548: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-18ccb0ba682e4b3f87e87047ff1f0548.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-deb40ff1e36d43998dca6864cf44c2c5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-deb40ff1e36d43998dca6864cf44c2c5.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-676beb1d81a94887951c26c0643ab315: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-676beb1d81a94887951c26c0643ab315.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-69f7669d92784ed1b84fe05ebd25d040: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-69f7669d92784ed1b84fe05ebd25d040.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-534bb52e3a5d489889bf9aa9d4351822: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-534bb52e3a5d489889bf9aa9d4351822.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-4dd985bdd64240419642caa0cc549ec6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-5acb7db3103444e4b6f195f7420aacb3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-c8cc0ae0f3eb44f180f53e7c8e11e7b1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-4dd985bdd64240419642caa0cc549ec6.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-5acb7db3103444e4b6f195f7420aacb3.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-f1ba1fa7eed5427bb8573e4d38b6b2f7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-0c54d060e6b4448194c7ec4ab3e38d3c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-c8cc0ae0f3eb44f180f53e7c8e11e7b1.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-f1ba1fa7eed5427bb8573e4d38b6b2f7.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-0c54d060e6b4448194c7ec4ab3e38d3c.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-b5a088b61922493888d56b7e3f1e3f41: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-db66f9197e384a7f92bb5120b7ec2a47: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-3f323964df244331905ef173ceea339c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-9138e11e559d4e19ab0b00723eac1f82: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-b5a088b61922493888d56b7e3f1e3f41.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-db66f9197e384a7f92bb5120b7ec2a47.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-a86a255cf3c64e49b7ed16703fc05507: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-3f323964df244331905ef173ceea339c.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-cec7e3d947fb407fa40f3f3797cc860d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-9138e11e559d4e19ab0b00723eac1f82.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-6969a319ed034078b3d967d4126f8b8c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-a86a255cf3c64e49b7ed16703fc05507.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-cec7e3d947fb407fa40f3f3797cc860d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-6969a319ed034078b3d967d4126f8b8c.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-eb2f761e071b4a388ff466278ba78544: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-eb2f761e071b4a388ff466278ba78544.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-c2566a161c964096bcbecb436e6def86: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-c2566a161c964096bcbecb436e6def86.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-f10dd70705d847788afe7b37caaed7b8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-f10dd70705d847788afe7b37caaed7b8.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-2c691ff104e342f3b489ecf6ccdad878: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-2c691ff104e342f3b489ecf6ccdad878.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-c147220a4c2f4fddbf7b832474313602: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-c147220a4c2f4fddbf7b832474313602.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-a01dda6b8fe543768bf5ee1ef1f923ae: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-a01dda6b8fe543768bf5ee1ef1f923ae.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-e8198d287906467e86c0464bb87c691b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-e8198d287906467e86c0464bb87c691b.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-77b7ef2db11e4f0eac180f9037109285: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-77b7ef2db11e4f0eac180f9037109285.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-6c6c36b5796a40a19a413e9a8181f69a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-6c6c36b5796a40a19a413e9a8181f69a.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-91df66395c264141b0d1f63bc49178bc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-91df66395c264141b0d1f63bc49178bc.
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [logger.py:43] Received request chatcmpl-9643f66e1a7e4eb798b6bc622c2c779d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47312 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:17 [async_llm.py:270] Added request chatcmpl-9643f66e1a7e4eb798b6bc622c2c779d.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-889ba77a736b4809aa73687216793cb1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-889ba77a736b4809aa73687216793cb1.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-99140e208e854ea2994168f8c4bcd54c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-99140e208e854ea2994168f8c4bcd54c.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-72b92f2ac73849e1a7dbf0331924bdc1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-72b92f2ac73849e1a7dbf0331924bdc1.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-79f05114ac24498f9122b7271c4cfe7d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-79f05114ac24498f9122b7271c4cfe7d.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-ff818c888c9840d59a4a6d78dcbb8bdb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-ff818c888c9840d59a4a6d78dcbb8bdb.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-ac6d9b191c15464fa60a508318078de2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-ac6d9b191c15464fa60a508318078de2.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-32a5e13c342946bfac4495be1c32c519: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-32a5e13c342946bfac4495be1c32c519.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-60d534b462404696b0220bd77317188b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-60d534b462404696b0220bd77317188b.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-a34ddf4f2bce4e37b84b11e14d12bbf9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-a34ddf4f2bce4e37b84b11e14d12bbf9.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-271aa06e38da49afb9d8f92a5c8c1e2a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-271aa06e38da49afb9d8f92a5c8c1e2a.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-83f469d1cc0e4dc493878c3954f31270: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-83f469d1cc0e4dc493878c3954f31270.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-a4b05f317967492d9801762edc4f7453: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-a4b05f317967492d9801762edc4f7453.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-876f8abd6d3c471ea4ba3734ee19561d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47436 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-876f8abd6d3c471ea4ba3734ee19561d.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-2fbd719745b94cf391a0d0319a85a5a9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-2fbd719745b94cf391a0d0319a85a5a9.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-965273a5925e4cb494253e92276cc1f1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-965273a5925e4cb494253e92276cc1f1.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-1eb8157b1e9c48e1b82d238696f81063: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-1eb8157b1e9c48e1b82d238696f81063.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-a1d8ddbb89814a47b251ae905cffa3c2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-a1d8ddbb89814a47b251ae905cffa3c2.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-f9b7a19901fa452e94d1e6f8519792d4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-f9b7a19901fa452e94d1e6f8519792d4.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-b0a34b17842349f7b7a7a6f2fc18ce7e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-b0a34b17842349f7b7a7a6f2fc18ce7e.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-d201649a30574726815177ad1b92a4f4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-d201649a30574726815177ad1b92a4f4.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-33f1ab80f6ec43148fac83bac2b5b83c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-52334924f30c4b4ebc8a9a4e82fc1f29: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-8578be1d8b3c4769860cd66a47130a8b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-33f1ab80f6ec43148fac83bac2b5b83c.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-52334924f30c4b4ebc8a9a4e82fc1f29.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-12b9fe754d9b463e8e0f308ef8793cb5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-8578be1d8b3c4769860cd66a47130a8b.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-41442d0f36f04e3f8c4b8295f136267a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-12b9fe754d9b463e8e0f308ef8793cb5.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-41442d0f36f04e3f8c4b8295f136267a.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-52a52fc332684b27bf03afbb02c9faf9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47534 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-52a52fc332684b27bf03afbb02c9faf9.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-347f56a761604ad29071b86720369d9c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-347f56a761604ad29071b86720369d9c.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-0a40b2f3f8064e51ab0c03346cf787d5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-0a40b2f3f8064e51ab0c03346cf787d5.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-983e83bc3a5648c5b2d4bdbdedce51be: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-983e83bc3a5648c5b2d4bdbdedce51be.
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [logger.py:43] Received request chatcmpl-b6cf6ebb98cf47c6a2a01bfc0e0b954a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:44:18 [async_llm.py:270] Added request chatcmpl-b6cf6ebb98cf47c6a2a01bfc0e0b954a.
[36mllm_server_1  |[0m INFO 07-21 18:44:27 [loggers.py:118] Engine 000: Avg prompt throughput: 110.8 tokens/s, Avg generation throughput: 2665.8 tokens/s, Running: 39 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.7%, Prefix cache hit rate: 84.7%
[36mllm_server_1  |[0m INFO 07-21 18:44:27 [loggers.py:118] Engine 001: Avg prompt throughput: 102.9 tokens/s, Avg generation throughput: 2629.7 tokens/s, Running: 35 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.6%, Prefix cache hit rate: 84.2%
[36mllm_server_1  |[0m INFO 07-21 18:44:27 [loggers.py:118] Engine 002: Avg prompt throughput: 90.5 tokens/s, Avg generation throughput: 2636.0 tokens/s, Running: 36 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.1%, Prefix cache hit rate: 84.8%
[36mllm_server_1  |[0m INFO 07-21 18:44:27 [loggers.py:118] Engine 003: Avg prompt throughput: 103.1 tokens/s, Avg generation throughput: 2707.0 tokens/s, Running: 38 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.1%, Prefix cache hit rate: 84.4%

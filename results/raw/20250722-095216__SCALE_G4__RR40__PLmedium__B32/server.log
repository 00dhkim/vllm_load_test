Attaching to vllm_load_test_llm_server_1
[36mllm_server_1  |[0m INFO 07-21 18:24:28 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:24:32 [api_server.py:1395] vLLM API server version 0.9.2
[36mllm_server_1  |[0m INFO 07-21 18:24:32 [cli_args.py:325] non-default args: {'model': '/llm', 'max_model_len': 32768, 'served_model_name': ['qwen3'], 'data_parallel_size': 4}
[36mllm_server_1  |[0m INFO 07-21 18:24:39 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
[36mllm_server_1  |[0m INFO 07-21 18:24:39 [config.py:1472] Using max model len 32768
[36mllm_server_1  |[0m INFO 07-21 18:24:39 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36mllm_server_1  |[0m INFO 07-21 18:24:40 [utils.py:364] Started DP Coordinator process (PID: 203)
[36mllm_server_1  |[0m INFO 07-21 18:24:45 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:24:45 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:24:45 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:24:45 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:24:45 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:47 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:47 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:48 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:48 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:49 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:49 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:49 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:49 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:49 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:49 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:49 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:49 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:52 [parallel_state.py:935] Adjusting world_size=4 rank=2 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:52 [parallel_state.py:935] Adjusting world_size=4 rank=0 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:52 [parallel_state.py:935] Adjusting world_size=4 rank=3 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:52 [parallel_state.py:935] Adjusting world_size=4 rank=1 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:52 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:52 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:52 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:52 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:52 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:52 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:52 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:52 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:53 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:53 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:53 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:53 [parallel_state.py:1076] rank 1 in world size 4 is assigned as DP rank 1, PP rank 0, TP rank 0, EP rank 1
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:53 [parallel_state.py:1076] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:53 [parallel_state.py:1076] rank 3 in world size 4 is assigned as DP rank 3, PP rank 0, TP rank 0, EP rank 3
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:53 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:53 [parallel_state.py:1076] rank 2 in world size 4 is assigned as DP rank 2, PP rank 0, TP rank 0, EP rank 2
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:53 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:53 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:53 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:53 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:53 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:53 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:53 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:53 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:53 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:53 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:53 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:53 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:24:53 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:24:53 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:24:53 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:24:53 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:01<00:08,  1.16s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:02<00:07,  1.24s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:03<00:06,  1.27s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:04<00:04,  1.03s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:05<00:03,  1.12s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:07<00:02,  1.19s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:25:01 [default_loader.py:272] Loading weights took 7.14 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:25:01 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 7.439423 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:08<00:01,  1.22s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:25:02 [default_loader.py:272] Loading weights took 8.85 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:25:03 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 9.136172 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:09<00:00,  1.24s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:09<00:00,  1.20s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m 
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:25:03 [default_loader.py:272] Loading weights took 9.63 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:25:03 [default_loader.py:272] Loading weights took 9.65 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:25:04 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 9.897317 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:25:04 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 9.899736 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:25:15 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_1/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:25:15 [backends.py:519] Dynamo bytecode transform time: 11.64 s
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:25:16 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_0/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:25:16 [backends.py:519] Dynamo bytecode transform time: 11.81 s
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:25:16 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_2/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:25:16 [backends.py:519] Dynamo bytecode transform time: 11.89 s
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:25:16 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_3/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:25:16 [backends.py:519] Dynamo bytecode transform time: 11.89 s
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:25:22 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:25:22 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:25:22 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:25:22 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:26:07 [backends.py:193] Compiling a graph for general shape takes 51.07 s
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:26:07 [backends.py:193] Compiling a graph for general shape takes 50.98 s
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:26:07 [backends.py:193] Compiling a graph for general shape takes 50.97 s
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:26:07 [backends.py:193] Compiling a graph for general shape takes 51.05 s
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:26:30 [monitor.py:34] torch.compile takes 62.71 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:26:30 [monitor.py:34] torch.compile takes 62.79 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:26:31 [monitor.py:34] torch.compile takes 62.86 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:26:31 [monitor.py:34] torch.compile takes 62.94 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:26:32 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:26:32 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:26:32 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:26:32 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:26:32 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:26:32 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:26:32 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:26:32 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:26:32 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:26:32 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:26:32 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:26:32 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:27:10 [gpu_model_runner.py:2326] Graph capturing finished in 38 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:27:10 [gpu_model_runner.py:2326] Graph capturing finished in 38 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:27:10 [gpu_model_runner.py:2326] Graph capturing finished in 38 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|â–         | 1/67 [00:00<00:56,  1.17it/s]Capturing CUDA graph shapes:   3%|â–Ž         | 2/67 [00:01<00:46,  1.40it/s]Capturing CUDA graph shapes:   4%|â–         | 3/67 [00:02<00:43,  1.49it/s]Capturing CUDA graph shapes:   6%|â–Œ         | 4/67 [00:02<00:40,  1.55it/s]Capturing CUDA graph shapes:   7%|â–‹         | 5/67 [00:03<00:38,  1.60it/s]Capturing CUDA graph shapes:   9%|â–‰         | 6/67 [00:03<00:38,  1.61it/s]Capturing CUDA graph shapes:  10%|â–ˆ         | 7/67 [00:04<00:36,  1.62it/s]Capturing CUDA graph shapes:  12%|â–ˆâ–        | 8/67 [00:05<00:35,  1.64it/s]Capturing CUDA graph shapes:  13%|â–ˆâ–Ž        | 9/67 [00:05<00:35,  1.64it/s]Capturing CUDA graph shapes:  15%|â–ˆâ–        | 10/67 [00:06<00:34,  1.65it/s]Capturing CUDA graph shapes:  16%|â–ˆâ–‹        | 11/67 [00:06<00:34,  1.64it/s]Capturing CUDA graph shapes:  18%|â–ˆâ–Š        | 12/67 [00:07<00:33,  1.65it/s]Capturing CUDA graph shapes:  19%|â–ˆâ–‰        | 13/67 [00:08<00:32,  1.65it/s]Capturing CUDA graph shapes:  21%|â–ˆâ–ˆ        | 14/67 [00:08<00:32,  1.65it/s]Capturing CUDA graph shapes:  22%|â–ˆâ–ˆâ–       | 15/67 [00:09<00:31,  1.64it/s]Capturing CUDA graph shapes:  24%|â–ˆâ–ˆâ–       | 16/67 [00:09<00:31,  1.64it/s]Capturing CUDA graph shapes:  25%|â–ˆâ–ˆâ–Œ       | 17/67 [00:10<00:29,  1.68it/s]Capturing CUDA graph shapes:  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:11<00:28,  1.70it/s]Capturing CUDA graph shapes:  28%|â–ˆâ–ˆâ–Š       | 19/67 [00:11<00:28,  1.71it/s]Capturing CUDA graph shapes:  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:12<00:27,  1.71it/s]Capturing CUDA graph shapes:  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:12<00:27,  1.70it/s]Capturing CUDA graph shapes:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 22/67 [00:13<00:26,  1.70it/s]Capturing CUDA graph shapes:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/67 [00:14<00:25,  1.72it/s]Capturing CUDA graph shapes:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:14<00:24,  1.72it/s]Capturing CUDA graph shapes:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:15<00:24,  1.72it/s]Capturing CUDA graph shapes:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 26/67 [00:15<00:23,  1.73it/s]Capturing CUDA graph shapes:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:16<00:23,  1.73it/s]Capturing CUDA graph shapes:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/67 [00:16<00:22,  1.73it/s]Capturing CUDA graph shapes:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/67 [00:17<00:21,  1.74it/s]Capturing CUDA graph shapes:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:18<00:21,  1.74it/s]Capturing CUDA graph shapes:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 31/67 [00:18<00:20,  1.74it/s]Capturing CUDA graph shapes:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 32/67 [00:19<00:20,  1.74it/s]Capturing CUDA graph shapes:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:19<00:19,  1.76it/s]Capturing CUDA graph shapes:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/67 [00:20<00:18,  1.78it/s]Capturing CUDA graph shapes:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/67 [00:20<00:17,  1.79it/s]Capturing CUDA graph shapes:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:21<00:17,  1.77it/s]Capturing CUDA graph shapes:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 37/67 [00:22<00:17,  1.76it/s]Capturing CUDA graph shapes:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 38/67 [00:22<00:16,  1.77it/s]Capturing CUDA graph shapes:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:23<00:15,  1.78it/s]Capturing CUDA graph shapes:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [00:23<00:15,  1.79it/s]Capturing CUDA graph shapes:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/67 [00:24<00:14,  1.78it/s]Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:24<00:13,  1.79it/s]Capturing CUDA graph shapes:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/67 [00:25<00:13,  1.80it/s]Capturing CUDA graph shapes:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 44/67 [00:25<00:12,  1.80it/s]Capturing CUDA graph shapes:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:26<00:12,  1.81it/s]Capturing CUDA graph shapes:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/67 [00:26<00:11,  1.81it/s]Capturing CUDA graph shapes:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 47/67 [00:27<00:11,  1.81it/s]Capturing CUDA graph shapes:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:28<00:10,  1.81it/s]Capturing CUDA graph shapes:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 49/67 [00:28<00:09,  1.82it/s]Capturing CUDA graph shapes:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:29<00:09,  1.82it/s]Capturing CUDA graph shapes:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:29<00:08,  1.83it/s]Capturing CUDA graph shapes:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 52/67 [00:30<00:08,  1.83it/s]Capturing CUDA graph shapes:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 53/67 [00:30<00:07,  1.78it/s]Capturing CUDA graph shapes:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:31<00:07,  1.80it/s]Capturing CUDA graph shapes:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/67 [00:31<00:06,  1.82it/s]Capturing CUDA graph shapes:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 56/67 [00:32<00:06,  1.82it/s]Capturing CUDA graph shapes:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:33<00:05,  1.84it/s]Capturing CUDA graph shapes:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 58/67 [00:33<00:04,  1.85it/s]Capturing CUDA graph shapes:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 59/67 [00:34<00:04,  1.85it/s]Capturing CUDA graph shapes:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:34<00:03,  1.86it/s]Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 61/67 [00:35<00:03,  1.87it/s]Capturing CUDA graph shapes:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 62/67 [00:35<00:02,  1.88it/s]Capturing CUDA graph shapes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:36<00:02,  1.88it/s]Capturing CUDA graph shapes:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:36<00:01,  1.89it/s]Capturing CUDA graph shapes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 65/67 [00:37<00:01,  1.89it/s]Capturing CUDA graph shapes:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:37<00:00,  1.90it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:38<00:00,  1.90it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:38<00:00,  1.75it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:27:10 [gpu_model_runner.py:2326] Graph capturing finished in 38 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:27:10 [core.py:172] init engine (profile, create kv cache, warmup model) took 126.86 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:27:10 [core.py:172] init engine (profile, create kv cache, warmup model) took 126.86 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:27:10 [core.py:172] init engine (profile, create kv cache, warmup model) took 127.73 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:27:10 [core.py:172] init engine (profile, create kv cache, warmup model) took 129.42 seconds
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 11764
[36mllm_server_1  |[0m WARNING 07-21 18:27:11 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:29] Available routes are:
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /docs, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /health, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /load, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /ping, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /ping, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /tokenize, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /detokenize, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /v1/models, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /version, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /v1/completions, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /v1/embeddings, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /pooling, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /classify, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /score, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /v1/score, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /v1/audio/translations, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /v1/rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /v2/rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /invocations, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:27:11 [launcher.py:37] Route: /metrics, Methods: GET
[36mllm_server_1  |[0m INFO:     Started server process [7]
[36mllm_server_1  |[0m INFO:     Waiting for application startup.
[36mllm_server_1  |[0m INFO:     Application startup complete.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35804 - "GET /health HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:28 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36mllm_server_1  |[0m INFO 07-21 18:27:28 [logger.py:43] Received request chatcmpl-dfab44b2eb9648888adaaa030aaa8580: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:28 [async_llm.py:270] Added request chatcmpl-dfab44b2eb9648888adaaa030aaa8580.
[36mllm_server_1  |[0m INFO 07-21 18:27:32 [loggers.py:118] Engine 000: Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 15.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-d9c409042e1b4c51aa44e59696c753bc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-d9c409042e1b4c51aa44e59696c753bc.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-e77a7cca2fc1477db865513eb011f419: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-e77a7cca2fc1477db865513eb011f419.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-afb1b6f9ec7447f7aad58b0ff5d9052b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-afb1b6f9ec7447f7aad58b0ff5d9052b.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-c45f2c62901748a1bbfaa8f40036851b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-b28f2523cb3f42f6a3595a4c675910c1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-e2649ef57320467897138003e188e357: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-c45f2c62901748a1bbfaa8f40036851b.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-b28f2523cb3f42f6a3595a4c675910c1.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-e2649ef57320467897138003e188e357.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-61f12df63b9046b5b5204607f87770d9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-61f12df63b9046b5b5204607f87770d9.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-8dca8f54d13d4cedb2dd0924fed3d403: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-8dca8f54d13d4cedb2dd0924fed3d403.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-9f01e55bdac84780af3ec3db5ffe4935: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-9f01e55bdac84780af3ec3db5ffe4935.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-69f3866523ac4ce2ad7b7511b9473011: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-69f3866523ac4ce2ad7b7511b9473011.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-dbc310c4e38c4e4da2a2a69dc5777d77: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-dbc310c4e38c4e4da2a2a69dc5777d77.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-6fd29d6d5c644e2493d4ed9d75e1a120: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-6fd29d6d5c644e2493d4ed9d75e1a120.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-214caa882df04d608ffd2a4273bd14ac: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-214caa882df04d608ffd2a4273bd14ac.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-be7349de69de44388675f540a37ebd7e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-be7349de69de44388675f540a37ebd7e.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-9c74a674e55f45aba9eae6d2a0a8db86: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-9c74a674e55f45aba9eae6d2a0a8db86.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-0569f45d4ff44736bd72fa8b5596aaf5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-07a5e607482c4158bb90285e5c4552d7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [logger.py:43] Received request chatcmpl-bb1d83d0d2df44cd91bab2538b5200ad: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-0569f45d4ff44736bd72fa8b5596aaf5.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-07a5e607482c4158bb90285e5c4552d7.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:34 [async_llm.py:270] Added request chatcmpl-bb1d83d0d2df44cd91bab2538b5200ad.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-18720f3eba094353b1f1525899b9e36d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-18720f3eba094353b1f1525899b9e36d.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-a746b54e315c47aca0d0dbf79cac6391: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-a746b54e315c47aca0d0dbf79cac6391.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-c8d70d29298945b4b7ceea6c4e72c766: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-c8d70d29298945b4b7ceea6c4e72c766.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-7517df338e6b4a4d91fb83bfb93dd76e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-7517df338e6b4a4d91fb83bfb93dd76e.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-f40ee44a90e5471d9ddc84d6a39386dd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-f40ee44a90e5471d9ddc84d6a39386dd.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-867efdf4d90f438f8935625a3a2c64f2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-867efdf4d90f438f8935625a3a2c64f2.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-27fc9a7a650d48fd8d1b723f9544f303: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-27fc9a7a650d48fd8d1b723f9544f303.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-0f17a44588a44d79a51225dd0dba41a6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57536 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-0f17a44588a44d79a51225dd0dba41a6.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-0fd60e9cdf39484da35200455df9caf8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-0fd60e9cdf39484da35200455df9caf8.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-23d2a4bb0d294408ae208ab773a14c14: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-23d2a4bb0d294408ae208ab773a14c14.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-b60483446ad74ffd9e833f52311b84e1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-b60483446ad74ffd9e833f52311b84e1.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-48b16ffe73c8402ea157a0038a9b524a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57552 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-48b16ffe73c8402ea157a0038a9b524a.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-b64f0d91934b475f943ed639884e643f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-b64f0d91934b475f943ed639884e643f.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-da1ad9aa33e94b91b166c23fea90935a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-da1ad9aa33e94b91b166c23fea90935a.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-b28aa3e265e941d5963012a749c7baa8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-b28aa3e265e941d5963012a749c7baa8.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-b695dcbda6ac486ca6f2480472e24721: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-b695dcbda6ac486ca6f2480472e24721.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-ebb0ad5c84b8428f8ed7be80ec361cd7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-ebb0ad5c84b8428f8ed7be80ec361cd7.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-6dee63ed35854fc4970ce9345579c65a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-6dee63ed35854fc4970ce9345579c65a.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-09fa973f092f44889b4396d07fa7967f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57632 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-09fa973f092f44889b4396d07fa7967f.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-5b2c84b255e94a7bbd529bedb5119b7a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-5b2c84b255e94a7bbd529bedb5119b7a.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-7a98563277584e8293fb4ab407e349d5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57656 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-7a98563277584e8293fb4ab407e349d5.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-cc48dbcc5fb94cdb95f059c362f82682: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-cc48dbcc5fb94cdb95f059c362f82682.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-7dae330a45f048b4bcf82c75e6f2fc6f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-7dae330a45f048b4bcf82c75e6f2fc6f.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-1d62ec5b1fca471793e03c96850f8b50: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-84b850c2d2254677ba2b4fb8ff18d23a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-1d62ec5b1fca471793e03c96850f8b50.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-9f6c335d4542420b88ca609309ddde05: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-84b850c2d2254677ba2b4fb8ff18d23a.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-46a2a71970db46699c7f84d8a6665a38: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-9f6c335d4542420b88ca609309ddde05.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-46a2a71970db46699c7f84d8a6665a38.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-e87479caa871402cafc45761456708ac: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-e87479caa871402cafc45761456708ac.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-5953d82466b14cc99e2e1900a104a9b0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-5953d82466b14cc99e2e1900a104a9b0.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-1041204978dc46618c216374d9ab5e1c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-1041204978dc46618c216374d9ab5e1c.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-cafe084add1642dc95d7ecdb7c3db857: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-cafe084add1642dc95d7ecdb7c3db857.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-20b7423c0e07491f9fb6afd511364bb0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-20b7423c0e07491f9fb6afd511364bb0.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-76600e2a2a244695bf0541a2b3e07843: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-76600e2a2a244695bf0541a2b3e07843.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-0b30cf951d754815871fa04db27b2bd6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-0b30cf951d754815871fa04db27b2bd6.
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [logger.py:43] Received request chatcmpl-5f78ba387aa34292b390c01a3e4114f6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:35 [async_llm.py:270] Added request chatcmpl-5f78ba387aa34292b390c01a3e4114f6.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-21f1534d83e340e08322057e53f5efa0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-21f1534d83e340e08322057e53f5efa0.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-8d8e9695e4474d7ead254bbe32f1125b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57814 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-8d8e9695e4474d7ead254bbe32f1125b.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-286c063f763c4ca5b23b0f9742b1f893: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57830 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-286c063f763c4ca5b23b0f9742b1f893.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-894a61ae956a4ec1b1c5042f8d463ee5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-894a61ae956a4ec1b1c5042f8d463ee5.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-a6eac62784fb49fc9ce47c3bb08e0a14: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-a6eac62784fb49fc9ce47c3bb08e0a14.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-1a31c3108d914aa68473be583938d822: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-1a31c3108d914aa68473be583938d822.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-e36fbc6b072848618c60a689b78afb71: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-e36fbc6b072848618c60a689b78afb71.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-bfd80d8b549849c79dd64998f26438c2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-bfd80d8b549849c79dd64998f26438c2.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-d736a0a0b8f14b0e987eae74aac4c75e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-d736a0a0b8f14b0e987eae74aac4c75e.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-2370d97536294063afd2920f1d4ee884: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-2370d97536294063afd2920f1d4ee884.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-76ebcf3535274de0bfb0430688895db3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-76ebcf3535274de0bfb0430688895db3.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-df58104b440c4ddeac924a8a8e8a82f6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-df58104b440c4ddeac924a8a8e8a82f6.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-c6719e44b9c646918e085d39daa57ffe: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-c6719e44b9c646918e085d39daa57ffe.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-e234bf40187847cbacdad2ba1df4fd1c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-e234bf40187847cbacdad2ba1df4fd1c.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-3ef32996052849a1b6bdb9161a9b32d6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-7a3e17bec06c403094ae988a7b2e0ba0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-3ef32996052849a1b6bdb9161a9b32d6.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-7a3e17bec06c403094ae988a7b2e0ba0.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-288a3e24ba164d96ba1907e647da2567: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-e0b30ad7457e469c861d321936c2a688: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-288a3e24ba164d96ba1907e647da2567.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-e0b30ad7457e469c861d321936c2a688.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-732ac4d9d90a4495b67e34c88d5bb45c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-732ac4d9d90a4495b67e34c88d5bb45c.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-e58dc870c5af4301908056ff61a91a05: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:57998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-e58dc870c5af4301908056ff61a91a05.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-0d71b334ca8444ebbfce5cff024fb871: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-0d71b334ca8444ebbfce5cff024fb871.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-279ed5982129460eb00196ce2b620115: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-279ed5982129460eb00196ce2b620115.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-e460253e32134e9c9ba08a47a2a69c16: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-e460253e32134e9c9ba08a47a2a69c16.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-e3283b609e04430ba4bbdfe9ebfd07ee: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-e3283b609e04430ba4bbdfe9ebfd07ee.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-cd350c0150684af192cac27ed5122145: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-cd350c0150684af192cac27ed5122145.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-f6c73cd2fc804bf4b5fc433fb7730da3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-f6c73cd2fc804bf4b5fc433fb7730da3.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-093b709bc38945359d7d4ef0a3303f8b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-093b709bc38945359d7d4ef0a3303f8b.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-80cfa05490aa4cac91717b29d227b869: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-80cfa05490aa4cac91717b29d227b869.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-3a6456694fa6428384e737f18f364fee: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-3a6456694fa6428384e737f18f364fee.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-77b64b0038d0410d9cf42583795a4b65: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-cbd9e9a5c1b044ed85fecfb567b5fae6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-77b64b0038d0410d9cf42583795a4b65.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-cbd9e9a5c1b044ed85fecfb567b5fae6.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-68d23393e2a24facad1c357269ec7d83: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-e26f258bb4bf458e9a1ebd04602dd1f6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-68d23393e2a24facad1c357269ec7d83.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-e26f258bb4bf458e9a1ebd04602dd1f6.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-aab2ca76ac674704af8c621fa2d9b055: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-aab2ca76ac674704af8c621fa2d9b055.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-67314a4b89d74873b38fdf22e693d37a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-67314a4b89d74873b38fdf22e693d37a.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-20240579476b479184952595504194ee: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58144 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-20240579476b479184952595504194ee.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-5293ce64178f440ab909ecafae99b36a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-5293ce64178f440ab909ecafae99b36a.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-bd073243189740d49395897efcb4d39f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-bd073243189740d49395897efcb4d39f.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-0da95227440d419faba05e0ea6cdca1f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-0da95227440d419faba05e0ea6cdca1f.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-2978c1b13bce4a458162c0104702ec56: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-2978c1b13bce4a458162c0104702ec56.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-4e1f9efea44048e9bf7b5fdd28ab2e86: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-4e1f9efea44048e9bf7b5fdd28ab2e86.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-fd431afc6f874708b124a72177e18474: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-fd431afc6f874708b124a72177e18474.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-9c06d6613d674382b18f9c6cbddfc3c0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-9c06d6613d674382b18f9c6cbddfc3c0.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-ed9675c833bb498eb95641d7f8ea426e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-ed9675c833bb498eb95641d7f8ea426e.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-687c6658b42c4b58b85053fb3062ec4a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-687c6658b42c4b58b85053fb3062ec4a.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-7ae37444d2a74cf9b7771907a74c0c43: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-7ae37444d2a74cf9b7771907a74c0c43.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-a0cc6c55e0d44d69b96ac7f2bf999d30: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-3ab243658d3d43e3b83575accd287db0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-a0cc6c55e0d44d69b96ac7f2bf999d30.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-3ab243658d3d43e3b83575accd287db0.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-b356567b62ab4038851585582f7b5582: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-b356567b62ab4038851585582f7b5582.
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [logger.py:43] Received request chatcmpl-8ac0ba504c7e41949492dea6e09b5b74: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:36 [async_llm.py:270] Added request chatcmpl-8ac0ba504c7e41949492dea6e09b5b74.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-9d3d767e3caa476f9b764c0d3f1b3b89: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-9d3d767e3caa476f9b764c0d3f1b3b89.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-78695957d07f4eeda6a22e426d5d98e9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-78695957d07f4eeda6a22e426d5d98e9.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-98b77bf1deb640dd8f7b35ec4d20904e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-98b77bf1deb640dd8f7b35ec4d20904e.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-16bd6094210340aabda774491564575d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-16bd6094210340aabda774491564575d.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-b2b656f14d8849e6a86fdcd92f7d96a0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-b2b656f14d8849e6a86fdcd92f7d96a0.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-ad0662c9c47c4d7b88592678e9434c48: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-ad0662c9c47c4d7b88592678e9434c48.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-e7dae804dd3f42bfab002304c6d2d4c1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-e7dae804dd3f42bfab002304c6d2d4c1.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-2b9a8461b1434251b4765d5e1912aca5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-2b9a8461b1434251b4765d5e1912aca5.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-2409e4cf9bfb47a8bbe89abf2281df6f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-2409e4cf9bfb47a8bbe89abf2281df6f.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-4e8a09540fe945b3bec17883097512ca: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-4e8a09540fe945b3bec17883097512ca.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-10ffb5e7420a458cacbd71728349dd1c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-10ffb5e7420a458cacbd71728349dd1c.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-f89446d8ae634cc38721788217bddecd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-f89446d8ae634cc38721788217bddecd.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-c7d1b097ded849bc8e5a85f9801ba05a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-c7d1b097ded849bc8e5a85f9801ba05a.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-49e4cacf4ae14ddc9c7a9d7e4448a2b6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-49e4cacf4ae14ddc9c7a9d7e4448a2b6.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-b3b70dad58a744b1b8086d1bbfcd61c1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-b3b70dad58a744b1b8086d1bbfcd61c1.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-a07d1981980a4da4a6d8a18747c91177: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-a07d1981980a4da4a6d8a18747c91177.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-3aac638969644f4a9b5b063cc8532bc5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-3aac638969644f4a9b5b063cc8532bc5.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-9e7326dbd01b4145af54dd750a7f0b68: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-9e7326dbd01b4145af54dd750a7f0b68.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-43189f362cda418bb4330c6749efa0bf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-43189f362cda418bb4330c6749efa0bf.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-56eecb7849b44052a33bad98f388b059: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-56eecb7849b44052a33bad98f388b059.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-a9ba17faf1c6423d97a471b861209b52: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-a9ba17faf1c6423d97a471b861209b52.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-50e88afddfbb466488dec416c4a18c04: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-50e88afddfbb466488dec416c4a18c04.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-729f44587bc54aa68fd8d89109f79c92: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-729f44587bc54aa68fd8d89109f79c92.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-6ece55f94db4498a9a3790449b7e4464: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-6ece55f94db4498a9a3790449b7e4464.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-d2aca3351dec4b7fac3cc27cda167be7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58524 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-d2aca3351dec4b7fac3cc27cda167be7.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-882ba71e57524dc4aebc38e65c901692: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-882ba71e57524dc4aebc38e65c901692.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-8cb81eaf9ed248b0be8eeb824ddf67cb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-8cb81eaf9ed248b0be8eeb824ddf67cb.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-0106758242af464994d2566750305d87: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-0106758242af464994d2566750305d87.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-0f70ab67f6d6401a84f2476802865571: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-0f70ab67f6d6401a84f2476802865571.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-74e9d6560b744d56b6a80690b3aa17a1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-74e9d6560b744d56b6a80690b3aa17a1.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-e6f1073065794e2f8f0fc034f0e1d9a8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-e6f1073065794e2f8f0fc034f0e1d9a8.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-0340348af3514f9a9661a9ed5dc25a01: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-0340348af3514f9a9661a9ed5dc25a01.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-c3e16aaa614b4ccd9be300fb10df0385: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-c3e16aaa614b4ccd9be300fb10df0385.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-3602468e9cd14efaa7b2a626235dd080: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-3602468e9cd14efaa7b2a626235dd080.
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [logger.py:43] Received request chatcmpl-f12e7e62dd8c43a6a0e698d77e14535b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:37 [async_llm.py:270] Added request chatcmpl-f12e7e62dd8c43a6a0e698d77e14535b.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-3a5c87b11a6149f8b49b598c26dd3526: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-3a5c87b11a6149f8b49b598c26dd3526.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-1d3de621a71f41a69f7a8699a275488a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-1d3de621a71f41a69f7a8699a275488a.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-3abb9196e87b4d69bb0d30a776d964e8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-3abb9196e87b4d69bb0d30a776d964e8.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-05a610120e59494e971ab679d5bbf080: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-05a610120e59494e971ab679d5bbf080.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-f2ee7a4ae10b4bddafea02c683c6fd78: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-f2ee7a4ae10b4bddafea02c683c6fd78.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-e3aebd1f64e84ce09cee34445dde3c34: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-e3aebd1f64e84ce09cee34445dde3c34.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-bf781010c6ac4d7194a83f09f0c8c591: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-bf781010c6ac4d7194a83f09f0c8c591.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-24990c613b204771b48c22d7718936ac: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-24990c613b204771b48c22d7718936ac.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-0c7968d6749e418f82d61ec5c6e99185: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-0c7968d6749e418f82d61ec5c6e99185.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-e3d7823a936a400fb81aa7bfd882e8b7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-e3d7823a936a400fb81aa7bfd882e8b7.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-0834a6c1e0da449883b6c86824a8cf46: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-0834a6c1e0da449883b6c86824a8cf46.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-c9726e8245644548ae083934a1503190: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-c9726e8245644548ae083934a1503190.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-58364d6e9cce42beb256104153addd69: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-58364d6e9cce42beb256104153addd69.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-69264f4f1d3043c8a6fbc9e384e2853e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-69264f4f1d3043c8a6fbc9e384e2853e.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-aedce8fb5754454d91defbb6bf238c49: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-aedce8fb5754454d91defbb6bf238c49.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-1baf438304d04e41a295e1e7cdbd7e96: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-1baf438304d04e41a295e1e7cdbd7e96.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-6c500ef36ae6427ba8f43696dbdb3743: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-6c500ef36ae6427ba8f43696dbdb3743.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-4009006ca01d4ec9a57a50e77025e74b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-4009006ca01d4ec9a57a50e77025e74b.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-a0d7937873664707aa4f92ab07480ff4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-a0d7937873664707aa4f92ab07480ff4.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-0ca35aa2f1314f81af4347d10012bd4a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-0ca35aa2f1314f81af4347d10012bd4a.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-05f7acd3ec3547068bcaf8f02229ec6d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-05f7acd3ec3547068bcaf8f02229ec6d.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-3bd6b93cbcb64e2289bcd7e6ed235a5a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-3bd6b93cbcb64e2289bcd7e6ed235a5a.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-e0f02cc78d854b7aaf19e519d2ca524a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-e0f02cc78d854b7aaf19e519d2ca524a.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-e319e424d79f473c95dc0374f69c3118: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-e319e424d79f473c95dc0374f69c3118.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-2540351d2c2a422daa3788eb61ff9dee: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-2540351d2c2a422daa3788eb61ff9dee.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-9f19d84cf08d4d5e997a25462b1f3930: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-9f19d84cf08d4d5e997a25462b1f3930.
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [logger.py:43] Received request chatcmpl-f8b5279ed2dc4701bee58a021ed5ddb8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:38 [async_llm.py:270] Added request chatcmpl-f8b5279ed2dc4701bee58a021ed5ddb8.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-f8b26c0352fe47239866265ca77f8889: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-f8b26c0352fe47239866265ca77f8889.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-f445c1e0a9dd420ab7b6571d9816d076: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-f445c1e0a9dd420ab7b6571d9816d076.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-634a22e19f0140bebc0c2749ff9cbb4e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-634a22e19f0140bebc0c2749ff9cbb4e.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-04e45ba8edca482e9efc2047d4224b39: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-04e45ba8edca482e9efc2047d4224b39.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-4d9103c5b13c4a40a158c725413c4384: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-503e59b0cf1744ceafe409b5b5df7308: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-4d9103c5b13c4a40a158c725413c4384.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-503e59b0cf1744ceafe409b5b5df7308.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-bb74d5d20ffc417c9a306c9424f8be0e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-f2642f8d5b684364a4e15e809af5a360: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-bb74d5d20ffc417c9a306c9424f8be0e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-f2642f8d5b684364a4e15e809af5a360.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-890e27463a754951af6aeae70c756035: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-c94caa05c43e4c2ea87d52e87c4ed207: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-890e27463a754951af6aeae70c756035.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-c94caa05c43e4c2ea87d52e87c4ed207.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-7a1a71308dfc4c13984261be873545ac: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-7a1a71308dfc4c13984261be873545ac.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-c262bbd551924c37affea62e1b1f2daa: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-c262bbd551924c37affea62e1b1f2daa.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-e3d5062ad1cc483caec9c1ff17159aa1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-e3d5062ad1cc483caec9c1ff17159aa1.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-f4d39b2f15734c7784268a330641bc49: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-f4d39b2f15734c7784268a330641bc49.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-2d6f0e2b562d49ebaf4f3c25501f5e1c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:58998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-2d6f0e2b562d49ebaf4f3c25501f5e1c.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-7e49c69b509045258d847a46eaa49e60: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-7e49c69b509045258d847a46eaa49e60.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-c6fc824921cd4ff09baeb444712610e7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-c6fc824921cd4ff09baeb444712610e7.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-124c4a3cd5064133a4d7fc9cfccb5a8a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-124c4a3cd5064133a4d7fc9cfccb5a8a.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-66208f34ba2948c5841cd37508c22a79: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-66208f34ba2948c5841cd37508c22a79.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-e2a4dc0e39184f41aac7615d9fc679ed: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-e2a4dc0e39184f41aac7615d9fc679ed.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-37f110f9a13a4d59bbd2ca35c7e81039: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-37f110f9a13a4d59bbd2ca35c7e81039.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-2f0d02157c8f454d9f236ee8ecad80f3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-2f0d02157c8f454d9f236ee8ecad80f3.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-45faaa403e7e45c1b96e0c31f443e343: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-45faaa403e7e45c1b96e0c31f443e343.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-b2f46824bbc8496db5ab9f03da9f93d2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-b2f46824bbc8496db5ab9f03da9f93d2.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-bb3e4c5a5f934c7988440fd2b54684c6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-bb3e4c5a5f934c7988440fd2b54684c6.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-b243c002615c4166a6038a0f67de40e4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-b243c002615c4166a6038a0f67de40e4.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-cc502d311c6f481cb62785c657b86550: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-cc502d311c6f481cb62785c657b86550.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-4e234bc8a1bd4230b8f495f6a32f94c5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-9ec4d49262ce4876865b97b86c266bf0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-4e234bc8a1bd4230b8f495f6a32f94c5.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-9ec4d49262ce4876865b97b86c266bf0.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-133f88368b6d43b8b3cd6c8218122d7a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-133f88368b6d43b8b3cd6c8218122d7a.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-d10d3581a7e54753a752a86279bdce4f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-8068f4af02e547b596a3a7ede8233d29: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-426f02dde4ff4066afe3e77c09b776e7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-d10d3581a7e54753a752a86279bdce4f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-8068f4af02e547b596a3a7ede8233d29.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-f176da4e04bc486c9f73acc93353fb1e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-32d5fd378dc643c3b3212c6cfc61f035: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-426f02dde4ff4066afe3e77c09b776e7.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-f176da4e04bc486c9f73acc93353fb1e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-32d5fd378dc643c3b3212c6cfc61f035.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-02804c6e489a42beaa3e14a62eb735e7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-02804c6e489a42beaa3e14a62eb735e7.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-715a4aaa88c24512a9db89e65e972171: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-715a4aaa88c24512a9db89e65e972171.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-3d5767fb66ed400ea51679d5f0c39268: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-3d5767fb66ed400ea51679d5f0c39268.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-718884bf1f394c1781fa81ff76cf857d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-718884bf1f394c1781fa81ff76cf857d.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-199927e85ff14296b0e234ade1258ab9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-199927e85ff14296b0e234ade1258ab9.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-83621d8d2f0f4d8a8af378ea2b6266b1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-83621d8d2f0f4d8a8af378ea2b6266b1.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-fc168eb4f3be4513a24de9e3f9f02cc2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-fc168eb4f3be4513a24de9e3f9f02cc2.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-835000872b064b7e8fcda116bd8ca262: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-835000872b064b7e8fcda116bd8ca262.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-f2fd1b5e637e42798c619b3f497a7419: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-f2fd1b5e637e42798c619b3f497a7419.
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [logger.py:43] Received request chatcmpl-e4c21e37b53a4c6aa529387b2c086a3c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:39 [async_llm.py:270] Added request chatcmpl-e4c21e37b53a4c6aa529387b2c086a3c.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-ca2b2b155b754356bf35ab2bf4cc871a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-ca2b2b155b754356bf35ab2bf4cc871a.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-01a8381e6902412aa0da4901e7c4236f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59260 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-01a8381e6902412aa0da4901e7c4236f.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-30225db63cbe43d194a872f25ebbb400: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-30225db63cbe43d194a872f25ebbb400.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-4f32641b81de4927b92f584e4db1fd01: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-4f32641b81de4927b92f584e4db1fd01.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-c529c6bea0f442449384ef04e3673a26: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-c529c6bea0f442449384ef04e3673a26.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-a7bf21f68a6846feb96883392f030d49: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-a7bf21f68a6846feb96883392f030d49.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-c7f1110c7d6a40f394ac9a5db7d956ad: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-c7f1110c7d6a40f394ac9a5db7d956ad.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-d47f1d53c2684b5584a784befeac11b2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-d47f1d53c2684b5584a784befeac11b2.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-4e93abe1b4fc433cb2836cb6a76ce02a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-4e93abe1b4fc433cb2836cb6a76ce02a.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-db3ce042fb05477690ab8903d95dd30c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-db3ce042fb05477690ab8903d95dd30c.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-ad988b67c2ec4dc9958104b6ede66e44: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-ad988b67c2ec4dc9958104b6ede66e44.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-7bf766a9f9ff4c698c6676a095e21750: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-7bf766a9f9ff4c698c6676a095e21750.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-e4df8012aecc459d921e2105dc208d86: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-e4df8012aecc459d921e2105dc208d86.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-f4ecaf726d724161bc2fab0e78553b1b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-f4ecaf726d724161bc2fab0e78553b1b.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-5477d1738b8c4e359ddb85dcc90d109d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-5477d1738b8c4e359ddb85dcc90d109d.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-397f2118d85243dbb9fec63b542f961e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-397f2118d85243dbb9fec63b542f961e.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-635856eb9ccd4236b0cf947f22d8108a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-635856eb9ccd4236b0cf947f22d8108a.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-4e7ca9c89d2943c4896b8a159e18b345: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-4e7ca9c89d2943c4896b8a159e18b345.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-6b3760c94f464f3d9c45230e8466503c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-6b3760c94f464f3d9c45230e8466503c.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-30d5ddf27c93454a80391fd2adef71c0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-30d5ddf27c93454a80391fd2adef71c0.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-0d0a88e21f5345f0b698ee09249c6783: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-0d0a88e21f5345f0b698ee09249c6783.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-70627ca2ddc64a0b909daf3a9ad3fc93: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-70627ca2ddc64a0b909daf3a9ad3fc93.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-fd76e53627764396aab2795dd272e5bd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-fd76e53627764396aab2795dd272e5bd.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-ae0241cdca3c40929a8115a8581f1ab4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-ae0241cdca3c40929a8115a8581f1ab4.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-ded113d0603749b79894b7ad1eedc88a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-ded113d0603749b79894b7ad1eedc88a.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-708ba60edfd44732bf5bb6c14be38708: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-708ba60edfd44732bf5bb6c14be38708.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-541a8afb2d23487bb39a638989814399: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-541a8afb2d23487bb39a638989814399.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-73856e5ace3b4ae1bccb584fceefaa3b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-73856e5ace3b4ae1bccb584fceefaa3b.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-c8419a6ae4f646dfb3d018dd8fb88d59: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-c8419a6ae4f646dfb3d018dd8fb88d59.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-7473334141f9464986ceff2a2746bb41: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-7473334141f9464986ceff2a2746bb41.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-973ee204b0b8451a838b1b3d1baf621e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-973ee204b0b8451a838b1b3d1baf621e.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-9f5afaec4b624745972729df5a3a69b3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-9f5afaec4b624745972729df5a3a69b3.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-2b1eb1b3ca914c54ae1e60940b04925d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-2b1eb1b3ca914c54ae1e60940b04925d.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-3716a6fcff11441ab6832f021918bed3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-3716a6fcff11441ab6832f021918bed3.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-365b27b0aa18418d9a38f9c71149d036: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59534 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-365b27b0aa18418d9a38f9c71149d036.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-d10b9169ed7b4691aca94efdfef631a5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-d10b9169ed7b4691aca94efdfef631a5.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-209e75f8eeae41a1a94db15c0d6d3cd5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-209e75f8eeae41a1a94db15c0d6d3cd5.
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [logger.py:43] Received request chatcmpl-0451fc16934e4b4f9c4d239923e03a08: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:40 [async_llm.py:270] Added request chatcmpl-0451fc16934e4b4f9c4d239923e03a08.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-6822af4c33424974aaf71f0ddd0b66ad: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-6822af4c33424974aaf71f0ddd0b66ad.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-cb0a2fbb2156404a96812be91015d496: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59574 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-cb0a2fbb2156404a96812be91015d496.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-93d18d97888f4afe8607a8dddba7cf26: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-93d18d97888f4afe8607a8dddba7cf26.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-e9c151b358844f229eb7b7a4812cfadd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-e9c151b358844f229eb7b7a4812cfadd.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-e8ce362ec03b4892b2f07adc07f3561f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-e8ce362ec03b4892b2f07adc07f3561f.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-95f2e9cdd36b4acba234516af788f0f1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-95f2e9cdd36b4acba234516af788f0f1.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-eec0ae69b37c4e6da1c233242b3842db: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-eec0ae69b37c4e6da1c233242b3842db.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-82445164719147c9bd875772c5f151f1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-82445164719147c9bd875772c5f151f1.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-b2dda0d1a77b4113beeb452028033639: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59642 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-b2dda0d1a77b4113beeb452028033639.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-f2b2d60d99da42d1a8b732d197c70dd3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-f2b2d60d99da42d1a8b732d197c70dd3.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-b560017462ca4c3ab29c617207547354: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-b560017462ca4c3ab29c617207547354.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-3cb28ed5464e4ee79ff5d998d9544fc6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-3cb28ed5464e4ee79ff5d998d9544fc6.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-bde466d121c140118c67b4b419676595: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-bde466d121c140118c67b4b419676595.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-3603b1a93e3447879f7908d16140d0c5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-3603b1a93e3447879f7908d16140d0c5.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-64c12b8407aa4a4db70d68fd19e09c68: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-64c12b8407aa4a4db70d68fd19e09c68.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-3af448fd975d4d9aade786c4e31f669f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-3af448fd975d4d9aade786c4e31f669f.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-e92e7ce079fe418486c2611be5fe5c8b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-e92e7ce079fe418486c2611be5fe5c8b.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-f6f235b1bff6430d8438201cb1b61df0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-f6f235b1bff6430d8438201cb1b61df0.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-da82e685f97c43c8a09c3a234a427aa7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-da82e685f97c43c8a09c3a234a427aa7.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-4d8de031d5684620a1b8861fb5035e9e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-4d8de031d5684620a1b8861fb5035e9e.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-45910a12a86c4aa987fcc5da50d94719: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-45910a12a86c4aa987fcc5da50d94719.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-a82e4a77a5254871842a16ede49a969e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-a82e4a77a5254871842a16ede49a969e.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-b42106de073f46a8b5ba331c8ea6acbd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-b42106de073f46a8b5ba331c8ea6acbd.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-133098bae6564d9a81d23f43cb6989b4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-133098bae6564d9a81d23f43cb6989b4.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-a5592b2779f34500b2b7b6661e6ae393: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-a5592b2779f34500b2b7b6661e6ae393.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-c330e5cf88c1424f86ff60d938f89adc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-c330e5cf88c1424f86ff60d938f89adc.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-99e273694d0942869d2dcbf49ad5834b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-99e273694d0942869d2dcbf49ad5834b.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-7981580013a44e449db6711a1f93eaf7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-7981580013a44e449db6711a1f93eaf7.
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [logger.py:43] Received request chatcmpl-effdb3efa77641ccacc58b18f397a9be: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59814 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:41 [async_llm.py:270] Added request chatcmpl-effdb3efa77641ccacc58b18f397a9be.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-e35520a31b46420dbd8ee24bdac5bb34: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-e35520a31b46420dbd8ee24bdac5bb34.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [loggers.py:118] Engine 000: Avg prompt throughput: 351.4 tokens/s, Avg generation throughput: 826.7 tokens/s, Running: 69 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.9%, Prefix cache hit rate: 80.1%
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [loggers.py:118] Engine 001: Avg prompt throughput: 366.6 tokens/s, Avg generation throughput: 810.7 tokens/s, Running: 69 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.5%, Prefix cache hit rate: 81.9%
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [loggers.py:118] Engine 002: Avg prompt throughput: 359.8 tokens/s, Avg generation throughput: 808.1 tokens/s, Running: 70 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.7%, Prefix cache hit rate: 80.8%
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [loggers.py:118] Engine 003: Avg prompt throughput: 356.4 tokens/s, Avg generation throughput: 800.2 tokens/s, Running: 69 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.4%, Prefix cache hit rate: 82.0%
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-a1a2b4fef67c4a69880020008e826b34: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-a1a2b4fef67c4a69880020008e826b34.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-aae6fc4daccf43318eb2c568a83a9229: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-aae6fc4daccf43318eb2c568a83a9229.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-7559773d9dc8410e96663f0fefbc0b66: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-6a93a07f588348cb83c52e445f7086a2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-7559773d9dc8410e96663f0fefbc0b66.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-6a93a07f588348cb83c52e445f7086a2.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-192ef96651de43cf9e69e06f61eea732: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-192ef96651de43cf9e69e06f61eea732.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-8ed167abfe3a4660b20947133d8da1bb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-8ed167abfe3a4660b20947133d8da1bb.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-8c44c0b0faf842fd98e90d50da467a96: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-8c44c0b0faf842fd98e90d50da467a96.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-5748d9570aba4bbeb274567d98c6c575: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-5748d9570aba4bbeb274567d98c6c575.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-5cab7eee1b584badb425d7096dff6282: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-5cab7eee1b584badb425d7096dff6282.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-8f28d35a760e454d96a15a1601c9b5eb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-8f28d35a760e454d96a15a1601c9b5eb.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-d8e922b6aea64087809bf43d63ab5907: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-d8e922b6aea64087809bf43d63ab5907.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-5cd97242838548ef925c4f01436e5755: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-5cd97242838548ef925c4f01436e5755.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-cfc21cbc1c914e709e2000ed89a42d7d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-cfc21cbc1c914e709e2000ed89a42d7d.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-2c73ea43a38647508acfba14f81c78db: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-2c73ea43a38647508acfba14f81c78db.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-24ab91c53f5947918abf0052d79953d3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-24ab91c53f5947918abf0052d79953d3.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-30c4ec3e7cdd42cfbaa16627e68bcc70: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-30c4ec3e7cdd42cfbaa16627e68bcc70.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-da001a96bc8841de850792ed337da5b0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-da001a96bc8841de850792ed337da5b0.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-da3233dc37f749ec8cf424d18c1e2541: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-da3233dc37f749ec8cf424d18c1e2541.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-3f8e2dba27d7427a9e537af2d439dc65: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-3f8e2dba27d7427a9e537af2d439dc65.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-9551bed0cfdf47f684f7e91d074b4d35: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-9551bed0cfdf47f684f7e91d074b4d35.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-edddbb9ce9584b4b8d8778190a9da83e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-edddbb9ce9584b4b8d8778190a9da83e.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-0a09b566443d442aaa4c8f2cf90a63e5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-0a09b566443d442aaa4c8f2cf90a63e5.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-d7bffaba8846482089fb35f7b2fadbc0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-d7bffaba8846482089fb35f7b2fadbc0.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-1d78ef40418f4ff284d0efebbe83ee73: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-1d78ef40418f4ff284d0efebbe83ee73.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-905c43b92a1a450096b389d176e70184: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-905c43b92a1a450096b389d176e70184.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-e742b70854254147b3b0ced5fdad205f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-e742b70854254147b3b0ced5fdad205f.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-88cde98b541a4dde899370a14fe4a42f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-0a4a314829f6490fad0e9ea3e107c805: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-88cde98b541a4dde899370a14fe4a42f.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-82e77166e9634415bb6f8af0991a7c21: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-0a4a314829f6490fad0e9ea3e107c805.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-82e77166e9634415bb6f8af0991a7c21.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-3302536f884d47f8bafc443b062c601a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-3302536f884d47f8bafc443b062c601a.
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [logger.py:43] Received request chatcmpl-45a1c240753c443ab01c05477fd14e5d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:42 [async_llm.py:270] Added request chatcmpl-45a1c240753c443ab01c05477fd14e5d.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-a16f403c928049528fe93cddb3e188b7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-a16f403c928049528fe93cddb3e188b7.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-193cc2baba194dff9548b6eafd1da98c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-193cc2baba194dff9548b6eafd1da98c.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-cf80a3738549414c9189c04d5daedf5d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-cf80a3738549414c9189c04d5daedf5d.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-b6832d05bd134622be1e1d9b4aad71a3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-b6832d05bd134622be1e1d9b4aad71a3.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-b3ec001fe16f4d5baa1fdbe82aaa0cf2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39260 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-b3ec001fe16f4d5baa1fdbe82aaa0cf2.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-5dcdd85227064584964c4b26904345f2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-5dcdd85227064584964c4b26904345f2.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-73a9706fc2ad479ba19b16ede92df5bd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-73a9706fc2ad479ba19b16ede92df5bd.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-2204e9aa9eb14257b8368317c660cd9e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-2204e9aa9eb14257b8368317c660cd9e.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-4238271a389740d1a935a67a59f35d1a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-4238271a389740d1a935a67a59f35d1a.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-3c702c14f6b74d0f948b52bc8269842f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-3c702c14f6b74d0f948b52bc8269842f.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-79febf3caf234c65a0d906b9716ffe5b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-79febf3caf234c65a0d906b9716ffe5b.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-cc820fe791ad4d3b83cf906dce36bcc1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-cc820fe791ad4d3b83cf906dce36bcc1.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-f42b630b65c4425b97f2d97cd8e9a00a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-f42b630b65c4425b97f2d97cd8e9a00a.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-f34f1f4bd9c0433a98199cad568a1b52: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-f34f1f4bd9c0433a98199cad568a1b52.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-1bf59fd822434f4a82c55835f4d0d090: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-1bf59fd822434f4a82c55835f4d0d090.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-22f92bc4bd1247ad966475320d9baffd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-22f92bc4bd1247ad966475320d9baffd.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-41603966001b46faa8c3f4f65b2e7621: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-41603966001b46faa8c3f4f65b2e7621.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-121769e882d34372ae658e97ab532089: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-121769e882d34372ae658e97ab532089.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-459cb25e55724f8198f3aad43bf709ee: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-459cb25e55724f8198f3aad43bf709ee.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-6a47345420984cc6bcad2c095eb2045a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-6a47345420984cc6bcad2c095eb2045a.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-9dc96d7d91ff4cfab39b7569664bdb50: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-9dc96d7d91ff4cfab39b7569664bdb50.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-9d72c0fdb60748a393ba0ddaff82a96c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-9d72c0fdb60748a393ba0ddaff82a96c.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-79446fcf868f45b2a0a5d6f5de85dc96: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-79446fcf868f45b2a0a5d6f5de85dc96.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-941cf73fc7cf4fa4bbb2c1f12ffd8947: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-941cf73fc7cf4fa4bbb2c1f12ffd8947.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-314223af013f4fb28a34cd6a49363f8e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39436 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-314223af013f4fb28a34cd6a49363f8e.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-6b4eeb0ad4fd452880c5ea7f7db05ba6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-6b4eeb0ad4fd452880c5ea7f7db05ba6.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-d2bfed11e53d40f6947e42e4fbddab53: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-d2bfed11e53d40f6947e42e4fbddab53.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-57c928726081469aa8af2f574369c308: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-57c928726081469aa8af2f574369c308.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-93376cc2455148468c17c60705d86bed: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39478 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-93376cc2455148468c17c60705d86bed.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-d2f1e8dbad66462eb526086f909dcb80: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-d2f1e8dbad66462eb526086f909dcb80.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-0d664bf07d3e4957a734a12b50309c62: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-0d664bf07d3e4957a734a12b50309c62.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-437bf2fa7826431facc3fe06ddbd7c00: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-437bf2fa7826431facc3fe06ddbd7c00.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-180cfb3fe96444dc8458a96bf3d00aad: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-180cfb3fe96444dc8458a96bf3d00aad.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-eee1bc86ae794b85b119dea881631e98: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-c48b542bd96d4a8aad80c1f0916a4207: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-eee1bc86ae794b85b119dea881631e98.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-cf43f3b9bd874d2f98ffc24956f49da3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-c48b542bd96d4a8aad80c1f0916a4207.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39534 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-cf43f3b9bd874d2f98ffc24956f49da3.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-ad65009d5f3d4a18811bd725803252a6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-ad65009d5f3d4a18811bd725803252a6.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-033149f77ab14ce6894205918b2236e3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-033149f77ab14ce6894205918b2236e3.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-2b9766a1a2e84b1bb13958fa450e4dc3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-2b9766a1a2e84b1bb13958fa450e4dc3.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-55831e84a2ea4c43837a6629a5a3a33b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-55831e84a2ea4c43837a6629a5a3a33b.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-f7b029449aad45728dccc46e605a9b16: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-f7b029449aad45728dccc46e605a9b16.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-bc3d449108cd4b49a0ebef6183aa7116: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-bc3d449108cd4b49a0ebef6183aa7116.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-41459ffe001f407ab736c1038854e331: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-41459ffe001f407ab736c1038854e331.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-ca734e314b194612b06e11a992c3eb20: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-ca734e314b194612b06e11a992c3eb20.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-75d455061332466d89f7f646bd4ec75b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-75d455061332466d89f7f646bd4ec75b.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-3a61e223c60d461fb1f0529113634be9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-3a61e223c60d461fb1f0529113634be9.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-82daa7cf2d2c417bbf9ef4bafec3e707: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-82daa7cf2d2c417bbf9ef4bafec3e707.
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [logger.py:43] Received request chatcmpl-41b08dff2563437ba305375a0e3ea0f9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:43 [async_llm.py:270] Added request chatcmpl-41b08dff2563437ba305375a0e3ea0f9.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-7460a248ae184936898dfba80c5de2fd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-7460a248ae184936898dfba80c5de2fd.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-8fb2836a20c94585a79fb944ac07dbc5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-8fb2836a20c94585a79fb944ac07dbc5.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-139fc59ca1894029ba357958674afbbd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-139fc59ca1894029ba357958674afbbd.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-672145be11b0421ab204a1a0a53b0ea8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-672145be11b0421ab204a1a0a53b0ea8.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-bac894fdbdce4bdf8a696ae02b85ccc1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-bac894fdbdce4bdf8a696ae02b85ccc1.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-85c9fd38ba1b4c7dbb548caf5361f961: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-85c9fd38ba1b4c7dbb548caf5361f961.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-dde662184efd485f8405a1cd80f531a8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-dde662184efd485f8405a1cd80f531a8.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-d2816c75cfb04ebbb8f897c9c2a2cdfb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-d2816c75cfb04ebbb8f897c9c2a2cdfb.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-28e337acb5074d58a82c04e714f4daed: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-28e337acb5074d58a82c04e714f4daed.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-1beb4ac992484b1ba93dfd6985c78ab2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-1beb4ac992484b1ba93dfd6985c78ab2.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-2c858b12073d4f14bc5498e14ad4b73f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-2c858b12073d4f14bc5498e14ad4b73f.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-84d09b30919842f6b2c44a233576d49a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-84d09b30919842f6b2c44a233576d49a.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-ee3e0af382c343c1bbf56281aa85525c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-ee3e0af382c343c1bbf56281aa85525c.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-f9c743ee523d42709abec9e85516933e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-f9c743ee523d42709abec9e85516933e.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-db55ef6b3c124805ac64138b08692145: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-db55ef6b3c124805ac64138b08692145.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-64a651fb63c74ff8872e6de815a3f191: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-64a651fb63c74ff8872e6de815a3f191.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-b1f866a90fc5464391c885b283587984: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-b1f866a90fc5464391c885b283587984.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-0d6f52ec9420409499ef2433a3a0173e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-0d6f52ec9420409499ef2433a3a0173e.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-999b4fdf8f714d8f9a8c0b6099cbd778: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-999b4fdf8f714d8f9a8c0b6099cbd778.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-06af24e740814a3cb71970c8cf31edd8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-06af24e740814a3cb71970c8cf31edd8.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-6322be0fa4d4423b81cb05d2c2d5fdc9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-6322be0fa4d4423b81cb05d2c2d5fdc9.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-3703278b6fc747afb16ad32af974256c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-3703278b6fc747afb16ad32af974256c.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-27f91a136db84d699f9022d8c2199613: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-27f91a136db84d699f9022d8c2199613.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-9233ec48db2e42bb84eab07d58556064: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-9233ec48db2e42bb84eab07d58556064.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-d6c5eeb107104ae1a7c86c328b48c072: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-d6c5eeb107104ae1a7c86c328b48c072.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-c3ec83e013774acfbada665ca13df482: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39900 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-c3ec83e013774acfbada665ca13df482.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-2e83d597a7ba4f27b90730d93d636097: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-2e83d597a7ba4f27b90730d93d636097.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-955f10974d274c7b9ea2e7dd3bc39456: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-955f10974d274c7b9ea2e7dd3bc39456.
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [logger.py:43] Received request chatcmpl-02f14f6cb4454fd4ba4c0c9da47ce157: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:44 [async_llm.py:270] Added request chatcmpl-02f14f6cb4454fd4ba4c0c9da47ce157.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-418885b9964346d3b6f55c7555cfc29a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-418885b9964346d3b6f55c7555cfc29a.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-85b080db579d487798555ad5765eb2c1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-85b080db579d487798555ad5765eb2c1.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-80bc80dd5af841a4a2cb9a0af6421086: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-80bc80dd5af841a4a2cb9a0af6421086.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-47629b132a4447b8b8431528ca047bd7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-47629b132a4447b8b8431528ca047bd7.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-3642dcae06174404b06194c3414b53ec: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-3642dcae06174404b06194c3414b53ec.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-551a3ee2f4b244db8807139dafc9ae87: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:39986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-551a3ee2f4b244db8807139dafc9ae87.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-c27462b154574a508f6288be9e51a50e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-c27462b154574a508f6288be9e51a50e.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-61e6f22dc8214674a40e377675bbc922: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-61e6f22dc8214674a40e377675bbc922.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-8f81447e50704d5fbaf168f179941477: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-8f81447e50704d5fbaf168f179941477.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-fe3183c82078433ca904e54d21e2af99: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-fe3183c82078433ca904e54d21e2af99.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-564a71c8b3994d079fc5460335f93f4d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-564a71c8b3994d079fc5460335f93f4d.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-6e7bd86c08a44cf082c49ed9a7c92d81: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-6e7bd86c08a44cf082c49ed9a7c92d81.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-fdd998c4dbfe416e967f5f7699bbc017: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-579c624dea624d089ae086edfd29fbb1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-fdd998c4dbfe416e967f5f7699bbc017.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40088 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-579c624dea624d089ae086edfd29fbb1.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-bae690c13f6c4f04b8a3e1101fb8b7e0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-bae690c13f6c4f04b8a3e1101fb8b7e0.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-e075304ad1034efd93a8c2b434d415fa: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-e075304ad1034efd93a8c2b434d415fa.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-378a34fb174b47cc95a25823b2a78803: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-378a34fb174b47cc95a25823b2a78803.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-ec133efa996547fcb91b4ed36e6adccc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-ec133efa996547fcb91b4ed36e6adccc.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-87fdd34467dc461ea01034ed32d7a85d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-87fdd34467dc461ea01034ed32d7a85d.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-9903c003e3d54a10ae29d82ce4a3fea2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40148 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-9903c003e3d54a10ae29d82ce4a3fea2.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-f4b63f0ed03748d39bf7e741c90ebb59: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-f4b63f0ed03748d39bf7e741c90ebb59.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-dc25e9254e5848cbb101cc76c3b101ad: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-dc25e9254e5848cbb101cc76c3b101ad.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-df2ce184a5914699ad5adf9a8ae39901: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-df2ce184a5914699ad5adf9a8ae39901.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-2336cb3c11544d61a55e8b5d712b9180: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-2336cb3c11544d61a55e8b5d712b9180.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-914ec2aee1f347ed9e3cb767dff7c138: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-914ec2aee1f347ed9e3cb767dff7c138.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-da4d7c2581fb4195bb2029cfe9c9938c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-da4d7c2581fb4195bb2029cfe9c9938c.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-bd8143f18fca455a93f96c682b6d75f5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-bd8143f18fca455a93f96c682b6d75f5.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-ba222e60e064477180119c691d691a88: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-ba222e60e064477180119c691d691a88.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-3162c931af4040ceb9ddccc2394453f7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-3162c931af4040ceb9ddccc2394453f7.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-9e18a2d23bf2401ea3c74e41346ca251: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-9e18a2d23bf2401ea3c74e41346ca251.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-f7b86f8507c44a2a98b8aa56c24df657: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-f7b86f8507c44a2a98b8aa56c24df657.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-4c4b75f710784705b06c5d87e1b10225: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-4c4b75f710784705b06c5d87e1b10225.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-9e2c23b35ee647ea9d07f953c3694918: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40280 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-9e2c23b35ee647ea9d07f953c3694918.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-9e7d91bfc47a49bf8ccff34eb0308131: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-9e7d91bfc47a49bf8ccff34eb0308131.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-68e75e8bffb5451785733cbfe43ece7a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-68e75e8bffb5451785733cbfe43ece7a.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-f1e6aa28dde14de197875864331ad796: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-f1e6aa28dde14de197875864331ad796.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-0e721e01caa446cc84d8334f9aaedbe5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-0e721e01caa446cc84d8334f9aaedbe5.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-a268e45ae60544f58ee9cbf0a2d614fd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-a268e45ae60544f58ee9cbf0a2d614fd.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-cdcc1b6c78434604a215e0f186b2f5f1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [async_llm.py:270] Added request chatcmpl-cdcc1b6c78434604a215e0f186b2f5f1.
[36mllm_server_1  |[0m INFO 07-21 18:27:45 [logger.py:43] Received request chatcmpl-5984c82ed0b04b11a98221c720a8cfb3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-5984c82ed0b04b11a98221c720a8cfb3.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-7e41a92892694acc9e88b63358d48317: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-7e41a92892694acc9e88b63358d48317.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-9cdb2bb4ef0d4735b3b40a7ecef0bee4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-9cdb2bb4ef0d4735b3b40a7ecef0bee4.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-db5115dfd6524b0cb3b139d3557ff109: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-db5115dfd6524b0cb3b139d3557ff109.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-1128cff0c9ee45fb9c9a2890cbdd2699: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-1128cff0c9ee45fb9c9a2890cbdd2699.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-d0e036854550493f9449343451ad23c2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-d0e036854550493f9449343451ad23c2.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-546ad125c92f4f678c99dc52a01c1004: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-546ad125c92f4f678c99dc52a01c1004.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-4e30fef58de3441a8e50b0da9ffb18d6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-4e30fef58de3441a8e50b0da9ffb18d6.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-1680f042c40745d7a35ac44d1711e94a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-1680f042c40745d7a35ac44d1711e94a.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-e03089096ea24932aa5b26e837409fa5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-e03089096ea24932aa5b26e837409fa5.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-49faaf8a35ff4232958508087fd5f656: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-49faaf8a35ff4232958508087fd5f656.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-9f4f55f5accb464d98152f06fa23573e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-9f4f55f5accb464d98152f06fa23573e.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-06bbb329e0b7481a8d39395d24d171fa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-06bbb329e0b7481a8d39395d24d171fa.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-a5063a03fe3b41e194eb7e3e849028e7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-a5063a03fe3b41e194eb7e3e849028e7.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-735b26f98a14452dba52fe484b8dd58f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-735b26f98a14452dba52fe484b8dd58f.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-b094ba69d3fa4e83991b5cceaa8c40bc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-b094ba69d3fa4e83991b5cceaa8c40bc.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-7e8e7edff5284a1d9b44e0510bee09eb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-7e8e7edff5284a1d9b44e0510bee09eb.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-4514a58ec45f4d118b37f13944757cd5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-4514a58ec45f4d118b37f13944757cd5.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-8836dad150d04b04b1b14678cb4b45aa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-8836dad150d04b04b1b14678cb4b45aa.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-9a02b9439cf74b49bf4d4110d83e689b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-9a02b9439cf74b49bf4d4110d83e689b.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-29a64f0f6c754707b8388693eebdf22a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-29a64f0f6c754707b8388693eebdf22a.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-14b556591a2d45379787872a173959b7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-14b556591a2d45379787872a173959b7.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-9c2c20ec5cd748a29c89d18e0f800ed5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-9c2c20ec5cd748a29c89d18e0f800ed5.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-88eda24f51934e57a0eff69ba7010990: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-88eda24f51934e57a0eff69ba7010990.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-84042ad081574d5d9ec6cf2f91a2f7b9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-84042ad081574d5d9ec6cf2f91a2f7b9.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-9c008fb173024addba0eeca1aea56342: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-9c008fb173024addba0eeca1aea56342.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-b4d25a2dfb1648afa02c62529f392315: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [async_llm.py:270] Added request chatcmpl-b4d25a2dfb1648afa02c62529f392315.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-05af44f4adfe46518d9dc3dc9b2b47b1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:46 [logger.py:43] Received request chatcmpl-b29e3c17831b4bbca470f063e740fce5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-05af44f4adfe46518d9dc3dc9b2b47b1.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-b29e3c17831b4bbca470f063e740fce5.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-9aabc0b9f22747e7a560624c9c33e5bb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-9aabc0b9f22747e7a560624c9c33e5bb.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-6a9768ea2ff64ca0aaa500503b8d1115: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-6a9768ea2ff64ca0aaa500503b8d1115.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-0907c5129802469e8ff39e96f95bd87f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-0907c5129802469e8ff39e96f95bd87f.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-7576c0127d2b432faaf2d533594dd251: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40656 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-7576c0127d2b432faaf2d533594dd251.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-f0590240f46d42409cb1f42745bb3439: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-f0590240f46d42409cb1f42745bb3439.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-73458469b3504df38abdd6d4eb7487dc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-73458469b3504df38abdd6d4eb7487dc.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-ecd5c667e37047679225cf11cfb0e2d1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-ecd5c667e37047679225cf11cfb0e2d1.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-a3c4a4564e554764a32134929a6deb42: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-a3c4a4564e554764a32134929a6deb42.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-9653e852d7a8418a8a025be545714553: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-9653e852d7a8418a8a025be545714553.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-20391b5fb90748bead7f2f84be5c1f2a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40712 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-20391b5fb90748bead7f2f84be5c1f2a.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-a2cf4a49d43347a48ebe0e40f2f81f12: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-a2cf4a49d43347a48ebe0e40f2f81f12.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-eaa5b9cd048b49c79c6c0141a690cdec: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-eaa5b9cd048b49c79c6c0141a690cdec.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-c886db17765f41fe8775107bf979c7b3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-c886db17765f41fe8775107bf979c7b3.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-496e9d09edd24d94accc4eb2480da977: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-496e9d09edd24d94accc4eb2480da977.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-0e19bf2baa4f40c48e6c5f7ab4699a4e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-0e19bf2baa4f40c48e6c5f7ab4699a4e.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-50bb329a45244897b153e3f4598bb0ff: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-50bb329a45244897b153e3f4598bb0ff.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-529ce3c582444233bf8dafd41223dda1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-529ce3c582444233bf8dafd41223dda1.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-faea5065b2274742aa69c5a0250ab31e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-faea5065b2274742aa69c5a0250ab31e.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-6dd660d8e3fb4eac983fe879ccb1673c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-6dd660d8e3fb4eac983fe879ccb1673c.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-5abb5fc0450d4367b783979a4b250879: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-5abb5fc0450d4367b783979a4b250879.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-eecc2284181e4b4b9f60f7bec8bd20e4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-eecc2284181e4b4b9f60f7bec8bd20e4.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-b021149f35824b7e942ec2a1f05ebf9b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-b021149f35824b7e942ec2a1f05ebf9b.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-26c3457e738e49ac898522df4eaf81a4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-26c3457e738e49ac898522df4eaf81a4.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-86671ff618e943ec9cc91bc3cbb54be1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-86671ff618e943ec9cc91bc3cbb54be1.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-bf8c3d7b7c934009bb8a9202c6392e12: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-bf8c3d7b7c934009bb8a9202c6392e12.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-a1faa788da2749a180c5e1bc172bb01c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-a1faa788da2749a180c5e1bc172bb01c.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-8c2cd6ed1090459e85c5593dd26aec89: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-8c2cd6ed1090459e85c5593dd26aec89.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-48a71d04eb964ddf9aee148af452eb34: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-a6d52b8238ad4ea792e47773c7a38ded: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [logger.py:43] Received request chatcmpl-4e006f8c220e423d9fbb40698e986caf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-48a71d04eb964ddf9aee148af452eb34.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-a6d52b8238ad4ea792e47773c7a38ded.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:47 [async_llm.py:270] Added request chatcmpl-4e006f8c220e423d9fbb40698e986caf.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-17275268d934425991be2c2c83127973: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-17275268d934425991be2c2c83127973.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-b07f40fe5f7a4d4e9515187ea34afc5e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-b07f40fe5f7a4d4e9515187ea34afc5e.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-4a49a014edcc492fa0b022d4231b37d6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-4a49a014edcc492fa0b022d4231b37d6.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-ac6f275cf9d5451ab532d883406a4a14: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-ac6f275cf9d5451ab532d883406a4a14.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-fdfc7510e0a54556a958b480dc98e283: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-fdfc7510e0a54556a958b480dc98e283.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-de2428dd891f4a2eb3ef461a759eb43e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-de2428dd891f4a2eb3ef461a759eb43e.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-0448c266ff89440a801a997fb44eea75: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-0448c266ff89440a801a997fb44eea75.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-a44fa869506f4a7285cabace0da174e3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-a44fa869506f4a7285cabace0da174e3.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-521c7915a03144508b585596d527f58a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:40992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-521c7915a03144508b585596d527f58a.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-a724217c0bdd49c6a198e664e2bb0931: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-a724217c0bdd49c6a198e664e2bb0931.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-8b55510bccfd4ab9a6fdee7ac9e19368: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-8b55510bccfd4ab9a6fdee7ac9e19368.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-be6b407c508e4f9887b22d06c956cca0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-be6b407c508e4f9887b22d06c956cca0.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-dd3dbb94d49643a9825998f5aec1d275: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-dd3dbb94d49643a9825998f5aec1d275.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-576d36b6c0ef421abbc3ba4453e0f2ac: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-576d36b6c0ef421abbc3ba4453e0f2ac.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-2b78544224eb4fb19cb586eb2cf086a6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-2b78544224eb4fb19cb586eb2cf086a6.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-a325b8eb6c3140efa23b7ee7c78fd5dc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-a325b8eb6c3140efa23b7ee7c78fd5dc.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-7874bb1983a9410b9b681cf3b225bee6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-7874bb1983a9410b9b681cf3b225bee6.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-b7b3786a6f5940bb989baee894cdd432: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-b7b3786a6f5940bb989baee894cdd432.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-f2e82997e4194ad68e28a15aa2d085e3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41108 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-f2e82997e4194ad68e28a15aa2d085e3.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-60c0a4b9930f4ca1b4667de7ed41406d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-60c0a4b9930f4ca1b4667de7ed41406d.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-cc92a4affbfa482280e4e76efd303df5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-cc92a4affbfa482280e4e76efd303df5.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-ecdefd9294a143eea46053bcdb70b600: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-ecdefd9294a143eea46053bcdb70b600.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-9ae2291d2ba74f049432d3aaf70a08b0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-9ae2291d2ba74f049432d3aaf70a08b0.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-26b55f2dbe3e4c56b283757a1dc67340: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-26b55f2dbe3e4c56b283757a1dc67340.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-9f079e111a484bb99acc1ce6224991bb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-9f079e111a484bb99acc1ce6224991bb.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-2554bff657fc4b9f90d25d4aebae6953: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-2554bff657fc4b9f90d25d4aebae6953.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-ca12939cb2be4558beeda7e878a63a69: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-ca12939cb2be4558beeda7e878a63a69.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-55197fa82add4ba994a9eb023735c277: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-55197fa82add4ba994a9eb023735c277.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-d9d821d898d44f10afba31c227499c74: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-d9d821d898d44f10afba31c227499c74.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-3874ca57d8864133b179dc79a323fa98: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-3874ca57d8864133b179dc79a323fa98.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-9548d04cbbf94d46be3f06e7255bedb9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-9548d04cbbf94d46be3f06e7255bedb9.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-38f32eb5ec5243638fbfded9cdb4a9db: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-38f32eb5ec5243638fbfded9cdb4a9db.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-1561220a4c45439eabb1976e474bf481: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-1561220a4c45439eabb1976e474bf481.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-c27e055baba5445da7847849eabd0074: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-c27e055baba5445da7847849eabd0074.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-b1c70d2cd426423cbf5bdf211e172570: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-b1c70d2cd426423cbf5bdf211e172570.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-d04565faeb8b428cb7c4adda0f4e2a70: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-d04565faeb8b428cb7c4adda0f4e2a70.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-dd76833d724a4054b69549e2fe00804d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-dd76833d724a4054b69549e2fe00804d.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-0e5df07282f445828e066dff4e076847: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-0e5df07282f445828e066dff4e076847.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-ba375e28bf0944a3ac7e48109a94ed45: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [logger.py:43] Received request chatcmpl-f32a42fe3f944c3d9d8cc6d2601ceb60: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-ba375e28bf0944a3ac7e48109a94ed45.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:48 [async_llm.py:270] Added request chatcmpl-f32a42fe3f944c3d9d8cc6d2601ceb60.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-5888d38a1f9a47dfa3583cab7a95e375: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-5888d38a1f9a47dfa3583cab7a95e375.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-82d7de0646584af3b243743bad54a054: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-82d7de0646584af3b243743bad54a054.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-60ad2a6ca5b14d52b9b1bc6fa36bfead: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-60ad2a6ca5b14d52b9b1bc6fa36bfead.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-93dccf6f9aa741fbaa660701a079f3f8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-93dccf6f9aa741fbaa660701a079f3f8.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-8f336f20f3a44472a600233f50c8ac23: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-8f336f20f3a44472a600233f50c8ac23.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-069e01a89a144667b0a6f2571afe5308: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-069e01a89a144667b0a6f2571afe5308.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-7f74799d7738444889b7b8310c00f8c1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-7f74799d7738444889b7b8310c00f8c1.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-c8044fb10281450d85428ca72d517cb5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-c8044fb10281450d85428ca72d517cb5.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-6cdfd36f8af74f6aa233c22502c8af67: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-6cdfd36f8af74f6aa233c22502c8af67.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-7966c2c99167455ba81e391da8fe07c2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-7966c2c99167455ba81e391da8fe07c2.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-caf98c8f3b9b4fd0b93f1556a8a9147e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-caf98c8f3b9b4fd0b93f1556a8a9147e.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-efc965ae97e6473a8a8012cac7d9b828: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-efc965ae97e6473a8a8012cac7d9b828.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-a2027422f01848448c073f62b3a5ebc5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-a2027422f01848448c073f62b3a5ebc5.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-3f3d4ba06dfc4c498277efca4ca3b063: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-3f3d4ba06dfc4c498277efca4ca3b063.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-6c57e591ef7e4655951bc558452a0ab1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-6c57e591ef7e4655951bc558452a0ab1.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-4519fbb6cfaa4a0eba47687b8b50849c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-4519fbb6cfaa4a0eba47687b8b50849c.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-faa73692a5284438a352cded866cdf92: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-faa73692a5284438a352cded866cdf92.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-d6da6d550d4540b8a7b049b3d1ba531c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-d6da6d550d4540b8a7b049b3d1ba531c.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-473d45e50f604faea2e7899ba1e6e172: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-473d45e50f604faea2e7899ba1e6e172.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-8722185ba16445609384cb39916c135c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-8722185ba16445609384cb39916c135c.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-0cfc56fa29d74d7e863b4da63a703485: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-0cfc56fa29d74d7e863b4da63a703485.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-cbebe9f78f724bec8b016c54f4748eb9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-cbebe9f78f724bec8b016c54f4748eb9.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-bec2dc9de5cb43ef881ed72475d41aef: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-bec2dc9de5cb43ef881ed72475d41aef.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-37a2dfe6854a41e69e6fecc0c0f2e979: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-37a2dfe6854a41e69e6fecc0c0f2e979.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-2319a8a215e045f0807b1b4f832e3937: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-2319a8a215e045f0807b1b4f832e3937.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-1d1737d7f34d4378bc2ea6579a8c2fb7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-1d1737d7f34d4378bc2ea6579a8c2fb7.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-0bfb1cdaee7b4817bde71f008d3ad25e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-0bfb1cdaee7b4817bde71f008d3ad25e.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-0e5c62733fc846e9906f5b771fa3e4f5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-0e5c62733fc846e9906f5b771fa3e4f5.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-63bcd0aff0004f0ea5a1ec659b6e9994: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-63bcd0aff0004f0ea5a1ec659b6e9994.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-287cc6d9bea74c0ea1372db8418737dc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-287cc6d9bea74c0ea1372db8418737dc.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-1533f4c3111a48308a26012e152851cc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41574 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-1533f4c3111a48308a26012e152851cc.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-868800eada1642bba70fd7d88312494d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-868800eada1642bba70fd7d88312494d.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-da8e6ac346914226a674dd70a293b5ab: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-da8e6ac346914226a674dd70a293b5ab.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-3bcb8ba5a1994bbea48b7b195e59f9f0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-3bcb8ba5a1994bbea48b7b195e59f9f0.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-4ed282a608744125991b4c9c8dfe1ffb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-4ed282a608744125991b4c9c8dfe1ffb.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-63f69b9e393445a1b1c7de0a58bfaafa: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-63f69b9e393445a1b1c7de0a58bfaafa.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-81031ae47b224eb8bc30ddb4f2ef7ba4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-81031ae47b224eb8bc30ddb4f2ef7ba4.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-7cf0cba8c6a64b0599716b93d5eb8f6b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-7cf0cba8c6a64b0599716b93d5eb8f6b.
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [logger.py:43] Received request chatcmpl-7b02457fde7745de9922ba36dd103e89: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:49 [async_llm.py:270] Added request chatcmpl-7b02457fde7745de9922ba36dd103e89.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-928ff5090d6a451599ac1159966b3c05: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41656 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-928ff5090d6a451599ac1159966b3c05.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-aab5909ee8e844b5b6f505359c4d867d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41666 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-aab5909ee8e844b5b6f505359c4d867d.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-b8b9438c56754d99b85b628e41808ecb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-b8b9438c56754d99b85b628e41808ecb.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-16b97af8adf24b879233b63466cc4a0a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-16b97af8adf24b879233b63466cc4a0a.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-e93ab8c7f9224a49924269e606e94f16: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-e93ab8c7f9224a49924269e606e94f16.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-8904e3fd56db4e3497a962da80a4490f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-8904e3fd56db4e3497a962da80a4490f.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-d9993f1548d04361b3aeaf48e934e2eb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-d9993f1548d04361b3aeaf48e934e2eb.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-02f41ac199964cd0afc9dc5cd2a0e6ba: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-02f41ac199964cd0afc9dc5cd2a0e6ba.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-9d14f6dd32fb4350b4e15fbc9db72473: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-9d14f6dd32fb4350b4e15fbc9db72473.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-2b1190d93d694ad5aa3d5a3041165646: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-2b1190d93d694ad5aa3d5a3041165646.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-be8bc2420fb240f2a1262fa556d504ba: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-be8bc2420fb240f2a1262fa556d504ba.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-4d32654eb2644c69a41db2300cc39961: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-4d32654eb2644c69a41db2300cc39961.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-530f89873144485baf90606d0e5fabed: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-530f89873144485baf90606d0e5fabed.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-4f3f820628964c85a640a473c6791371: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-4f3f820628964c85a640a473c6791371.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-303ce5aaa4854ed19ba9d651772d25c7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-303ce5aaa4854ed19ba9d651772d25c7.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-fa525bac694b421bab55deee140ba59e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-fa525bac694b421bab55deee140ba59e.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-16f89d4ba1af410cadde4b1bc5e609d4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-16f89d4ba1af410cadde4b1bc5e609d4.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-0d8ec609760a49528d1b7d89c084fc10: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-0d8ec609760a49528d1b7d89c084fc10.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-3fc0dff1b1864786a9ab8de50b8e88e9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-3fc0dff1b1864786a9ab8de50b8e88e9.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-64d5a6c2f4984766a8c8fd4728a255d2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-64d5a6c2f4984766a8c8fd4728a255d2.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-dde333fc72cb4f6d9eedb209a6446ad6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-dde333fc72cb4f6d9eedb209a6446ad6.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-e5f5ad9f581e45e8a1780714acdefee8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-e5f5ad9f581e45e8a1780714acdefee8.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-7aad9f4be7354c3688254737f1ba8aca: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-fccdc944fc184aaeb3f03198b1e5bf3d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-7aad9f4be7354c3688254737f1ba8aca.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-bb34309980324345bf42e8c61712d6ae: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-e79abb96c6f44763a877ed84d9b6f85e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-fccdc944fc184aaeb3f03198b1e5bf3d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-bb34309980324345bf42e8c61712d6ae.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-bc8f917879194c7bba6be63efc59b66d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-e79abb96c6f44763a877ed84d9b6f85e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-bc8f917879194c7bba6be63efc59b66d.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-b6ec57e42b80453da59f644b08a00469: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-b6ec57e42b80453da59f644b08a00469.
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [logger.py:43] Received request chatcmpl-17a0460fd6264185bddbc8c9a030a10a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:50 [async_llm.py:270] Added request chatcmpl-17a0460fd6264185bddbc8c9a030a10a.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-670af2b23d3b4abbbd9a5e61d47a40ea: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-670af2b23d3b4abbbd9a5e61d47a40ea.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-2918143c118a4fccadaf1c4a059a2d02: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-2918143c118a4fccadaf1c4a059a2d02.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-e20403ba00924e268233c6adb93e5177: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-e20403ba00924e268233c6adb93e5177.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-bb2600b008544d3c853ba66dd00cfc64: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-bb2600b008544d3c853ba66dd00cfc64.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-c411578a28fa4bd8be140768e7198b73: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-c411578a28fa4bd8be140768e7198b73.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-7510f2b04cea4b169cdfcc3abaf80ee2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-7510f2b04cea4b169cdfcc3abaf80ee2.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-b3f7ddb998674595a35138dc6183f181: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-b3f7ddb998674595a35138dc6183f181.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-583f9c773c9245cd897187085c06e6f6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-583f9c773c9245cd897187085c06e6f6.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-dc486e2b36ae47e79ec426e03d3a0f07: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-dc486e2b36ae47e79ec426e03d3a0f07.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-36806d4bef7a416bb2f2280d7194ce45: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-36806d4bef7a416bb2f2280d7194ce45.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-bf22f93ee1864febaccfa35e1f607147: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-bf22f93ee1864febaccfa35e1f607147.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-8e77a5812a2046a0a52dced32dde5c6b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-bfc691ce7e8c44f1a768e2899b901ae3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-8e77a5812a2046a0a52dced32dde5c6b.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-bfc691ce7e8c44f1a768e2899b901ae3.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-e89888585c20423a9c8b1474cbfabaaa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-e89888585c20423a9c8b1474cbfabaaa.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-48eefe55bb60447b9ab88e2bb53dde98: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-48eefe55bb60447b9ab88e2bb53dde98.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-9d0adeddd6af41028830e29cceb7f21e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-9d0adeddd6af41028830e29cceb7f21e.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-bc72cb2d9c5e4f0bb2e763fe4685db89: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-bc72cb2d9c5e4f0bb2e763fe4685db89.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-167e5c6aed7d41a7865f2016c2f4f947: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-167e5c6aed7d41a7865f2016c2f4f947.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-7d74adb02b2e492d8f08cae7e1b12305: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-7d74adb02b2e492d8f08cae7e1b12305.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-e298f85656f94c4f85b9c643bbc6f54c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-e298f85656f94c4f85b9c643bbc6f54c.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-bbc52043eee743d2baf5b0a9e2694f11: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-bbc52043eee743d2baf5b0a9e2694f11.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-492cb10fe92041e1b494577ebf37594a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-492cb10fe92041e1b494577ebf37594a.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-66d5017ed09d4d1e8544c15ea96753fe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-66d5017ed09d4d1e8544c15ea96753fe.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-d328e5b573fc4a5b8867267a07e363dc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-d328e5b573fc4a5b8867267a07e363dc.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-1db9a77ccb0f4925bba83a64ca1f1255: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-1db9a77ccb0f4925bba83a64ca1f1255.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-ee889ed940be48fb98c9fb087782bf23: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-ee889ed940be48fb98c9fb087782bf23.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-160fe93ec46b4a9baffb81f6f5f9da37: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-160fe93ec46b4a9baffb81f6f5f9da37.
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [logger.py:43] Received request chatcmpl-4c5cce5b832f4a80b5e7359853e12368: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:51 [async_llm.py:270] Added request chatcmpl-4c5cce5b832f4a80b5e7359853e12368.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-c0b80253b77c46b99d57bf6b01f5b1b4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-c0b80253b77c46b99d57bf6b01f5b1b4.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-faf9d4702d824c38b4c3158091ea298b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-faf9d4702d824c38b4c3158091ea298b.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-085471d011914640b6df6a94ccac4368: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-085471d011914640b6df6a94ccac4368.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [loggers.py:118] Engine 000: Avg prompt throughput: 439.8 tokens/s, Avg generation throughput: 2072.8 tokens/s, Running: 87 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.5%, Prefix cache hit rate: 83.3%
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [loggers.py:118] Engine 001: Avg prompt throughput: 450.5 tokens/s, Avg generation throughput: 2065.4 tokens/s, Running: 88 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.8%, Prefix cache hit rate: 83.6%
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [loggers.py:118] Engine 002: Avg prompt throughput: 426.1 tokens/s, Avg generation throughput: 2056.5 tokens/s, Running: 88 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.9%, Prefix cache hit rate: 83.4%
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [loggers.py:118] Engine 003: Avg prompt throughput: 446.3 tokens/s, Avg generation throughput: 2051.4 tokens/s, Running: 87 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.7%, Prefix cache hit rate: 83.5%
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-9b3daa9009934a35abbd474bcb4d05a4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-9b3daa9009934a35abbd474bcb4d05a4.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-1a2c14d8b5c14e138953b3fbf87b427b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-1a2c14d8b5c14e138953b3fbf87b427b.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-2ef85938224a43b2ae580ff8e5a10fd4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-2ef85938224a43b2ae580ff8e5a10fd4.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-4003c145359c4e008ba01298fd5fd2b6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-4003c145359c4e008ba01298fd5fd2b6.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-fd89c872fba547cfadffbaf119df9f70: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-fd89c872fba547cfadffbaf119df9f70.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-432829f1e4e3442d8bfe8297a4972a30: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-432829f1e4e3442d8bfe8297a4972a30.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-3ab3d101f87941588d75a93ff9340900: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-3ab3d101f87941588d75a93ff9340900.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-35d76efc6f6e4aac83ca67ab619a5442: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-35d76efc6f6e4aac83ca67ab619a5442.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-fbb95477f9f34e5b9e07b295d4bd9722: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-fbb95477f9f34e5b9e07b295d4bd9722.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-66cd489fd7e041069d3908b748a05946: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-66cd489fd7e041069d3908b748a05946.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-3462e50dac8045289d384400b2af494d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-3462e50dac8045289d384400b2af494d.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-5f8a74c9ba914682b2587a58bfed144b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-5f8a74c9ba914682b2587a58bfed144b.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-81532356d4354f93aa776991d4e118a4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42312 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-81532356d4354f93aa776991d4e118a4.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-2783d8b667e144e1abac1522bb921126: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-2783d8b667e144e1abac1522bb921126.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-69228d29184b4f709ab129e6792ee5ed: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-69228d29184b4f709ab129e6792ee5ed.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-d13bf64d7e58485ea1af14d5a0db6567: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-d13bf64d7e58485ea1af14d5a0db6567.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-f8f75f887a73411c97f7be2a399bd31e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-f8f75f887a73411c97f7be2a399bd31e.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-c0516928331d46e6bd6a913b0be2f3f6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-c0516928331d46e6bd6a913b0be2f3f6.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-e0d6cb9caf0a485bb1a4231899d71d6c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-e0d6cb9caf0a485bb1a4231899d71d6c.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-d9da8c8d14c3455288afeb06097caefe: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:42414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-d9da8c8d14c3455288afeb06097caefe.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-1949a438cd034a8391ea9d5418c0db49: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-1949a438cd034a8391ea9d5418c0db49.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-8cf0781b392046218aec1c1723b20c47: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-8cf0781b392046218aec1c1723b20c47.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-de163cb762b94478ba86a1f4dfad689b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-de163cb762b94478ba86a1f4dfad689b.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-8ccfd859be2b4f078d52c45f04384921: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-8ccfd859be2b4f078d52c45f04384921.
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [logger.py:43] Received request chatcmpl-7a2ff49ee59041d0afe3b6b487ae7916: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:52 [async_llm.py:270] Added request chatcmpl-7a2ff49ee59041d0afe3b6b487ae7916.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-486b8fa21a0548468d9b4b202b0342e1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-486b8fa21a0548468d9b4b202b0342e1.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-e88e6aea5353407bb1bdb0aa9766142d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-e88e6aea5353407bb1bdb0aa9766142d.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-7e85e408507c4a6b9aec0c6e624c3dc6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-7e85e408507c4a6b9aec0c6e624c3dc6.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-c703687ff95f43e0980b68d7f6fa6741: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-c703687ff95f43e0980b68d7f6fa6741.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-fbccd03e9737485ab0c8ecbbbb2eebc9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-fbccd03e9737485ab0c8ecbbbb2eebc9.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-04aba5ec649d4dce805916f8edd9bdbc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-04aba5ec649d4dce805916f8edd9bdbc.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-f59d32252dc84af5860b6e1dbdfd7276: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-f59d32252dc84af5860b6e1dbdfd7276.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-03f539b58f654223bfe255d37a5a3eca: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-9218928c9c224b13847e93eb56502ca8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-03f539b58f654223bfe255d37a5a3eca.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-9218928c9c224b13847e93eb56502ca8.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-1fc2b7461da94d0d92424efd75001835: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-1fc2b7461da94d0d92424efd75001835.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-2ed6f8b19f67423d91a57a513e411f45: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-2ed6f8b19f67423d91a57a513e411f45.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-67fc2f71272b446481930073fe59073e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-67fc2f71272b446481930073fe59073e.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-34f06eaf07f64098886d8598b1b73e4f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-34f06eaf07f64098886d8598b1b73e4f.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-c70f21d80e2b4ae6996bb37fb6c7745c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-c70f21d80e2b4ae6996bb37fb6c7745c.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-ae64e5fee13e4a9da0140100aed03a8e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-ae64e5fee13e4a9da0140100aed03a8e.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-ad404f52d8e24df9a502a3670f60d9e2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-ad404f52d8e24df9a502a3670f60d9e2.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-a5bfec9c5cc14e17b199c82ec7fee4d0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-a5bfec9c5cc14e17b199c82ec7fee4d0.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-b51fd6a60e2f489cbb6fea16e57fa463: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-b51fd6a60e2f489cbb6fea16e57fa463.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-db9aa39adab34a0f9cc79bca4d1fa758: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-db9aa39adab34a0f9cc79bca4d1fa758.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-7c76e011a2ff48769859b6bf38a2b6e4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-7c76e011a2ff48769859b6bf38a2b6e4.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-4e5b09c9997448b3b378d6f97ef6f58d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-4e5b09c9997448b3b378d6f97ef6f58d.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-f2c6c7cf03654939a8ba6d3b40b03034: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-f2c6c7cf03654939a8ba6d3b40b03034.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-d04d94a024c54b9d95b5ddd4659c46ab: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-d04d94a024c54b9d95b5ddd4659c46ab.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-3ecfec4780aa4242b01f474c861a4654: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44524 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-3ecfec4780aa4242b01f474c861a4654.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-940251aae5e04cf3a77eec92b0162031: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-940251aae5e04cf3a77eec92b0162031.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-a551d3b6a8bb4e2c93bf24ad618a91d9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-a551d3b6a8bb4e2c93bf24ad618a91d9.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-c09d2c3770824107ba21f710ffd1a882: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-c09d2c3770824107ba21f710ffd1a882.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-ad6936cfb8ce4e11ad95113f16b9a04a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-ad6936cfb8ce4e11ad95113f16b9a04a.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-e6b2655523554a3b988e2d7e00745253: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-e6b2655523554a3b988e2d7e00745253.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-b098c8337e62481f9882949c5ea2d54d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-b098c8337e62481f9882949c5ea2d54d.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-6fb8167d1d4a4947a82ff4bd4e0cf891: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-6fb8167d1d4a4947a82ff4bd4e0cf891.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-94a41ef472b642579dccb4f7f06eb7ec: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-94a41ef472b642579dccb4f7f06eb7ec.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-99cc85eb5d3e40c8866fb2cbe6c6e111: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-99cc85eb5d3e40c8866fb2cbe6c6e111.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-ef07fad2dfe940ba8b3d037628ce2850: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-ef07fad2dfe940ba8b3d037628ce2850.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-6fa74c40a3104b70acf8f38f46d7fe60: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44656 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-6fa74c40a3104b70acf8f38f46d7fe60.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-6901d44e2164498aa24692add0fb2814: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-6901d44e2164498aa24692add0fb2814.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-96050409940045c19bc50ae6646e7241: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-96050409940045c19bc50ae6646e7241.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-91dc4009490448bebcd0965b50b1e067: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-91dc4009490448bebcd0965b50b1e067.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-841a1ef7904046f0a1239a1f457710d3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-841a1ef7904046f0a1239a1f457710d3.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-47431473ab084578b5ba14208e804a94: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-47431473ab084578b5ba14208e804a94.
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [logger.py:43] Received request chatcmpl-f49d274260d047c18651bb29f2f11fbe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:53 [async_llm.py:270] Added request chatcmpl-f49d274260d047c18651bb29f2f11fbe.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-a122fe27bcd349f1a7d1dc979ba48bcc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-a122fe27bcd349f1a7d1dc979ba48bcc.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-a7d2f682c30f4cf587df01948cfc18b1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-a7d2f682c30f4cf587df01948cfc18b1.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-414b571669064793b7dd40968124585d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-414b571669064793b7dd40968124585d.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-b142c0920e504db492e29b9950bd75da: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-b142c0920e504db492e29b9950bd75da.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-a3de83c347f94081bcd34ee5b4cb6874: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-a3de83c347f94081bcd34ee5b4cb6874.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-8963344c9fe3417dad3eb10f032bd75f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-8963344c9fe3417dad3eb10f032bd75f.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-f545bb3e44e845168cd81ac0670ab4f0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-f545bb3e44e845168cd81ac0670ab4f0.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-bca49f4a626e4d9abf93d330fd4d4896: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-bca49f4a626e4d9abf93d330fd4d4896.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-9603a694568241dcad6e0e644a0f6cff: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-f73b2484db74458a9dcea197306ebf67: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-9603a694568241dcad6e0e644a0f6cff.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-f73b2484db74458a9dcea197306ebf67.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-aa55243d3b58495a937afbabdf866e36: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-aa55243d3b58495a937afbabdf866e36.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-2f18c69346344f9d9c3e4b07b3a4efca: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-2f18c69346344f9d9c3e4b07b3a4efca.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-27bfbf0ad7634cda9aa35a4f21a9abde: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-ff5eef9ceb4a44ceba137ad1aa4619e8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-94b6387e410f41a98afb85ab3e2d960f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-27bfbf0ad7634cda9aa35a4f21a9abde.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-ff5eef9ceb4a44ceba137ad1aa4619e8.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-94b6387e410f41a98afb85ab3e2d960f.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-7f9f072375974b34b84d1e55d6a86bad: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-daaef5b31ded40d68ce97f6eb6584913: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-7f9f072375974b34b84d1e55d6a86bad.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-daaef5b31ded40d68ce97f6eb6584913.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-37c0e2f1bb33484d9c6863bd6738a50d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-37c0e2f1bb33484d9c6863bd6738a50d.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-102ab59c26324b5cac8e544b587914bb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-2fce719512a447a09d6470eba59a9a6d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-102ab59c26324b5cac8e544b587914bb.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44900 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-2fce719512a447a09d6470eba59a9a6d.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-22ea98fbc0ff4328beb3def1f0226eee: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-22ea98fbc0ff4328beb3def1f0226eee.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-1a8640d17f974dc0970ae2e7af9d21d0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-1a8640d17f974dc0970ae2e7af9d21d0.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-7e5852a042e2470f81e4ada3b8546d47: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-7e5852a042e2470f81e4ada3b8546d47.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-f038fef643804dd69846933c41c9e1c3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-f038fef643804dd69846933c41c9e1c3.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-3123f51780724cf8bbc814c16f897508: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-3123f51780724cf8bbc814c16f897508.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-b0443488c44249cba7d6723c5c9771b3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-b0443488c44249cba7d6723c5c9771b3.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-2b4dfedd2a4046039a34ea3edcc298cb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-2b4dfedd2a4046039a34ea3edcc298cb.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-59abb2aaf1eb47acb5d75bb7d5399689: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-59abb2aaf1eb47acb5d75bb7d5399689.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-a419de8cb59a488cb3f4b0803645e0cf: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-a419de8cb59a488cb3f4b0803645e0cf.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-51585aa1b08f41ab87d0e85587c6bae7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-51585aa1b08f41ab87d0e85587c6bae7.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-c523490d81ff4a7fa9377cfb53c66896: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-c523490d81ff4a7fa9377cfb53c66896.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-fb06b85091e34cf5ba83b798a4f3a200: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-fb06b85091e34cf5ba83b798a4f3a200.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-188e6bf4b24240739743ad26cd5170c1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-188e6bf4b24240739743ad26cd5170c1.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-f9c22cf9ecf34136abe3a9459f5a86cb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-f9c22cf9ecf34136abe3a9459f5a86cb.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-8a78567e70c74f03be4a4028c28872d7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-8a78567e70c74f03be4a4028c28872d7.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-4fb46d0a6c204685b2eb872b11612059: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-4fb46d0a6c204685b2eb872b11612059.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-bf022d26d1c0472a8f24adbc0a2fdddb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-bf022d26d1c0472a8f24adbc0a2fdddb.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-6fe7082c573047a0a0c9f347ca0d0b0b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-6fe7082c573047a0a0c9f347ca0d0b0b.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-b823a6524f4f42dab92fcee81a391799: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45040 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-b823a6524f4f42dab92fcee81a391799.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-c5260b4fb0c8432382be11f0316dfc8c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-c5260b4fb0c8432382be11f0316dfc8c.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-170531879ed641e9b1a95446bc7799b2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-170531879ed641e9b1a95446bc7799b2.
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [logger.py:43] Received request chatcmpl-9d52a75f47164041aa43575d8850330d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:54 [async_llm.py:270] Added request chatcmpl-9d52a75f47164041aa43575d8850330d.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-1e67a648a10b41b68cf6b5f761395bb5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-1e67a648a10b41b68cf6b5f761395bb5.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-37ddd82aabc240829c7c783fa3fec524: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-37ddd82aabc240829c7c783fa3fec524.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-48ad79a4c7dc4a589c9cc3269565dc13: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-48ad79a4c7dc4a589c9cc3269565dc13.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-ef7375b75ff5488092de149fed53a3ff: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-ef7375b75ff5488092de149fed53a3ff.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-22310165a91247ea855802220266f28a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-22310165a91247ea855802220266f28a.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-ba03da083ffd44de8713f0e1c71817c8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-ba03da083ffd44de8713f0e1c71817c8.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-934a05d17cd54e5281e71b3e4bf30d72: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-934a05d17cd54e5281e71b3e4bf30d72.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-54c70c463af948d5b0704b57a3d86c02: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-54c70c463af948d5b0704b57a3d86c02.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-db625205ad034e21be03321d88f5e101: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-db625205ad034e21be03321d88f5e101.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-6480223661f74438a65ca023539a650b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-6480223661f74438a65ca023539a650b.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-ecb122615f034f478196f433ec9654e6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-ecb122615f034f478196f433ec9654e6.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-fa11006ebc8f4b388ba506af167c93d4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-fa11006ebc8f4b388ba506af167c93d4.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-7903276d76c1454cbceed7442df46279: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-7903276d76c1454cbceed7442df46279.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-f54eea50aa044638b6c52c59c5fabfc5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-f54eea50aa044638b6c52c59c5fabfc5.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-7396a841792441cea9a8401e489def6e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-7396a841792441cea9a8401e489def6e.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-99c8641535d448c09cc0459f98e46f05: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-99c8641535d448c09cc0459f98e46f05.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-01aa2de49c8e46e99a9e55fec8870404: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-01aa2de49c8e46e99a9e55fec8870404.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-37ebec958a33454ba4bca08af939202b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-37ebec958a33454ba4bca08af939202b.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-283b1587445d4d62be06db9d58583886: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-283b1587445d4d62be06db9d58583886.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-4bf17c3fafbf4c54bd45a1c6ca045966: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-4bf17c3fafbf4c54bd45a1c6ca045966.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-70e469c9843b48db981485d775f68660: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-70e469c9843b48db981485d775f68660.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-5a59ef99b5d24ec8928d2f2135fa5612: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45260 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-5a59ef99b5d24ec8928d2f2135fa5612.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-07bf277b61dd4671ab7f0e32a72bc7c5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-07bf277b61dd4671ab7f0e32a72bc7c5.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-79571342b6284267a123a99ac341a452: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-581d44dee81243f8a1491e23ac4e4f7c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-79571342b6284267a123a99ac341a452.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-581d44dee81243f8a1491e23ac4e4f7c.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-2341dc26d9024b8b98ff0b86beb34afd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-2341dc26d9024b8b98ff0b86beb34afd.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-74a8179f92064ec687f8422bd289f73f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-ac33a2b011624f52a623d20b5953ee8e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-74a8179f92064ec687f8422bd289f73f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-ac33a2b011624f52a623d20b5953ee8e.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-726e0fe70781486c9bd542356eb32759: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-726e0fe70781486c9bd542356eb32759.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-a7010f2aad8f403c8fe2f10cad2b56db: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-a7010f2aad8f403c8fe2f10cad2b56db.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-316be81f42914543b7c7cbe425c6ef39: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-316be81f42914543b7c7cbe425c6ef39.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-9b1fb033cc51450ab886d8dd542ea73f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-9b1fb033cc51450ab886d8dd542ea73f.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-2a193ecf6fb64398a3565d81f3fbbf41: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-2a193ecf6fb64398a3565d81f3fbbf41.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-282014a967ec4e8ca0f21b954272d0ee: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-282014a967ec4e8ca0f21b954272d0ee.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-1107e751ee1545be9b644d2155c45bee: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-1107e751ee1545be9b644d2155c45bee.
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [logger.py:43] Received request chatcmpl-1b2e116a2da34db0946f77739d5589b5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:55 [async_llm.py:270] Added request chatcmpl-1b2e116a2da34db0946f77739d5589b5.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-51dd5433c5974d17a3a504629d6e1743: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-51dd5433c5974d17a3a504629d6e1743.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-f807b034037848b890579d052b81d81b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-f807b034037848b890579d052b81d81b.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-8ce3201e005042128ee1087735d21fc4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-8ce3201e005042128ee1087735d21fc4.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-13142ddbb89047cd89da397c884d1cbf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-13142ddbb89047cd89da397c884d1cbf.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-f9414f26028b4f97a5801a0e4723ff12: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-f9414f26028b4f97a5801a0e4723ff12.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-0a4acbe638d84eb4be3ed4d04f960867: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-0a4acbe638d84eb4be3ed4d04f960867.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-ed8ec3df3e144298a623ae6a7c5046e2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-ed8ec3df3e144298a623ae6a7c5046e2.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-7c8ec9fef2724c648b20ded19b32af96: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-7c8ec9fef2724c648b20ded19b32af96.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-2d5a4285527e4c0abc7d6be74e0dea22: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-2d5a4285527e4c0abc7d6be74e0dea22.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-8073d9dfdc61469ea8555543229cc14b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-8073d9dfdc61469ea8555543229cc14b.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-beeda822833f43d2bb0947d949a6a53e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-beeda822833f43d2bb0947d949a6a53e.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-dcb481cc97224c01a59d72713d00ece9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-dcb481cc97224c01a59d72713d00ece9.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-0b46884760af4ff1b4b8dc3c03d7edc7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-0b46884760af4ff1b4b8dc3c03d7edc7.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-d30d901d2d5a4b498b7fd24490b37cf1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-d30d901d2d5a4b498b7fd24490b37cf1.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-8a2c2492a201496fbb5ffd2d693b9daa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-8a2c2492a201496fbb5ffd2d693b9daa.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-c6585ab052ec4a3aa394e691d2dd5f1f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-c6585ab052ec4a3aa394e691d2dd5f1f.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-f3e44d2739d14cad8ce0be5a6b5c534b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-f3e44d2739d14cad8ce0be5a6b5c534b.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-3173833860de4cf4a16b6d824566482d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-3173833860de4cf4a16b6d824566482d.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-fe160ca6c0524093bcdc27e2935a878b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-fe160ca6c0524093bcdc27e2935a878b.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-7eb790f06ddb4eb38d1775519784d462: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-7eb790f06ddb4eb38d1775519784d462.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-474918f5b57346c7ba128d7157c4e085: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-474918f5b57346c7ba128d7157c4e085.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-1f71b7f95a5240b1933abc98bb682edf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-1f71b7f95a5240b1933abc98bb682edf.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-e7e132087236459e94483d9f650c84b9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-e7e132087236459e94483d9f650c84b9.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-c77a2fe6598c49d8a4a4b53a65b422e4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-c77a2fe6598c49d8a4a4b53a65b422e4.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-3102148fcbfb4958a915442af7bc948f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-3102148fcbfb4958a915442af7bc948f.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-e2cbb77c25a849669798d6a3dcefe6ab: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-e2cbb77c25a849669798d6a3dcefe6ab.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-ab46246460be4ca493f602899812815d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-ab46246460be4ca493f602899812815d.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-3adda1b3e0ef4f9886544fdc9d5784b3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-3adda1b3e0ef4f9886544fdc9d5784b3.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-0a6fb32bfac8422ebada259902896ffc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-0a6fb32bfac8422ebada259902896ffc.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-8f9f5156c0bf4b509a48fe2f01846dc2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45616 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-8f9f5156c0bf4b509a48fe2f01846dc2.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-65cf0384bcb640888e4149ff765c0a7a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-65cf0384bcb640888e4149ff765c0a7a.
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [logger.py:43] Received request chatcmpl-524fcf73e7644d77ac25af2ed5e78c61: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:56 [async_llm.py:270] Added request chatcmpl-524fcf73e7644d77ac25af2ed5e78c61.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-ffe3eb2e7a7a4861878ba87bdbd070a7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-ffe3eb2e7a7a4861878ba87bdbd070a7.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-40b7cb1170964168900754219810a28a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-40b7cb1170964168900754219810a28a.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-8e8e5ec83ae14aec9b3cf5e1b73494c1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45666 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-8e8e5ec83ae14aec9b3cf5e1b73494c1.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-8e4b154a92954a8dbf0d6f552706b2fa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-8e4b154a92954a8dbf0d6f552706b2fa.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-3aaae87e56dc4ed88e2e2cdc7feed673: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-3aaae87e56dc4ed88e2e2cdc7feed673.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-2949f6c47d924c32bee10daa1829acde: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-2949f6c47d924c32bee10daa1829acde.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-4dc4c0626f734d2a9fab6258185fcfeb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-4dc4c0626f734d2a9fab6258185fcfeb.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-3b66ed1edf12425c85910cdf7ca41d10: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-3b66ed1edf12425c85910cdf7ca41d10.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-ed9d1e43080b4f08ac3ea2e388a564b7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-ed9d1e43080b4f08ac3ea2e388a564b7.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-32139e41a2ec4e8eb31ba3395adafbdb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-32139e41a2ec4e8eb31ba3395adafbdb.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-f23c1cb84de340c98937bbba2177559a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-f23c1cb84de340c98937bbba2177559a.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-9e4a0623e0bf4c9f8161f03fb6ae249b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-9e4a0623e0bf4c9f8161f03fb6ae249b.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-603391884ba34fe1a1b558ae1ec0e588: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-603391884ba34fe1a1b558ae1ec0e588.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-87aad5e1d587439c916209e41d577493: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-87aad5e1d587439c916209e41d577493.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-728d94b4b8b941bb9ec85aee90b43218: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-728d94b4b8b941bb9ec85aee90b43218.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-8bd9f9d0b9724ae0b32da6567adbfc23: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-8bd9f9d0b9724ae0b32da6567adbfc23.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-6109f8cea114425eacc31ad5b5bca1dc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-6109f8cea114425eacc31ad5b5bca1dc.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-345f9cf3d29c4c0e82dfa2727c5368ef: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45778 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-345f9cf3d29c4c0e82dfa2727c5368ef.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-afc0b298a6c54caf9290a958ce722d00: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-afc0b298a6c54caf9290a958ce722d00.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-22b9a32ba4aa483b8a60e19a6276b319: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-22b9a32ba4aa483b8a60e19a6276b319.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-daa2a662d93b49cbbfb7c1bc00827465: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-daa2a662d93b49cbbfb7c1bc00827465.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-f307ec7206934457bc0e85ad7cae1c37: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-f307ec7206934457bc0e85ad7cae1c37.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-39e15ecaa4f2476ea1af4466d0a61d7a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-39e15ecaa4f2476ea1af4466d0a61d7a.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-d20472b6d9d14c8f9d247c1e384464c3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-d20472b6d9d14c8f9d247c1e384464c3.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-120e26a9743d46c6a1157758a64e883d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-120e26a9743d46c6a1157758a64e883d.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-bc4fe336596449a2a37338128ceacabc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-bc4fe336596449a2a37338128ceacabc.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-dc5a9152992446a1a69d6df55b9c4842: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-dc5a9152992446a1a69d6df55b9c4842.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-0946f43266f046379713624723175079: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-0946f43266f046379713624723175079.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-0968204f26ab4cf4ba00c7000b43eda8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-0968204f26ab4cf4ba00c7000b43eda8.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-369ff65cb1f6468d9d729b77a24ae7cd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-369ff65cb1f6468d9d729b77a24ae7cd.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-7a060ce4fe494f7cb36bc72aee8d5cd7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-7a060ce4fe494f7cb36bc72aee8d5cd7.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-f13b19d132014b2faed54f29a977c757: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-f13b19d132014b2faed54f29a977c757.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-86ffd075bfae45978b5d1b546467ffea: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-86ffd075bfae45978b5d1b546467ffea.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-69408dcce47d49bfb931211de3cd9cb3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-69408dcce47d49bfb931211de3cd9cb3.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-f2e24b425ee945fbba4da657729acdc3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-f2e24b425ee945fbba4da657729acdc3.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-76fdd752e840438d91e39b55a6b1b510: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-76fdd752e840438d91e39b55a6b1b510.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-0c3e1c5e8fe8405497e05ca55cc0ed76: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-0c3e1c5e8fe8405497e05ca55cc0ed76.
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [logger.py:43] Received request chatcmpl-3f36db03c7ee447eb1f788b8709df228: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:57 [async_llm.py:270] Added request chatcmpl-3f36db03c7ee447eb1f788b8709df228.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-d89dd291d28e471994dc7acba648ab4f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-d89dd291d28e471994dc7acba648ab4f.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-5bb2807280ee4512b317b909cc2b46f9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-5bb2807280ee4512b317b909cc2b46f9.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-494856dc080941b99943c29a6da513ab: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-494856dc080941b99943c29a6da513ab.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-e3d1818938e64c70a945746dc8e2f203: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-e3d1818938e64c70a945746dc8e2f203.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-21f7826cbbdb4eb5b2c2363c07c50295: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-21f7826cbbdb4eb5b2c2363c07c50295.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-45e1f0ebfb934d64a95fe755fc3bbc95: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-45e1f0ebfb934d64a95fe755fc3bbc95.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-7938e2c1176e411f8d6898424fdd08dd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46040 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-7938e2c1176e411f8d6898424fdd08dd.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-0e0312ea759d46d788d234a304cf456c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-0e0312ea759d46d788d234a304cf456c.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-addbced6e8d14ef588612892ffe93d17: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-addbced6e8d14ef588612892ffe93d17.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-94139bc54ed44570929bf15e92e2d497: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-94139bc54ed44570929bf15e92e2d497.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-ffd0cff00bca4b59b078d2550146d9ee: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-ffd0cff00bca4b59b078d2550146d9ee.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-9e25b3416b4f4228911c247fc0386f97: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-9e25b3416b4f4228911c247fc0386f97.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-3d94c16493d34ebc87267e4e588f446a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-3d94c16493d34ebc87267e4e588f446a.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-7616e8f60b7e416ba88167f6518315b5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-7616e8f60b7e416ba88167f6518315b5.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-625169bf2be84ead92f04b88066eb679: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-625169bf2be84ead92f04b88066eb679.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-325015a967e9428c941fa05f78fb81f9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-0cbd143eb5e94fdfa696958b6188bc9f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-325015a967e9428c941fa05f78fb81f9.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-0cbd143eb5e94fdfa696958b6188bc9f.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-af756ac15deb409490f4327ec0799192: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-af756ac15deb409490f4327ec0799192.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-2777a46b7ad94ebdac2433db2cd12dac: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-2777a46b7ad94ebdac2433db2cd12dac.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-6eb2e9d864bc4e98b1b2c98335654f9c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-6eb2e9d864bc4e98b1b2c98335654f9c.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-fd15d5e0e3764e8fb446a61d6a2e8fe1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-fd15d5e0e3764e8fb446a61d6a2e8fe1.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-aa97486628ce46ae9bb9891c2405f0fb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-aa97486628ce46ae9bb9891c2405f0fb.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-f507e66b21cc41d9bbb0849d100eaa0f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-f507e66b21cc41d9bbb0849d100eaa0f.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-aa48734fb7f64609b3bac541290973ae: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-aa48734fb7f64609b3bac541290973ae.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-204921e05dea46f29c667e07fa493349: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-204921e05dea46f29c667e07fa493349.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-bf8cf957b9ea4602bb785af1ca708ecb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-bf8cf957b9ea4602bb785af1ca708ecb.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-768d710f97454642b3e0a83cb26477be: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-768d710f97454642b3e0a83cb26477be.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-76ace0c199be4b6bab9e5508d759a6cc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-76ace0c199be4b6bab9e5508d759a6cc.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-ac083e74ad5b4beea8c841147c80c559: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46260 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-ac083e74ad5b4beea8c841147c80c559.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-880cfaf699164762ba47185c2077a461: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-880cfaf699164762ba47185c2077a461.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-b235a8f0a23f4ea499b32dc501b22bcb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-b235a8f0a23f4ea499b32dc501b22bcb.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-ed4218033f7b402391747fbf7a87cff6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-ed4218033f7b402391747fbf7a87cff6.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-56f75acf98ca44e99ad2cd9391efa78c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-56f75acf98ca44e99ad2cd9391efa78c.
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [logger.py:43] Received request chatcmpl-da143856203849288ca7d36b4345fe3e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:58 [async_llm.py:270] Added request chatcmpl-da143856203849288ca7d36b4345fe3e.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-31d1fb6ef85c4f50b0dc51aa5c41526b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-31d1fb6ef85c4f50b0dc51aa5c41526b.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-ba2a101b040e4590a1c5c7c53e569fb8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-ba2a101b040e4590a1c5c7c53e569fb8.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-0044bcce559e449f8062bed85d4dcd83: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-0044bcce559e449f8062bed85d4dcd83.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-b7afe5c238154588b9eb56e23bbf0371: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-b7afe5c238154588b9eb56e23bbf0371.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-67bf18eeb1b2418e8f3936d5b337a93a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-67bf18eeb1b2418e8f3936d5b337a93a.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-dbc5820e7b1d4ac6abdb4d099cccbac5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-dbc5820e7b1d4ac6abdb4d099cccbac5.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-ef57d72f53c241a5a0ade5d90653ea9e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-ef57d72f53c241a5a0ade5d90653ea9e.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-713112fcb00243ea8b62b55c8a34d3b1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-713112fcb00243ea8b62b55c8a34d3b1.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-4379d5f0449d488392e7eb5832e8edc8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-4379d5f0449d488392e7eb5832e8edc8.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-c857715d55c94dac8cc0f8545309a8c1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-c857715d55c94dac8cc0f8545309a8c1.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-faa298a0a8c042c0aee26cc1ae1997d2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-faa298a0a8c042c0aee26cc1ae1997d2.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-7ac846307dd849b2b88cbd1ff3640406: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-7ac846307dd849b2b88cbd1ff3640406.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-c033ca655670424789819c48e3a0a53c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-c033ca655670424789819c48e3a0a53c.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-e3f4d67bfeae4eb7853be17ce0f192f5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-e3f4d67bfeae4eb7853be17ce0f192f5.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-184f4d4cd41548f78337d7bc4c530ca6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-184f4d4cd41548f78337d7bc4c530ca6.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-f45cadf4181c4a2ab29a49379631d61c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-f45cadf4181c4a2ab29a49379631d61c.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-962d0cb023c84585afdbc6a1cb253669: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-962d0cb023c84585afdbc6a1cb253669.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-5b6bb2191fc649d8a5eabc1c66fa83c6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-5b6bb2191fc649d8a5eabc1c66fa83c6.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-d4dbad077f54407a9a9e4ea565db17a0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-d4dbad077f54407a9a9e4ea565db17a0.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-abcd104ec0f3442b9af6b5e95a8d1c68: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-abcd104ec0f3442b9af6b5e95a8d1c68.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-2c53838c6a334173ae078a3128c8014a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-2c53838c6a334173ae078a3128c8014a.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-321a8b9d455b4aea83d79606a60a2a82: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-321a8b9d455b4aea83d79606a60a2a82.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-bb3b14fa70d14806b5c10a526f548c5e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-bb3b14fa70d14806b5c10a526f548c5e.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-bd625e614ad14c8896469a0616df1884: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-bd625e614ad14c8896469a0616df1884.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-c42a4e45fc9048208d0ca4b0df249a81: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-c42a4e45fc9048208d0ca4b0df249a81.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-a836d803f31849ff90dc2e1ff8c0945f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-a836d803f31849ff90dc2e1ff8c0945f.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-d8ae929d7ab54a57918b298b84d6320b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-d8ae929d7ab54a57918b298b84d6320b.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-946e7278460041b99bf05827762f2a08: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46552 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-946e7278460041b99bf05827762f2a08.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-78cfbf63853040f5b7c1c3e7ddd58fbc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-78cfbf63853040f5b7c1c3e7ddd58fbc.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-57ce81f9fba14efcae3b3b42d2a2cb9b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-57ce81f9fba14efcae3b3b42d2a2cb9b.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-23f264213d1f4b928ffdf46bdb573832: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-23f264213d1f4b928ffdf46bdb573832.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-facd8a36087245d68e6875dce0cba37b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-0f8b214992e043d2b623f7b2732dd777: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-facd8a36087245d68e6875dce0cba37b.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-0f8b214992e043d2b623f7b2732dd777.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-dd6a89021b384014a86991e5edb247ed: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-dd6a89021b384014a86991e5edb247ed.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-ee9ca472014c4bce9b6b216fe69b93db: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-ee9ca472014c4bce9b6b216fe69b93db.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-4dd42f66675d4782b10578fd96177bb0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-4dd42f66675d4782b10578fd96177bb0.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-256d6c2c371b4240b5e8850aa5ebc6e0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-256d6c2c371b4240b5e8850aa5ebc6e0.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-dff539b1a47740d7a834e17884e4e449: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-dff539b1a47740d7a834e17884e4e449.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-f8c1caae18ff4717977e82edab8858ed: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-f8c1caae18ff4717977e82edab8858ed.
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [logger.py:43] Received request chatcmpl-da159f90d02144509fc98dce1fdbe128: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:27:59 [async_llm.py:270] Added request chatcmpl-da159f90d02144509fc98dce1fdbe128.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-3f0c601ec96645ea86d31f9c9aab3d58: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-3f0c601ec96645ea86d31f9c9aab3d58.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-3ace586a5deb4997876c1f16876e5994: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-3ace586a5deb4997876c1f16876e5994.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-f12c6599c6934e148536ecbd6fe1917d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-f12c6599c6934e148536ecbd6fe1917d.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-5e060bc0b87e4118bd3337f8262da6b7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-5e060bc0b87e4118bd3337f8262da6b7.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-865927b3e0384201994c0833593052f4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-865927b3e0384201994c0833593052f4.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-dd0db8784ccd441f94ba41b9d625f7b2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-dd0db8784ccd441f94ba41b9d625f7b2.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-b621cd64eb24417c9225f9f7584f9ba8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-b621cd64eb24417c9225f9f7584f9ba8.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-2e519d82a613429683d2f14bcd344331: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-2e519d82a613429683d2f14bcd344331.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-9d6cc296eea44ae98dd341fa7deaaea7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-9d6cc296eea44ae98dd341fa7deaaea7.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-71d321f7b36f44588f9c0ba4c18a61f1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-71d321f7b36f44588f9c0ba4c18a61f1.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-59374ce8f9b54a35809a733cebbadf67: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-59374ce8f9b54a35809a733cebbadf67.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-8cbe80363e40433bb595aac1d71feb93: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-8cbe80363e40433bb595aac1d71feb93.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-df19404f8a144f3f84b53c0fbc447a07: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-df19404f8a144f3f84b53c0fbc447a07.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-8536fc4251654a9f871bc34f09b1c7b5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-8536fc4251654a9f871bc34f09b1c7b5.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-addb0d8ccc484177b7967043a428b67e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-addb0d8ccc484177b7967043a428b67e.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-1efa1e0710d147029510474d4afb0739: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-1efa1e0710d147029510474d4afb0739.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-efd8c0a816d441f892bbca3be939d124: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-efd8c0a816d441f892bbca3be939d124.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-69e8717f8cc842cba32987087538d417: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-69e8717f8cc842cba32987087538d417.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-d54fd08182b84c71ac142e53ee5d1050: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-d54fd08182b84c71ac142e53ee5d1050.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-49655432d1b34020ae84220bdb4c0cb8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-49655432d1b34020ae84220bdb4c0cb8.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-ab732ebec44f435b88e851e83d18ad42: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-ab732ebec44f435b88e851e83d18ad42.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-0a89edf239754001839da2f282c59760: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-0a89edf239754001839da2f282c59760.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-0f3ef463fc8746a0a2b20885cc5cf454: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-0f3ef463fc8746a0a2b20885cc5cf454.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-31b5fb270eb749c1bd09d7b769e69f0c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-31b5fb270eb749c1bd09d7b769e69f0c.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-3fdcd3b2eb7742fb8f08f08213c3f711: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46900 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-3fdcd3b2eb7742fb8f08f08213c3f711.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-30f82ba4161341d7bc0ed6031b44c494: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-30f82ba4161341d7bc0ed6031b44c494.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-f527f3cf76dc4f8fa6472489ac65064f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-f527f3cf76dc4f8fa6472489ac65064f.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-5176e39104f04d10b91a04b8dfee7f06: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-5176e39104f04d10b91a04b8dfee7f06.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-e561c38e55654443803b0f66a6431745: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-e561c38e55654443803b0f66a6431745.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-59e138fddd9c4367832aaa07e11e0e5c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-34f7e282dabd4f9983ed022047dca718: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-59e138fddd9c4367832aaa07e11e0e5c.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-34f7e282dabd4f9983ed022047dca718.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-5ed6cc12057247748a45fc83a07aa854: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-ea93a8befb044d61b1b381ddc5631d68: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-5ed6cc12057247748a45fc83a07aa854.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-ea93a8befb044d61b1b381ddc5631d68.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-9ac04587684e41a28cc79203779d181e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-9ac04587684e41a28cc79203779d181e.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-cd3ca9551a5f43c8b25d81490a494e36: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-cd3ca9551a5f43c8b25d81490a494e36.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-503df4e52f7544a6949acdc7fa8c6957: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-503df4e52f7544a6949acdc7fa8c6957.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-4acc332ea9a44268b1eb76f034d111c7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-4acc332ea9a44268b1eb76f034d111c7.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-5aa8b136474f4849b119bef40de0fb8f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-5aa8b136474f4849b119bef40de0fb8f.
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [logger.py:43] Received request chatcmpl-d6caa1449afe45569412f8438a31deea: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:00 [async_llm.py:270] Added request chatcmpl-d6caa1449afe45569412f8438a31deea.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-59686aa86146451d80f611dabfc0312f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-59686aa86146451d80f611dabfc0312f.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-4e360a9239e24647bc8c9aa712bb8642: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-4e360a9239e24647bc8c9aa712bb8642.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-20c83335a90f42589a6cfdfffcc385bb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-20c83335a90f42589a6cfdfffcc385bb.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-1e1554843ba8477ca128eb52db629745: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-1e1554843ba8477ca128eb52db629745.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-604b7bdeca854297acad627471b8cbe6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-604b7bdeca854297acad627471b8cbe6.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-672c73dac8cd4ff7ad02e7dee69c8868: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-672c73dac8cd4ff7ad02e7dee69c8868.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-d7944a8d22c04849a7ca70a5f841bc11: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-d7944a8d22c04849a7ca70a5f841bc11.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-49425ee909d34ea4a53c9209c7b8d73b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-49425ee909d34ea4a53c9209c7b8d73b.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-f9759dafd3d74cc6a266a631913324b7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-f9759dafd3d74cc6a266a631913324b7.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-fd791a8f3a024a5b847e359a99eed1c9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-fd791a8f3a024a5b847e359a99eed1c9.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-5b4df56a81a94325956d0ebae88359e6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-5b4df56a81a94325956d0ebae88359e6.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-6e2f28f57278423a8bb368ab5d29ca68: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-6e2f28f57278423a8bb368ab5d29ca68.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-d5762564d311431ca1c89f1e74b4ff64: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-d5762564d311431ca1c89f1e74b4ff64.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-d19c090b2b824af2b5dd1ca971aa6573: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-d19c090b2b824af2b5dd1ca971aa6573.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-dce730c3977c40cb84b558bc1f325a70: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47144 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-dce730c3977c40cb84b558bc1f325a70.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-8b18164b3f51454b874c2f6937f61ae8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-8b18164b3f51454b874c2f6937f61ae8.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-6efaa7cbe2e64e2282220d73dd63896a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-6efaa7cbe2e64e2282220d73dd63896a.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-f55e92be421e4e54a9bbcbf283745793: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-f55e92be421e4e54a9bbcbf283745793.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-43b47199741d42da81106b758b97d1ad: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-43b47199741d42da81106b758b97d1ad.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-cd9f8e4fd9aa44aa86cf754efda523f7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-cd9f8e4fd9aa44aa86cf754efda523f7.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-1ff979b396f9403c96de25b450dcb26f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-1ff979b396f9403c96de25b450dcb26f.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-6f344a2141044186b08d326d916ce53e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-6f344a2141044186b08d326d916ce53e.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-60e4a4dc8ced480fb9acd1f7f1a045eb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-60e4a4dc8ced480fb9acd1f7f1a045eb.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-70589b9dae334a9bb6b9c299ee0a857a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-c3466fcd5fc046a6a98ddb88938882ff: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-70589b9dae334a9bb6b9c299ee0a857a.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-c3466fcd5fc046a6a98ddb88938882ff.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-76ff4a2bd13b4b95849ff6b693aaa615: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-76ff4a2bd13b4b95849ff6b693aaa615.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-cb104e16c78b484796e6b08fb99513d7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-cb104e16c78b484796e6b08fb99513d7.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-25b72df766f042d394fa28df50306db0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47260 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-25b72df766f042d394fa28df50306db0.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-e4fb3269a0c8497b8430406d4eeacd4e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-e4fb3269a0c8497b8430406d4eeacd4e.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-ec3aad88b0b2461994595c1ef104c069: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-ec3aad88b0b2461994595c1ef104c069.
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [logger.py:43] Received request chatcmpl-ab2d776105eb4249b86572f114595fb8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:01 [async_llm.py:270] Added request chatcmpl-ab2d776105eb4249b86572f114595fb8.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-a9d737d828e54198b0f7e71ec4f1d721: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-a9d737d828e54198b0f7e71ec4f1d721.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-56b34f8310cb46648660f48013fa8c1e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-56b34f8310cb46648660f48013fa8c1e.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [loggers.py:118] Engine 000: Avg prompt throughput: 469.2 tokens/s, Avg generation throughput: 2198.1 tokens/s, Running: 97 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.2%, Prefix cache hit rate: 84.7%
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [loggers.py:118] Engine 001: Avg prompt throughput: 458.2 tokens/s, Avg generation throughput: 2189.7 tokens/s, Running: 95 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.3%, Prefix cache hit rate: 84.5%
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [loggers.py:118] Engine 002: Avg prompt throughput: 463.5 tokens/s, Avg generation throughput: 2181.0 tokens/s, Running: 95 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.8%, Prefix cache hit rate: 84.2%
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [loggers.py:118] Engine 003: Avg prompt throughput: 442.7 tokens/s, Avg generation throughput: 2176.4 tokens/s, Running: 95 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.1%, Prefix cache hit rate: 84.6%
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-3e6a8e7319cf474684e373f924a15f38: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47312 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-3e6a8e7319cf474684e373f924a15f38.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-09a8142ab6b64f9eb2bc97e571f07e74: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-09a8142ab6b64f9eb2bc97e571f07e74.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-d4dd94301eaf4475afce613b709bd5c4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-d4dd94301eaf4475afce613b709bd5c4.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-45e5eb7fd7804589aa63eba8ae56b6d6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-45e5eb7fd7804589aa63eba8ae56b6d6.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-491166d7eda94f01b792c5a686a62d00: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-491166d7eda94f01b792c5a686a62d00.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-af6b0dc717be40208c846ea2af6ece80: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-af6b0dc717be40208c846ea2af6ece80.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-6e2ec6802f7741bf93e18007077c58a7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-6e2ec6802f7741bf93e18007077c58a7.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-6a02a6deb8f446adb417bc28abb3494d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-2c44fb1286964387863184fc4192283c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-6a02a6deb8f446adb417bc28abb3494d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-2c44fb1286964387863184fc4192283c.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-3a1fd27e1afb4d2aaaceb9def2f77122: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-3a1fd27e1afb4d2aaaceb9def2f77122.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-81269c804df444dfba9da81a55951cd2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-81269c804df444dfba9da81a55951cd2.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-eaa369da8b2842ceb4aedac60af21561: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-eaa369da8b2842ceb4aedac60af21561.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-4808b36d2de94d58a988f94d41944ebd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-4808b36d2de94d58a988f94d41944ebd.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-20bce6640d3c468185eafb3e362ce45d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-20bce6640d3c468185eafb3e362ce45d.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-cbe432f2b6e24aefa90e5eb2e69b4591: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-cbe432f2b6e24aefa90e5eb2e69b4591.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-bf9edb9016c443cab2ec80e8d3dc1ce3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-bf9edb9016c443cab2ec80e8d3dc1ce3.
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [logger.py:43] Received request chatcmpl-d42c299aa10c449d91d825de5fd804bb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47436 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:28:02 [async_llm.py:270] Added request chatcmpl-d42c299aa10c449d91d825de5fd804bb.

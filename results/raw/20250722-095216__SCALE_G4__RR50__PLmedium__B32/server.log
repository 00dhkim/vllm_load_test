Attaching to vllm_load_test_llm_server_1
[36mllm_server_1  |[0m INFO 07-21 18:32:46 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:32:50 [api_server.py:1395] vLLM API server version 0.9.2
[36mllm_server_1  |[0m INFO 07-21 18:32:50 [cli_args.py:325] non-default args: {'model': '/llm', 'max_model_len': 32768, 'served_model_name': ['qwen3'], 'data_parallel_size': 4}
[36mllm_server_1  |[0m INFO 07-21 18:32:57 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
[36mllm_server_1  |[0m INFO 07-21 18:32:57 [config.py:1472] Using max model len 32768
[36mllm_server_1  |[0m INFO 07-21 18:32:57 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36mllm_server_1  |[0m INFO 07-21 18:32:58 [utils.py:364] Started DP Coordinator process (PID: 203)
[36mllm_server_1  |[0m INFO 07-21 18:33:03 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:33:03 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:33:03 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:33:03 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-21 18:33:03 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:05 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:05 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:05 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:05 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:05 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:05 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:05 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:05 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:05 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:05 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:05 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:05 [__init__.py:699] Port 18806 is already in use, trying port 18807
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:08 [parallel_state.py:935] Adjusting world_size=4 rank=0 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:08 [parallel_state.py:935] Adjusting world_size=4 rank=1 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:08 [parallel_state.py:935] Adjusting world_size=4 rank=3 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:08 [parallel_state.py:935] Adjusting world_size=4 rank=2 distributed_init_method=tcp://127.0.0.1:18807 for DP
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:08 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:08 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:08 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:08 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:08 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:08 [__init__.py:1152] Found nccl from library libnccl.so.2
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:08 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:08 [pynccl.py:70] vLLM is using nccl==2.26.2
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:09 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:09 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:09 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:09 [parallel_state.py:1076] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:09 [parallel_state.py:1076] rank 3 in world size 4 is assigned as DP rank 3, PP rank 0, TP rank 0, EP rank 3
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:09 [parallel_state.py:1076] rank 2 in world size 4 is assigned as DP rank 2, PP rank 0, TP rank 0, EP rank 2
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:09 [cuda_communicator.py:77] Using naive all2all manager.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:09 [parallel_state.py:1076] rank 1 in world size 4 is assigned as DP rank 1, PP rank 0, TP rank 0, EP rank 1
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:09 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:09 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:09 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:09 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:09 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:09 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:09 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:09 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:09 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:09 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:09 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:09 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:09 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:09 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:09 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:09 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:06,  1.06it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:02<00:06,  1.05s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:03<00:05,  1.09s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.14it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.04it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:05<00:02,  1.02s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:07<00:01,  1.05s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:08<00:00,  1.09s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:08<00:00,  1.04s/it]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m 
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:17 [default_loader.py:272] Loading weights took 8.35 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:17 [default_loader.py:272] Loading weights took 8.36 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:18 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 8.609383 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:18 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 8.618718 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:19 [default_loader.py:272] Loading weights took 9.33 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:19 [default_loader.py:272] Loading weights took 9.28 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:19 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 9.559442 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:19 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 9.509373 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:31 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_1/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:31 [backends.py:519] Dynamo bytecode transform time: 11.72 s
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:31 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_3/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:31 [backends.py:519] Dynamo bytecode transform time: 11.77 s
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:31 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_0/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:31 [backends.py:519] Dynamo bytecode transform time: 11.83 s
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:31 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/d121af2b0e/rank_0_2/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:31 [backends.py:519] Dynamo bytecode transform time: 12.06 s
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:33:37 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:33:37 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:33:37 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:33:37 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:34:23 [backends.py:193] Compiling a graph for general shape takes 51.05 s
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:34:23 [backends.py:193] Compiling a graph for general shape takes 51.59 s
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:34:24 [backends.py:193] Compiling a graph for general shape takes 51.52 s
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:34:26 [backends.py:193] Compiling a graph for general shape takes 54.22 s
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:34:46 [monitor.py:34] torch.compile takes 62.88 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:34:46 [monitor.py:34] torch.compile takes 63.36 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:34:47 [monitor.py:34] torch.compile takes 63.57 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:34:47 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:34:48 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:34:48 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:34:48 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:34:48 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:34:48 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:34:48 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:34:48 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:34:48 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:34:49 [monitor.py:34] torch.compile takes 65.94 s in total
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_1 pid=207)[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m   warnings.warn(
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:34:50 [gpu_worker.py:232] Available KV cache memory: 7.18 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:34:51 [kv_cache_utils.py:716] GPU KV cache size: 47,056 tokens
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:34:51 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.44x
[36mllm_server_1  |[0m Capturing CUDA graph shapes:   1%|▏         | 1/67 [00:03<04:06,  3.74s/it]Capturing CUDA graph shapes:   3%|▎         | 2/67 [00:04<02:01,  1.88s/it]Capturing CUDA graph shapes:   4%|▍         | 3/67 [00:04<01:21,  1.28s/it]Capturing CUDA graph shapes:   6%|▌         | 4/67 [00:05<01:03,  1.00s/it]Capturing CUDA graph shapes:   7%|▋         | 5/67 [00:06<00:52,  1.18it/s]Capturing CUDA graph shapes:   9%|▉         | 6/67 [00:06<00:46,  1.32it/s]Capturing CUDA graph shapes:  10%|█         | 7/67 [00:07<00:41,  1.43it/s]Capturing CUDA graph shapes:  12%|█▏        | 8/67 [00:07<00:39,  1.51it/s]Capturing CUDA graph shapes:  13%|█▎        | 9/67 [00:08<00:36,  1.58it/s]Capturing CUDA graph shapes:  15%|█▍        | 10/67 [00:08<00:34,  1.64it/s]Capturing CUDA graph shapes:  16%|█▋        | 11/67 [00:09<00:33,  1.69it/s]Capturing CUDA graph shapes:  18%|█▊        | 12/67 [00:10<00:31,  1.72it/s]Capturing CUDA graph shapes:  19%|█▉        | 13/67 [00:10<00:31,  1.71it/s]Capturing CUDA graph shapes:  21%|██        | 14/67 [00:11<00:30,  1.75it/s]Capturing CUDA graph shapes:  22%|██▏       | 15/67 [00:11<00:29,  1.75it/s]Capturing CUDA graph shapes:  24%|██▍       | 16/67 [00:12<00:29,  1.72it/s]Capturing CUDA graph shapes:  25%|██▌       | 17/67 [00:12<00:28,  1.75it/s]Capturing CUDA graph shapes:  27%|██▋       | 18/67 [00:13<00:27,  1.77it/s]Capturing CUDA graph shapes:  28%|██▊       | 19/67 [00:13<00:26,  1.78it/s]Capturing CUDA graph shapes:  30%|██▉       | 20/67 [00:14<00:26,  1.79it/s]Capturing CUDA graph shapes:  31%|███▏      | 21/67 [00:15<00:25,  1.77it/s]Capturing CUDA graph shapes:  33%|███▎      | 22/67 [00:15<00:25,  1.80it/s]Capturing CUDA graph shapes:  34%|███▍      | 23/67 [00:16<00:24,  1.80it/s]Capturing CUDA graph shapes:  36%|███▌      | 24/67 [00:16<00:24,  1.76it/s]Capturing CUDA graph shapes:  37%|███▋      | 25/67 [00:17<00:23,  1.78it/s]Capturing CUDA graph shapes:  39%|███▉      | 26/67 [00:17<00:22,  1.78it/s]Capturing CUDA graph shapes:  40%|████      | 27/67 [00:18<00:22,  1.77it/s]Capturing CUDA graph shapes:  42%|████▏     | 28/67 [00:19<00:21,  1.78it/s]Capturing CUDA graph shapes:  43%|████▎     | 29/67 [00:19<00:21,  1.79it/s]Capturing CUDA graph shapes:  45%|████▍     | 30/67 [00:20<00:20,  1.79it/s]Capturing CUDA graph shapes:  46%|████▋     | 31/67 [00:20<00:20,  1.79it/s]Capturing CUDA graph shapes:  48%|████▊     | 32/67 [00:21<00:19,  1.79it/s]Capturing CUDA graph shapes:  49%|████▉     | 33/67 [00:21<00:18,  1.81it/s]Capturing CUDA graph shapes:  51%|█████     | 34/67 [00:22<00:18,  1.82it/s]Capturing CUDA graph shapes:  52%|█████▏    | 35/67 [00:22<00:17,  1.84it/s]Capturing CUDA graph shapes:  54%|█████▎    | 36/67 [00:23<00:16,  1.85it/s]Capturing CUDA graph shapes:  55%|█████▌    | 37/67 [00:23<00:16,  1.86it/s]Capturing CUDA graph shapes:  57%|█████▋    | 38/67 [00:24<00:15,  1.86it/s]Capturing CUDA graph shapes:  58%|█████▊    | 39/67 [00:25<00:15,  1.86it/s]Capturing CUDA graph shapes:  60%|█████▉    | 40/67 [00:25<00:14,  1.80it/s]Capturing CUDA graph shapes:  61%|██████    | 41/67 [00:26<00:14,  1.79it/s]Capturing CUDA graph shapes:  63%|██████▎   | 42/67 [00:26<00:13,  1.80it/s]Capturing CUDA graph shapes:  64%|██████▍   | 43/67 [00:27<00:13,  1.81it/s]Capturing CUDA graph shapes:  66%|██████▌   | 44/67 [00:27<00:12,  1.83it/s]Capturing CUDA graph shapes:  67%|██████▋   | 45/67 [00:28<00:11,  1.84it/s]Capturing CUDA graph shapes:  69%|██████▊   | 46/67 [00:28<00:11,  1.85it/s]Capturing CUDA graph shapes:  70%|███████   | 47/67 [00:29<00:10,  1.85it/s]Capturing CUDA graph shapes:  72%|███████▏  | 48/67 [00:29<00:10,  1.86it/s]Capturing CUDA graph shapes:  73%|███████▎  | 49/67 [00:30<00:09,  1.88it/s]Capturing CUDA graph shapes:  75%|███████▍  | 50/67 [00:30<00:08,  1.89it/s]Capturing CUDA graph shapes:  76%|███████▌  | 51/67 [00:31<00:08,  1.88it/s]Capturing CUDA graph shapes:  78%|███████▊  | 52/67 [00:32<00:07,  1.90it/s]Capturing CUDA graph shapes:  79%|███████▉  | 53/67 [00:32<00:07,  1.85it/s]Capturing CUDA graph shapes:  81%|████████  | 54/67 [00:33<00:06,  1.88it/s]Capturing CUDA graph shapes:  82%|████████▏ | 55/67 [00:33<00:06,  1.90it/s]Capturing CUDA graph shapes:  84%|████████▎ | 56/67 [00:34<00:05,  1.89it/s]Capturing CUDA graph shapes:  85%|████████▌ | 57/67 [00:34<00:05,  1.91it/s]Capturing CUDA graph shapes:  87%|████████▋ | 58/67 [00:35<00:04,  1.91it/s]Capturing CUDA graph shapes:  88%|████████▊ | 59/67 [00:35<00:04,  1.92it/s]Capturing CUDA graph shapes:  90%|████████▉ | 60/67 [00:36<00:03,  1.92it/s]Capturing CUDA graph shapes:  91%|█████████ | 61/67 [00:36<00:03,  1.93it/s]Capturing CUDA graph shapes:  93%|█████████▎| 62/67 [00:37<00:02,  1.93it/s]Capturing CUDA graph shapes:  94%|█████████▍| 63/67 [00:37<00:02,  1.94it/s]Capturing CUDA graph shapes:  96%|█████████▌| 64/67 [00:38<00:01,  1.94it/s]Capturing CUDA graph shapes:  97%|█████████▋| 65/67 [00:38<00:01,  1.95it/s]Capturing CUDA graph shapes:  99%|█████████▊| 66/67 [00:39<00:00,  1.96it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:39<00:00,  1.96it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:39<00:00,  1.68it/s]
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:35:28 [gpu_model_runner.py:2326] Graph capturing finished in 40 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:35:28 [gpu_model_runner.py:2326] Graph capturing finished in 37 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:35:28 [gpu_model_runner.py:2326] Graph capturing finished in 39 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:35:28 [gpu_model_runner.py:2326] Graph capturing finished in 39 secs, took 0.71 GiB
[36mllm_server_1  |[0m [1;36m(EngineCore_0 pid=206)[0;0m INFO 07-21 18:35:28 [core.py:172] init engine (profile, create kv cache, warmup model) took 129.60 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_2 pid=208)[0;0m INFO 07-21 18:35:28 [core.py:172] init engine (profile, create kv cache, warmup model) took 128.48 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_3 pid=209)[0;0m INFO 07-21 18:35:28 [core.py:172] init engine (profile, create kv cache, warmup model) took 128.44 seconds
[36mllm_server_1  |[0m [1;36m(EngineCore_1 pid=207)[0;0m INFO 07-21 18:35:28 [core.py:172] init engine (profile, create kv cache, warmup model) took 129.62 seconds
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 11764
[36mllm_server_1  |[0m WARNING 07-21 18:35:28 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:29] Available routes are:
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /docs, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /health, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /load, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /ping, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /ping, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /tokenize, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /detokenize, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /v1/models, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /version, Methods: GET
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /v1/completions, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /v1/embeddings, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /pooling, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /classify, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /score, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /v1/score, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /v1/audio/translations, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /v1/rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /v2/rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /invocations, Methods: POST
[36mllm_server_1  |[0m INFO 07-21 18:35:28 [launcher.py:37] Route: /metrics, Methods: GET
[36mllm_server_1  |[0m INFO:     Started server process [7]
[36mllm_server_1  |[0m INFO:     Waiting for application startup.
[36mllm_server_1  |[0m INFO:     Application startup complete.
[36mllm_server_1  |[0m INFO:     172.22.0.1:59710 - "GET /health HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:41 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36mllm_server_1  |[0m INFO 07-21 18:35:41 [logger.py:43] Received request chatcmpl-616c0eb1d921410ba9725d59588a2976: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:41050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:41 [async_llm.py:270] Added request chatcmpl-616c0eb1d921410ba9725d59588a2976.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-ac3a42da60814c1cac7f745ff1e35190: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-ac3a42da60814c1cac7f745ff1e35190.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-0ed8e72b27c5403fa01222ce08690ec2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-0ed8e72b27c5403fa01222ce08690ec2.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-c57a4e8a09774407957bcede1a996ede: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-c57a4e8a09774407957bcede1a996ede.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-c7008eac342a4900881afc560d080e4d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-c7008eac342a4900881afc560d080e4d.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-34e4384e92a14f75a32d46b8516affe9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-bf959584592d4213a873b7823a68a08e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-34e4384e92a14f75a32d46b8516affe9.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-bf959584592d4213a873b7823a68a08e.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-a3723c299a204fdd9b68a0a54455070d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-a3723c299a204fdd9b68a0a54455070d.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-624d77240ec54c9094179eac7621f31d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-624d77240ec54c9094179eac7621f31d.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-a13c8e1118f54db09d2a4fa3c2c57bf2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-a13c8e1118f54db09d2a4fa3c2c57bf2.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-1b227c4f8980455cb0341c0acc6c8767: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-1b227c4f8980455cb0341c0acc6c8767.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-6c0798081e0a4da4acfcbe93aa323f55: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-6c0798081e0a4da4acfcbe93aa323f55.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-491902c3b4944a7a8f2642d828e2d8eb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-491902c3b4944a7a8f2642d828e2d8eb.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-5e03a56a6cea487898e769f82ca9e947: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-5e03a56a6cea487898e769f82ca9e947.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-43d07b1d1ae8491ea0ef69c87b0bae70: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-43d07b1d1ae8491ea0ef69c87b0bae70.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-150ba2e6c9c347369224ec4d8f31b129: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-150ba2e6c9c347369224ec4d8f31b129.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-ad907d4a7d0748ce9d75c27a8453b2e5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-d5da450081d041b1a38d0804a4e4e301: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-ad907d4a7d0748ce9d75c27a8453b2e5.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-d5da450081d041b1a38d0804a4e4e301.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-8ba1c21edd3c4b4ebbba072e9e1c8680: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-8ba1c21edd3c4b4ebbba072e9e1c8680.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-e48a33c9a54e401eb488340f20342972: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:53994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-e48a33c9a54e401eb488340f20342972.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-c1f60c46892347718d9ec54463bcfde5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-c1f60c46892347718d9ec54463bcfde5.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-12ce2c1c92e74f90b60d78559d228d2f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-12ce2c1c92e74f90b60d78559d228d2f.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-0fb757eb8b254e46b5dcc172d6e7cf35: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-0fb757eb8b254e46b5dcc172d6e7cf35.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-d32532e10de647f3b48194e0693b8a75: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-d32532e10de647f3b48194e0693b8a75.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-2198cd71f41a4b5bb63cf03cc9420a03: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-2198cd71f41a4b5bb63cf03cc9420a03.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-f591ee9078f643ae97c31f3d0873e27a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-f591ee9078f643ae97c31f3d0873e27a.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-1c8f385e417d4a679904942f4458dcef: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-1c8f385e417d4a679904942f4458dcef.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-e687c454286344e291e2c2a71e68d11e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-e687c454286344e291e2c2a71e68d11e.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-80a9a58823cf48c4be6ef7e4f7aa03e6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-80a9a58823cf48c4be6ef7e4f7aa03e6.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-4b124c3bee324526b15101bb8f0bcbd7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-4b124c3bee324526b15101bb8f0bcbd7.
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [logger.py:43] Received request chatcmpl-308f149f3ec943e6b8ee5b5309ba07ac: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54088 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:48 [async_llm.py:270] Added request chatcmpl-308f149f3ec943e6b8ee5b5309ba07ac.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-d9bdb318990746b786b294c08d148461: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-d9bdb318990746b786b294c08d148461.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-1552aafc2ffb49e7971364f2c95754e4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-1552aafc2ffb49e7971364f2c95754e4.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-a0b267984e13487bbf9d1fe34af4f072: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-a0b267984e13487bbf9d1fe34af4f072.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-e174a5ce744842df94a7021a1d904ece: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-e174a5ce744842df94a7021a1d904ece.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-45f2a70155af4efea9d478e6c694d350: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-45f2a70155af4efea9d478e6c694d350.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-7dbb3300384e4dd7b1291727e7c927ce: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-7dbb3300384e4dd7b1291727e7c927ce.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-63cb2d342526430eb5952c8ba7905b17: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-63cb2d342526430eb5952c8ba7905b17.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-f925460e73b141e5a599c53eba6daecc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-f925460e73b141e5a599c53eba6daecc.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-770d1ce2245d40c8bfb332d5daa25466: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-770d1ce2245d40c8bfb332d5daa25466.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-71ad86c3d3b540a3a78a032a56f87bfa: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-71ad86c3d3b540a3a78a032a56f87bfa.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-842ec37e104a4821aaa09e0f7690c782: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-842ec37e104a4821aaa09e0f7690c782.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-f4e33e6e138d4a72a35cbdf2c64f4ab7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-f4e33e6e138d4a72a35cbdf2c64f4ab7.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-17aa8013a2584e51bc2fe6f9254200d4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-17aa8013a2584e51bc2fe6f9254200d4.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-2c57c8bfc1244ca588692dea0ad71113: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-2c57c8bfc1244ca588692dea0ad71113.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-1e4e665d9fd140318feeda85c20d1e06: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-1e4e665d9fd140318feeda85c20d1e06.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-0b5830666ccd446f9ad580f506e81f91: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-0b5830666ccd446f9ad580f506e81f91.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-1bdb6601c79f4137b5473e535f3bb074: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-1bdb6601c79f4137b5473e535f3bb074.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-ee951086fa9e4ed6b554f2fdb9943279: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [loggers.py:118] Engine 000: Avg prompt throughput: 62.5 tokens/s, Avg generation throughput: 46.9 tokens/s, Running: 11 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 56.3%
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [loggers.py:118] Engine 001: Avg prompt throughput: 61.7 tokens/s, Avg generation throughput: 20.6 tokens/s, Running: 11 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 62.2%
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [loggers.py:118] Engine 002: Avg prompt throughput: 59.0 tokens/s, Avg generation throughput: 20.9 tokens/s, Running: 11 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 56.9%
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [loggers.py:118] Engine 003: Avg prompt throughput: 57.6 tokens/s, Avg generation throughput: 18.1 tokens/s, Running: 10 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 58.3%
[36mllm_server_1  |[0m INFO:     172.22.0.1:54240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-ee951086fa9e4ed6b554f2fdb9943279.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-eadaa25efd9c42eda96d127b9dbfc174: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-eadaa25efd9c42eda96d127b9dbfc174.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-8ccc7fbbd7714e2890cbd8b8b4b68a3c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-8ccc7fbbd7714e2890cbd8b8b4b68a3c.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-aabe21b3c77648e8bde7088bd1b9611e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-aabe21b3c77648e8bde7088bd1b9611e.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-56b74cd4d7294c2dab9edfe92f067e68: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-56b74cd4d7294c2dab9edfe92f067e68.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-d3e530cc598c4f038db7b51ea7317807: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-d3e530cc598c4f038db7b51ea7317807.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-4cf85130ccc6460c838aad3068e70b95: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54312 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-4cf85130ccc6460c838aad3068e70b95.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-43bda65a0ba241c5b20e03eb216eb1ca: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-bc162b8354184d7bba94aa6e3b6f9989: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-3331134ec78243faaf242117dfd74a44: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-43bda65a0ba241c5b20e03eb216eb1ca.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-bc162b8354184d7bba94aa6e3b6f9989.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-3331134ec78243faaf242117dfd74a44.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-3a0c6f07672d43e28698177144cc9aca: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-3a0c6f07672d43e28698177144cc9aca.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-208316b2010e4eceac5097ab6fb1d5b2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-208316b2010e4eceac5097ab6fb1d5b2.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-96c8c70c59014428b19b82ce60118002: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-96c8c70c59014428b19b82ce60118002.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-3533c9c4f54545b0bcc39f8451fcf1b4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-95fc2b76867a44c7a217920faec7d31b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-3533c9c4f54545b0bcc39f8451fcf1b4.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-8af80587433947a2890fcf1ab642edd8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-95fc2b76867a44c7a217920faec7d31b.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-8af80587433947a2890fcf1ab642edd8.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-6c324fd48d834aadba05d5d24df349ee: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-6c324fd48d834aadba05d5d24df349ee.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-57b271d025794aeb852306dbae637d1d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-57b271d025794aeb852306dbae637d1d.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-04566b0c2bca4ee3b38b846077e12c19: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-04566b0c2bca4ee3b38b846077e12c19.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-aa4136556dbd4d5da559bee2ccf04e2d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-aa4136556dbd4d5da559bee2ccf04e2d.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-46735d5dd08644c080e7efcbbaf01b80: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-facfb0ced0404b76bf04a8fb09c986b3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-46735d5dd08644c080e7efcbbaf01b80.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-f03162b1057b4024a6f7e3c1a773f2e2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-facfb0ced0404b76bf04a8fb09c986b3.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-11108a46848e4c2bad6d2970d610c34c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-f03162b1057b4024a6f7e3c1a773f2e2.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-11108a46848e4c2bad6d2970d610c34c.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-4d68880bcd384e55a4abf68bc1cbf893: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-4d68880bcd384e55a4abf68bc1cbf893.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-1ae0b74b8074426189f4f5a1fb23ba05: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-1ae0b74b8074426189f4f5a1fb23ba05.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-dbc5a764c7c447ce88731360a2273284: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-dbc5a764c7c447ce88731360a2273284.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-07de244b4c454bc893a21485bc130086: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-07de244b4c454bc893a21485bc130086.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-d189812e49ad4795b543e8122abd1ed1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-4348db97fdf24c0bbd0436803b50a7bb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-d189812e49ad4795b543e8122abd1ed1.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-4348db97fdf24c0bbd0436803b50a7bb.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-9aaafa67288144d2a0d9e1bca1f54124: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-9aaafa67288144d2a0d9e1bca1f54124.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-3ee0626b5d0341d5a93713b517177128: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-3ee0626b5d0341d5a93713b517177128.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-24af688ca49749238ba96060ba88d07d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-24af688ca49749238ba96060ba88d07d.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-0f417dfc7f5e4fdd8d8c1e7c0c84ee24: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-be53e32335484d268c1aec9b010dc2a4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-0f417dfc7f5e4fdd8d8c1e7c0c84ee24.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-be53e32335484d268c1aec9b010dc2a4.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-91cc5702c4994e96a11bf96bd1b23188: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-91cc5702c4994e96a11bf96bd1b23188.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-0a3e8e7f659a4098b7de7263d9d48d49: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-0a3e8e7f659a4098b7de7263d9d48d49.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-6fd634ce972c4b0cb14e6fa49a3b2109: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-e554a32074b14052abefdbc5851f3db9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-0336f9aa547c444d9cd736634428e15f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-6fd634ce972c4b0cb14e6fa49a3b2109.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-acc582337ad140d7870f7d0a0e3b8565: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-e554a32074b14052abefdbc5851f3db9.
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [logger.py:43] Received request chatcmpl-a44b57bc2c4f40b2b42bffffd360e23c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-0336f9aa547c444d9cd736634428e15f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:49 [async_llm.py:270] Added request chatcmpl-acc582337ad140d7870f7d0a0e3b8565.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-a44b57bc2c4f40b2b42bffffd360e23c.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-3c3adee0ab8940df9880303dbc5e27f1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-3c3adee0ab8940df9880303dbc5e27f1.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-1f97ee5601704de0941f20d8bd91aa57: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-1f97ee5601704de0941f20d8bd91aa57.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-8dde6c36a77c499aa7a324826e91a212: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-8dde6c36a77c499aa7a324826e91a212.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-0bfff45bde0649a5b5f0a205997ae4e1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54616 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-0bfff45bde0649a5b5f0a205997ae4e1.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-fe70038aa7944c379d491f2ea691f795: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-fe70038aa7944c379d491f2ea691f795.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-83223096d79a4a139947da293964e89c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54632 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-83223096d79a4a139947da293964e89c.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-82e4ff2be2724fb0b765239a5a0ba4f1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-a1617856e4fa4f3eba2e111b9a965d42: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-82e4ff2be2724fb0b765239a5a0ba4f1.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-a1617856e4fa4f3eba2e111b9a965d42.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-edf6a37ac7e7426e83ca11db3b4bd4f9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-edf6a37ac7e7426e83ca11db3b4bd4f9.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-c540ee1a4d0846219420077437905982: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54662 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-c540ee1a4d0846219420077437905982.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-4d76d2905ed346859ff3ad3f95afdf48: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-4d76d2905ed346859ff3ad3f95afdf48.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-944a258c505f41ea9e729156e991d226: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-944a258c505f41ea9e729156e991d226.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-9e36a3d5565741b0911a2ed0aa5a7ab5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-9e36a3d5565741b0911a2ed0aa5a7ab5.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-875cb3bf23fd43f9a6768043edc48c9c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-875cb3bf23fd43f9a6768043edc48c9c.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-fedc76ed36514bf28bda2371e9efa963: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-fedc76ed36514bf28bda2371e9efa963.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-b99c635203cd473d8bdd37579323b5ab: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-b99c635203cd473d8bdd37579323b5ab.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-807b27e245da40cbbc1c108b95507875: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-807b27e245da40cbbc1c108b95507875.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-3b7c4f2a33c64bc4aea1d81d88f2f4ef: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-3b7c4f2a33c64bc4aea1d81d88f2f4ef.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-ed53e676a7224c6684228cadc1dfe792: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-ed53e676a7224c6684228cadc1dfe792.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-512e33ea5d694c00a6775d1ec0f7cafe: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-512e33ea5d694c00a6775d1ec0f7cafe.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-8bc88acd0a9e4516a22cb8ca397a1266: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-8bc88acd0a9e4516a22cb8ca397a1266.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-9fcfe1cb4ebb4968aa234dd6113a4903: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-9fcfe1cb4ebb4968aa234dd6113a4903.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-0c2c19d2e2f44b39b8992cd54e03d21f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-0c2c19d2e2f44b39b8992cd54e03d21f.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-a5a22d3dae5343aab68132e7b7a4c8c6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-a5a22d3dae5343aab68132e7b7a4c8c6.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-2f4611c900f643748411c7205f694436: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-2f4611c900f643748411c7205f694436.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-bf3105f27bb346019c43667fc3386212: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-bf3105f27bb346019c43667fc3386212.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-c9ac0b8c99864d4d96ae0fe70d4c83e2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-c9ac0b8c99864d4d96ae0fe70d4c83e2.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-96e9b389eace4740a8b15e931f6be9ca: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-96e9b389eace4740a8b15e931f6be9ca.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-3cdb8c3b24b44ed99db659bf22068970: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-3cdb8c3b24b44ed99db659bf22068970.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-90558950ab3648d5b31cb32eb583ab51: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-90558950ab3648d5b31cb32eb583ab51.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-dcf10b7465d44e379960fcabf3ed6705: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-dcf10b7465d44e379960fcabf3ed6705.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-c354a49a0d7541b294424ef8b5f1b51d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-c354a49a0d7541b294424ef8b5f1b51d.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-dc726fe46db9424e9b294b709ade7e5f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-dc726fe46db9424e9b294b709ade7e5f.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-f6ed9ca76d4941a499da3411eef09799: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-f6ed9ca76d4941a499da3411eef09799.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-dbad974f43a74a89832839aa4c3aeb61: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-dbad974f43a74a89832839aa4c3aeb61.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-f9e9ab00e09e4f51a3d52ed6abc5501c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-f9e9ab00e09e4f51a3d52ed6abc5501c.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-65c23225d1624b8382cf0c353197195e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54908 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-65c23225d1624b8382cf0c353197195e.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-0589f72613d04000a81ebb6d742e7889: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-1b582430d78f419ab439aca973d315a0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-0589f72613d04000a81ebb6d742e7889.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-1b582430d78f419ab439aca973d315a0.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-6666382afaa34a128d75d0a76c99634c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-6666382afaa34a128d75d0a76c99634c.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-d041b4e0069c4e3e9cf6fcef6a53bea5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-d041b4e0069c4e3e9cf6fcef6a53bea5.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-78aeddfe574c4a3781848d739fd2e4a4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-78aeddfe574c4a3781848d739fd2e4a4.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-434a30f2cc5c4164af6e7cf530930ba5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-434a30f2cc5c4164af6e7cf530930ba5.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-49e5f66cca8749d4a1deaebdedb7bb00: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-49e5f66cca8749d4a1deaebdedb7bb00.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-09b6c261d5c14dcf8a0ef8ec4e967cbc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:54998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-09b6c261d5c14dcf8a0ef8ec4e967cbc.
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [logger.py:43] Received request chatcmpl-6687dea06e0347aa85a168bb82fe9fb3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:50 [async_llm.py:270] Added request chatcmpl-6687dea06e0347aa85a168bb82fe9fb3.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-f1b9bac6ac4c4e7b8487ba09fcd47235: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-f1b9bac6ac4c4e7b8487ba09fcd47235.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-51da6593dd0e48abb0e43a4be51fc3fe: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-51da6593dd0e48abb0e43a4be51fc3fe.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-92457fa6bfcf4cd184ad5f7bb5653140: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55040 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-92457fa6bfcf4cd184ad5f7bb5653140.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-6df83b3033014c4e90f1948c8ea16577: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-6df83b3033014c4e90f1948c8ea16577.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-c15e28e6038f4535b28d944f1d9de388: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-c15e28e6038f4535b28d944f1d9de388.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-1b9588fa86d14c31bab0616afb57bb75: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-1b9588fa86d14c31bab0616afb57bb75.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-7b557fa2ed3c4cbb9f50fa68e258062d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-7b557fa2ed3c4cbb9f50fa68e258062d.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-73369f8f824149c0b7dd18d400b74b6c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-73369f8f824149c0b7dd18d400b74b6c.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-7302ceec988b4d5dbf1a586c66dd81a6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-7302ceec988b4d5dbf1a586c66dd81a6.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-8de7fe520cc04eb682f8c9e8fd92e2dd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-8de7fe520cc04eb682f8c9e8fd92e2dd.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-66ef2485e0a44d5bb0dbc62c52c6939e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-66ef2485e0a44d5bb0dbc62c52c6939e.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-c0c30e8d93034d8e8712d3052305cb1d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-c0c30e8d93034d8e8712d3052305cb1d.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-390db103ca764bed819243280390e509: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-390db103ca764bed819243280390e509.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-12aeb008890d4d1195adbf47a7d1eba3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-12aeb008890d4d1195adbf47a7d1eba3.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-cb2bf317f02f4593b18b29b181b74fa7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55144 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-cb2bf317f02f4593b18b29b181b74fa7.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-1fb817d4d6994083ab17cd62afce3dbb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-1fb817d4d6994083ab17cd62afce3dbb.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-cf6bab7850db4af1ab1d5ba1b15b1dc0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-cf6bab7850db4af1ab1d5ba1b15b1dc0.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-24c6b3080b284ce6a99e09108cf772a2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-24c6b3080b284ce6a99e09108cf772a2.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-2c235e8b9c044175b0ef504feb599192: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-2c235e8b9c044175b0ef504feb599192.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-f6305221e96f449d9164147bc32031d8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-f6305221e96f449d9164147bc32031d8.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-df0bd2ca065f48e3b270b029a683aaf0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-df0bd2ca065f48e3b270b029a683aaf0.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-35aa32d7bfc9484989eea7d4ef1ba2e3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-35aa32d7bfc9484989eea7d4ef1ba2e3.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-4226316bed8f40b0b609b9defb3487e5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-4226316bed8f40b0b609b9defb3487e5.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-ff178f61278b4d58b99007ebadebb76e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-ff178f61278b4d58b99007ebadebb76e.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-8f5e70d5e9a94b9f9627d2b14e01c942: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-65c2b1713424492e9a48afb9702247ee: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-8f5e70d5e9a94b9f9627d2b14e01c942.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-65c2b1713424492e9a48afb9702247ee.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-4735cfd4cc8a4c98a44e827c6107d756: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-4735cfd4cc8a4c98a44e827c6107d756.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-94d053ec0dfa47d299269898ec2f9f9f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-94d053ec0dfa47d299269898ec2f9f9f.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-92eee2c19d3f4cbbb8673eb32d40c3a7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-92eee2c19d3f4cbbb8673eb32d40c3a7.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-8eed440f48bf433b83ad8eff5ed73975: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-8eed440f48bf433b83ad8eff5ed73975.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-6fe6a24da9714c7aa1bcc4a427825378: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-6fe6a24da9714c7aa1bcc4a427825378.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-5c9e4d8a831945b7817684c91a7a01c6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-dabceeb4a180438cadb7209de67f4dbf: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-5c9e4d8a831945b7817684c91a7a01c6.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-dabceeb4a180438cadb7209de67f4dbf.
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [logger.py:43] Received request chatcmpl-b41624cb3cb9400182bc86918dca6949: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:51 [async_llm.py:270] Added request chatcmpl-b41624cb3cb9400182bc86918dca6949.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-cb25db33b0c5421bb1d5aa4bfb4946d6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55314 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-cb25db33b0c5421bb1d5aa4bfb4946d6.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-07732975b31946f6ab0e69c027045339: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-07732975b31946f6ab0e69c027045339.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-3496cc1df92c4764b221166b99af73b0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-3496cc1df92c4764b221166b99af73b0.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-45e0d248ac94450c9c26a5c77669cc5d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-45e0d248ac94450c9c26a5c77669cc5d.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-af6f9bd32db449399905cc32bb150a70: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-c4a90395918f4c00b390dfda7e671a79: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-af6f9bd32db449399905cc32bb150a70.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-c4a90395918f4c00b390dfda7e671a79.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-9b4595e90efd48b894b65c0ba5efdaf0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-9b4595e90efd48b894b65c0ba5efdaf0.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-e61381df59d941beab76cb11fd11e99a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-e61381df59d941beab76cb11fd11e99a.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-0fe4875c55e14ff9bd93a00455622d3c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-0fe4875c55e14ff9bd93a00455622d3c.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-4f97beb19524462291ec5bdf5d574b5d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-4f97beb19524462291ec5bdf5d574b5d.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-f8a7a9e7b50a4c42b0f7bbf77f213086: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-5e71b50a9e5741c683bceeade659b0c0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-f8a7a9e7b50a4c42b0f7bbf77f213086.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-5e71b50a9e5741c683bceeade659b0c0.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-28fb045f81b64b74a917bc9d56dc8c1a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-28fb045f81b64b74a917bc9d56dc8c1a.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-992b5e98fe894dcdb7cc142ab32b6260: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-992b5e98fe894dcdb7cc142ab32b6260.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-7e621e5bcfd4496c94cb7b91e4efc4b5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-7e621e5bcfd4496c94cb7b91e4efc4b5.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-bdc200c2abe84f5890a4277f653ad034: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-bdc200c2abe84f5890a4277f653ad034.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-d6d8e1aae8df42638df11d142d0b3187: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-d6d8e1aae8df42638df11d142d0b3187.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-777df6cb2e3044348da791f5aa76b49e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-777df6cb2e3044348da791f5aa76b49e.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-88f094ef0c204622b11a697eecfee831: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-88f094ef0c204622b11a697eecfee831.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-80d31f97cd6240ec81927c8056a2da6e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-80d31f97cd6240ec81927c8056a2da6e.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-effd77a3e60b4c7eab07570042e8b7e8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-effd77a3e60b4c7eab07570042e8b7e8.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-f99cdd51d55f4c5089944e3cc5673343: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-f99cdd51d55f4c5089944e3cc5673343.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-33828b0f51e94db79336e58437f7ea39: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-84453cd5bc3f4d1a80d42fbfdcb894d6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-33828b0f51e94db79336e58437f7ea39.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-84453cd5bc3f4d1a80d42fbfdcb894d6.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-eccb186862524a9fa2613f189a81a21f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-eccb186862524a9fa2613f189a81a21f.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-fc51d332555c4f19a5fb10dc1fa6aa18: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-fc51d332555c4f19a5fb10dc1fa6aa18.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-45c76199a9db4603b8ae05f78f7e01f7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-f8ea0ba4e90042aaaa654ed97567f0aa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-45c76199a9db4603b8ae05f78f7e01f7.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-d2cbb3db7ec94b8f870e0c90c1a44ba1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-af6035b00de849b4903cf87219a3417f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-a63f864c79ed4ced80a9f0a9a35c521c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-f8ea0ba4e90042aaaa654ed97567f0aa.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-d2cbb3db7ec94b8f870e0c90c1a44ba1.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-fcaa1090eb08458b9b4c2f12b95755b5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-df63a1b1222c4a829c8e20036fc61b5b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-af6035b00de849b4903cf87219a3417f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-a63f864c79ed4ced80a9f0a9a35c521c.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-f8defad686a745fbb02c872ff5516f07: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-fcaa1090eb08458b9b4c2f12b95755b5.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-df63a1b1222c4a829c8e20036fc61b5b.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-b1cc14c9e70246ed994b0372b2dd1b79: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-f8defad686a745fbb02c872ff5516f07.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-b1cc14c9e70246ed994b0372b2dd1b79.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-d9ac029a37ee4c81a735e6496e93c494: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-d9ac029a37ee4c81a735e6496e93c494.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-ecd05ea69a474e0dbf2f35e91aaaa817: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-b641c06b73f54eea9da62bcf6abc8649: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-ecd05ea69a474e0dbf2f35e91aaaa817.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-b641c06b73f54eea9da62bcf6abc8649.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-282c0c15870f4f8181f7e1cb415952ed: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-282c0c15870f4f8181f7e1cb415952ed.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-ba9f30746eb44795bdbd1471e1d22873: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-ba9f30746eb44795bdbd1471e1d22873.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-977786269029477e8f85962de3499882: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-977786269029477e8f85962de3499882.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-1914128a43ff477181f4e6e01d88f93f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-1914128a43ff477181f4e6e01d88f93f.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-77b473f3173b4e70806b929971443c3f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-6cdb2333279b4277a92cf8e3c62fa242: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-77b473f3173b4e70806b929971443c3f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-6cdb2333279b4277a92cf8e3c62fa242.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-8206603611d24afabefb898116d39ab1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-8206603611d24afabefb898116d39ab1.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-daa1f3abb34d4fcfb839d8908d0f8d2b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-daa1f3abb34d4fcfb839d8908d0f8d2b.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-b44593f24db84aba888e842171727e0d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:55650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-b44593f24db84aba888e842171727e0d.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-666706ea21dc4a31a15458173ecd0b7d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-666706ea21dc4a31a15458173ecd0b7d.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-31636512b92f4bf295b1b34dd75429a0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-31636512b92f4bf295b1b34dd75429a0.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-502a922d48394c2592e7e48356a52101: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-502a922d48394c2592e7e48356a52101.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-a7209742062c4720a9550b088047f0b3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-a7209742062c4720a9550b088047f0b3.
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [logger.py:43] Received request chatcmpl-ee36f8eb29b0410c941eef2f227f9b61: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:52 [async_llm.py:270] Added request chatcmpl-ee36f8eb29b0410c941eef2f227f9b61.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-eafb2e112e5c41d287c0eacc3d1de6ba: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-eafb2e112e5c41d287c0eacc3d1de6ba.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-13b534dacce64200be6bfe54773e7d6e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-13b534dacce64200be6bfe54773e7d6e.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-2f29bd4d4f09461a857e3731c925a0b7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-2f29bd4d4f09461a857e3731c925a0b7.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-66fe4d2008a74d078d9d9dde635d89ac: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-66fe4d2008a74d078d9d9dde635d89ac.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-515dbb7b890945fda159fdfbdb8a673e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-515dbb7b890945fda159fdfbdb8a673e.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-db04939f27924116a60d25eb1e243ae3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-db04939f27924116a60d25eb1e243ae3.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-07314855e9bd47219a026797c411afcc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-07314855e9bd47219a026797c411afcc.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-9246504538094ec2bc2f75b1725b607b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-9246504538094ec2bc2f75b1725b607b.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-4e49468fe5d1496683f2db1d8fffa956: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-4e49468fe5d1496683f2db1d8fffa956.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-a0f34cd50e1a4fffbb7192940cff42dd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-a0f34cd50e1a4fffbb7192940cff42dd.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-d1a5e8275b7340ee978214d22622d93b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-d1a5e8275b7340ee978214d22622d93b.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-a11d2d9f4651437ca6ca3446fb6d5fd9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-a11d2d9f4651437ca6ca3446fb6d5fd9.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-7df36528274a47a1942361f68c0a4738: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-7df36528274a47a1942361f68c0a4738.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-533931733911443b9a1ed70d045e379d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-533931733911443b9a1ed70d045e379d.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-9edd8514424f41ba8df59906256f90b6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-79255aa3da784e1ca663832f13cee056: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-3d32397136294dd3b9330180c69f0438: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-9edd8514424f41ba8df59906256f90b6.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-a731792d629a400c9af69537077f883d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-d39af8fb7c9f446f8730d8012081d6b7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-79255aa3da784e1ca663832f13cee056.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-3d32397136294dd3b9330180c69f0438.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-a731792d629a400c9af69537077f883d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-d39af8fb7c9f446f8730d8012081d6b7.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-df77e370ac0949c9be9a4219466a202b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-df77e370ac0949c9be9a4219466a202b.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-70b4663ac9934e90856842bf4714e267: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-70b4663ac9934e90856842bf4714e267.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-1974e206d9eb434e91c8024db771366d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-1974e206d9eb434e91c8024db771366d.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-1d5669c78f764e548355974bf79c16f2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44888 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-1d5669c78f764e548355974bf79c16f2.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-ba4d69b3e3634240a1422686b01fb5cc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-ba4d69b3e3634240a1422686b01fb5cc.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-4668e9209d8949a792aa378433cc5450: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-4668e9209d8949a792aa378433cc5450.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-52dd4ea30f434eb19335038c7a2d6539: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-52dd4ea30f434eb19335038c7a2d6539.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-6c39c0e7f09c45de9bdb1c471a902b52: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-6c39c0e7f09c45de9bdb1c471a902b52.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-8f2f737c46b848cca6a55f1dbaa60296: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-8f2f737c46b848cca6a55f1dbaa60296.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-4d18d6a5195e43cbbfcf678aa05224c9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-4d18d6a5195e43cbbfcf678aa05224c9.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-928e11b0bd1748009e61873e03e5ecb0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-928e11b0bd1748009e61873e03e5ecb0.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-831973606a4c4424bd307c3adb7afc37: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-831973606a4c4424bd307c3adb7afc37.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-0fa4fca66d944c0f8fc24104cdbbf67e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-0fa4fca66d944c0f8fc24104cdbbf67e.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-c5b1a6e315e64ea2a8eb8e19e4bedad9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-c5b1a6e315e64ea2a8eb8e19e4bedad9.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-4214d5f67a024be5af060bd1673d4d31: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:44996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-4214d5f67a024be5af060bd1673d4d31.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-b4ac9ad5b3864e73b39b0af2f4a83871: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-b4ac9ad5b3864e73b39b0af2f4a83871.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-c1a7021b060b433a83d2625d9c332a80: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-c1a7021b060b433a83d2625d9c332a80.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-2ef4767e2edb4316b00c45f97a228ff9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-2ef4767e2edb4316b00c45f97a228ff9.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-76fbe606a0e941f794afdd193a60af6f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-76fbe606a0e941f794afdd193a60af6f.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-83ae942489a444e2986627e8b57d9cf7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-590dffc89512419292594dbb83e9fb72: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-83ae942489a444e2986627e8b57d9cf7.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-590dffc89512419292594dbb83e9fb72.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-363f9a2de3b748cca5ae93168a5241bb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-363f9a2de3b748cca5ae93168a5241bb.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-4f77ca8c854846ae86da568d79787ace: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-4f77ca8c854846ae86da568d79787ace.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-4020b85c1b604f378be84de3c50d8ac0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-4020b85c1b604f378be84de3c50d8ac0.
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [logger.py:43] Received request chatcmpl-8d577e7a8fce4c1d99076b42ff9c6862: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:53 [async_llm.py:270] Added request chatcmpl-8d577e7a8fce4c1d99076b42ff9c6862.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-65ed8ef31b2c4f15b079854b3cfb4277: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-65ed8ef31b2c4f15b079854b3cfb4277.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-20e13e00fb004de1b089bffc01a33ccb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-20e13e00fb004de1b089bffc01a33ccb.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-905fab9345714c20b0978d48bc02f10e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-905fab9345714c20b0978d48bc02f10e.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-82946808edf14ea3a190c6ba15811df6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-82946808edf14ea3a190c6ba15811df6.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-078ea7993ace46838353f83612eb082a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-078ea7993ace46838353f83612eb082a.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-18351d923807400293878a8896f935a0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-18351d923807400293878a8896f935a0.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-34cf6e66d393492fb5e0f1d8717013ca: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-34cf6e66d393492fb5e0f1d8717013ca.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-07967d0a9a234e0bbf10a46e6e2acb86: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-07967d0a9a234e0bbf10a46e6e2acb86.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-3ef5da0b5e344fa98bd03438951fb2a6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-3ef5da0b5e344fa98bd03438951fb2a6.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-d36abd2f96024a16ac97f9dbf8109b69: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-ab0906d5827c4963b7b56c0e4c82e4c5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-d36abd2f96024a16ac97f9dbf8109b69.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-ab0906d5827c4963b7b56c0e4c82e4c5.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-0d8a38482a5242739cf6700c185cf00d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-0d8a38482a5242739cf6700c185cf00d.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-5f65dc0e07544d0a80615bb2315cbbc8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-5f65dc0e07544d0a80615bb2315cbbc8.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-a3e8ab4570a545729eb4dd429001ad80: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-a3e8ab4570a545729eb4dd429001ad80.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-af390ccef3f044fca648971126ce4a3d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-af390ccef3f044fca648971126ce4a3d.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-3c4a37cb156b466291d49b5417270bf2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-3c4a37cb156b466291d49b5417270bf2.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-947f1235fb2644a895d9995c3fc6b360: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-947f1235fb2644a895d9995c3fc6b360.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-e33bb7ce369a42cfa98c2fabedb21b0a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-e33bb7ce369a42cfa98c2fabedb21b0a.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-fd3c83e38356401faf9bd97918059649: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-df70da7981cd4588a908d65f31917d99: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-fd3c83e38356401faf9bd97918059649.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-df70da7981cd4588a908d65f31917d99.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-9394b33dd55a472c8885d0866d528ed9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45260 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-9394b33dd55a472c8885d0866d528ed9.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-240d2c9a4cd94b4eb76bc6a0e7d8a4fe: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-240d2c9a4cd94b4eb76bc6a0e7d8a4fe.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-97e816056db745db822c54742f5a4a0b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-97e816056db745db822c54742f5a4a0b.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-e47181ef54974ea2aa6a56ea65e655dd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-e47181ef54974ea2aa6a56ea65e655dd.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-a65e3affc7694f76b55853d7f6dd75d9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-a65e3affc7694f76b55853d7f6dd75d9.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-4aeea73e1f7e4dc09d16c73b6fdb046b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-4aeea73e1f7e4dc09d16c73b6fdb046b.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-080a83c98bec4879ac907971c079ec98: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-080a83c98bec4879ac907971c079ec98.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-abadb1cbfdac4066a16852e960922112: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-abadb1cbfdac4066a16852e960922112.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-0a654fc2ce5b40338d074061d5d69754: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-0a654fc2ce5b40338d074061d5d69754.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-e3198ab4ff28486ba723bcb72a815c79: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-e3198ab4ff28486ba723bcb72a815c79.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-786d1566010040b8b0469740b3398b1f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-786d1566010040b8b0469740b3398b1f.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-08432f88ca3d4b40845eecb06f4815b3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-08432f88ca3d4b40845eecb06f4815b3.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-944610cbb3df435a994de30f357913b2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-944610cbb3df435a994de30f357913b2.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-80115464502d4b8dab22c2d05143f1ff: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-80115464502d4b8dab22c2d05143f1ff.
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [logger.py:43] Received request chatcmpl-63fcbf76f8424ed588f413f12a133898: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:54 [async_llm.py:270] Added request chatcmpl-63fcbf76f8424ed588f413f12a133898.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-fa9b8d273e1d45aa9e717990c68579b9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-fa9b8d273e1d45aa9e717990c68579b9.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-949d22ab5ced41ca99dc4e7c0767fc56: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-949d22ab5ced41ca99dc4e7c0767fc56.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-cc652d3d95c2433e94f57bb38c89113d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45436 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-cc652d3d95c2433e94f57bb38c89113d.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-197b50c25ddc40bf959ab9a258d0ba47: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-197b50c25ddc40bf959ab9a258d0ba47.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-a564b3bf63c44eb489af0f9cdcec889c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-a564b3bf63c44eb489af0f9cdcec889c.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-85d49b47df5043448634aa729ec4f1fa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-7d5be251e5774ec783811937c47618e8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-85d49b47df5043448634aa729ec4f1fa.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-7d5be251e5774ec783811937c47618e8.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-08c8076d41d14b6481158d5364e41d3f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-93bdbd0b4fa340bcabcc5643750bebbd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-08c8076d41d14b6481158d5364e41d3f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-93bdbd0b4fa340bcabcc5643750bebbd.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-ffba36fe9e2e40d8847be6c1132a23a1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-f7934ccf55b44a089e08a32a9e0b0ea9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-ffba36fe9e2e40d8847be6c1132a23a1.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-f7934ccf55b44a089e08a32a9e0b0ea9.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-0e89ed1445f244fa9ede84edc54853a7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-0e89ed1445f244fa9ede84edc54853a7.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-dde3dc15f972447b9a55b299e675374e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-dde3dc15f972447b9a55b299e675374e.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-2de8316922904756b787ba304058a2d5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-cd53375cafdf44a887f74e82f3f00751: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-2de8316922904756b787ba304058a2d5.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-cd53375cafdf44a887f74e82f3f00751.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-e9451d3fb04043af89ce84cb04409108: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-9535874a665a40fcb11f8eeb2c021567: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-e9451d3fb04043af89ce84cb04409108.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-9535874a665a40fcb11f8eeb2c021567.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-07622d99dac4485984816ac1221a1e33: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-07622d99dac4485984816ac1221a1e33.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-8e0a2803014b4c51b8940bb56f280a15: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-8e0a2803014b4c51b8940bb56f280a15.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-e235cec2ab414856825ad3a7cd20a1cd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-e235cec2ab414856825ad3a7cd20a1cd.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-e62a0b3407bc4d42af6eac1cb350d552: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-e62a0b3407bc4d42af6eac1cb350d552.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-6e19488f39e64f43971ce15f9839e3b1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-de532467381d46338220dc1e969a342e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-e14fb52d308d4345899cf4b59e791e6f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-6e19488f39e64f43971ce15f9839e3b1.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-9a456e12f02a4c21b777674b52c53e24: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-de532467381d46338220dc1e969a342e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-e14fb52d308d4345899cf4b59e791e6f.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-506ad558bca447a8baf0aec13bed02a2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-9a456e12f02a4c21b777674b52c53e24.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-506ad558bca447a8baf0aec13bed02a2.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-f97220cafd1d42cbab737378dceef13f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-f97220cafd1d42cbab737378dceef13f.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-ec10417727384985999daaca312c9003: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-ec10417727384985999daaca312c9003.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-032c515bdf024b4b9621000459908e75: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-032c515bdf024b4b9621000459908e75.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-d7b15373b250419fb74d1a85483b3150: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-d7b15373b250419fb74d1a85483b3150.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-fbfa5916212b42f29a1d628bbb29f05f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-fbfa5916212b42f29a1d628bbb29f05f.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-ab20d06199044d1d9b8aa8571b18ae31: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-ab20d06199044d1d9b8aa8571b18ae31.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-3027a3343d5c443db08d60ee2586a53e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-16d04de445db4e5388587adf94f17e19: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-3027a3343d5c443db08d60ee2586a53e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-16d04de445db4e5388587adf94f17e19.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-4a57e59e3b5644659719770f1050facd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-4a57e59e3b5644659719770f1050facd.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-5bfe8d45058b4224b28d30cf0ce4170b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-5bfe8d45058b4224b28d30cf0ce4170b.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-cfe751c41b04474cb6cc605c10790adc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-cfe751c41b04474cb6cc605c10790adc.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-6686c99bb21d469883b3e9d767cdea5b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-6686c99bb21d469883b3e9d767cdea5b.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-138924d691c743c79708c3a00a9d916b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-138924d691c743c79708c3a00a9d916b.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-9fdc7d64a36a4658a715cf91459b1d2f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-9fdc7d64a36a4658a715cf91459b1d2f.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-c79ec8239cc84a0aad955d4c4e50083a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-f82e0c2291bf4c28828c90c74f2ed3d2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-c79ec8239cc84a0aad955d4c4e50083a.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-f82e0c2291bf4c28828c90c74f2ed3d2.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-30aef8afda3e4ea78e7814d70f5f50d3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-187c9642a23c47b682db2e8d64c33374: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-30aef8afda3e4ea78e7814d70f5f50d3.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-187c9642a23c47b682db2e8d64c33374.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-2972ee70a33f41d4be07e7f438495cf9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-2972ee70a33f41d4be07e7f438495cf9.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-b7b87136e957486287a8b8770b65f09f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-f9c6c164aed749b0821e2570bb6e8b5d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-b7b87136e957486287a8b8770b65f09f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-f9c6c164aed749b0821e2570bb6e8b5d.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-16d6a8665a55442495175c2545e6113d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-16d6a8665a55442495175c2545e6113d.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-2f13c255163d497db3f9ee417522056a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-2f13c255163d497db3f9ee417522056a.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-a2f94156a36b454c97dcfd23c778ffa0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-a2f94156a36b454c97dcfd23c778ffa0.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-f47a047fbd60465d96b130550ad2dbbb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-f47a047fbd60465d96b130550ad2dbbb.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-6a8f6a3d20ae4f1798c8f4ec80b60ee7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [async_llm.py:270] Added request chatcmpl-6a8f6a3d20ae4f1798c8f4ec80b60ee7.
[36mllm_server_1  |[0m INFO 07-21 18:35:55 [logger.py:43] Received request chatcmpl-c5c49413f3e74c27bef9559b1507968a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-c5c49413f3e74c27bef9559b1507968a.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-959635b395494036b9a9a424fa844601: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-959635b395494036b9a9a424fa844601.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-3ae5ce29bc8840c8927be1259ede94c1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-3ae5ce29bc8840c8927be1259ede94c1.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-4c0e2bdba69f4df2b4309d2d3d946aa3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-4c0e2bdba69f4df2b4309d2d3d946aa3.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-2fe7041745654ffdafbda10604427b34: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-2fe7041745654ffdafbda10604427b34.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-5e8e97ca69234330b16a140f530d370f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45908 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-5e8e97ca69234330b16a140f530d370f.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-2d8856bbbfb24629af7b599e804d8f85: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-2d8856bbbfb24629af7b599e804d8f85.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-d37e177539e447369a7cf36450731e34: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-d37e177539e447369a7cf36450731e34.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-44d416cd26f848a4a66dd4a23682f937: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-44d416cd26f848a4a66dd4a23682f937.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-a0f6fb83fbb64971a1fea6212c81284f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-a0f6fb83fbb64971a1fea6212c81284f.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-1c84cbc8d4d845e39a14449f6583ff62: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-1c84cbc8d4d845e39a14449f6583ff62.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-b4ca46fb6f5c453ea10288311e2904a9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-b4ca46fb6f5c453ea10288311e2904a9.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-5752d5c7b6f146d899fef59c9cdd8638: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-5752d5c7b6f146d899fef59c9cdd8638.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-c1f66aaeb8db40cebb5a9430b98f7254: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-c1f66aaeb8db40cebb5a9430b98f7254.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-3af0ae61ef4e483b91e196eaea02a13b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-3af0ae61ef4e483b91e196eaea02a13b.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-01ec99a8131346a597d87ceb37623d70: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-01ec99a8131346a597d87ceb37623d70.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-db16970ce30b4a029cd8dbba399309ac: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:45998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-db16970ce30b4a029cd8dbba399309ac.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-56617a9465384c379d314f7d6a98d633: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-56617a9465384c379d314f7d6a98d633.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-17d1eecc872047f2bdd6e801ee5a5c34: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-17d1eecc872047f2bdd6e801ee5a5c34.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-4dfb2341d34248b6a12a0b3637e55b09: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-4dfb2341d34248b6a12a0b3637e55b09.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-feacace12705417c819e25a986727b1e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-feacace12705417c819e25a986727b1e.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-32947d2b73c14edc979c851908cef28d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-32947d2b73c14edc979c851908cef28d.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-21cec368c87745e59f5673c134a4285a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-21cec368c87745e59f5673c134a4285a.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-aaca61e2e3c7438180a0bbe96e96a9f3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-aaca61e2e3c7438180a0bbe96e96a9f3.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-abaea74107d74595982a836c6c949697: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46074 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-abaea74107d74595982a836c6c949697.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-f22600643f0e4178a7917df726bbba3e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-f22600643f0e4178a7917df726bbba3e.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-314dbd5d3bc24d3db93323dde5ebefa0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-314dbd5d3bc24d3db93323dde5ebefa0.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-ee4ed4a06a4b4fd78daaf557dc092493: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-ee4ed4a06a4b4fd78daaf557dc092493.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-4d11ccae1e494e4cace1500f3e7713df: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-4d11ccae1e494e4cace1500f3e7713df.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-8a4ea0b36f1543a89acdd7028c39fc41: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-8a4ea0b36f1543a89acdd7028c39fc41.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-b011604e9ccb46769e46720c2d285c7b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-b011604e9ccb46769e46720c2d285c7b.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-fea348b90da74b8f932c0a4e8dd99df0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-fea348b90da74b8f932c0a4e8dd99df0.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-a5009c624bd04b289850dabe365f8b32: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-a5009c624bd04b289850dabe365f8b32.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-4c6a6b5141054caf8c16ef6085681d5d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-4c6a6b5141054caf8c16ef6085681d5d.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-bf72b69c5b3140ff996f55b3a2d0d01e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-bf72b69c5b3140ff996f55b3a2d0d01e.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-2ccd595c35fc425d9399e3a2e5962a8c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-2ccd595c35fc425d9399e3a2e5962a8c.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-79270fa1059244bfba5acd7d187e6202: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-79270fa1059244bfba5acd7d187e6202.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-aee978da35cd43e68df024ddcac0205e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-aee978da35cd43e68df024ddcac0205e.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-dd052de4b131467886e855d53b3dd088: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-dd052de4b131467886e855d53b3dd088.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-4253b9dd6a1048e3b0d7ccf089f538fb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-4253b9dd6a1048e3b0d7ccf089f538fb.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-820181a2b3f34faf82bd98816ebb2fa4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [async_llm.py:270] Added request chatcmpl-820181a2b3f34faf82bd98816ebb2fa4.
[36mllm_server_1  |[0m INFO 07-21 18:35:56 [logger.py:43] Received request chatcmpl-b3e16b552fca4988900bde7dc1cab2b6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-b3e16b552fca4988900bde7dc1cab2b6.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-1bde0329db5f434994b15cf1ddf7a485: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-1bde0329db5f434994b15cf1ddf7a485.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-ef5c81c1685b4b8980fd6d4c95dcaf11: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-ef5c81c1685b4b8980fd6d4c95dcaf11.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-10b112b5c95e4142a0ef30632f727f0b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-1b3cf32c4712471ab33d4117774d6469: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-10b112b5c95e4142a0ef30632f727f0b.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-1210859692604ff5abef8c5b5e468ed2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-1b3cf32c4712471ab33d4117774d6469.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-07e3341803294bf5b3000080f084a701: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-1210859692604ff5abef8c5b5e468ed2.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-07e3341803294bf5b3000080f084a701.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-54a575e207104acb9f1869c75a1087a2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-54a575e207104acb9f1869c75a1087a2.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-c190ba9bf4f344859df461f9b9168b80: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-c190ba9bf4f344859df461f9b9168b80.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-ae5398a2976f485a9091a07e81b939ca: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-7d1cd9b882314a709bf4d248bca8f52d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-ae5398a2976f485a9091a07e81b939ca.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-7d1cd9b882314a709bf4d248bca8f52d.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-6fa32b71a6054b13ad42f5c837439d6b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-6fa32b71a6054b13ad42f5c837439d6b.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-bf6133baa87943ed8d97c8439fc9c808: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-bf6133baa87943ed8d97c8439fc9c808.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-295e7e9923364d44b83cf46fbbd37443: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-295e7e9923364d44b83cf46fbbd37443.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-a39a6add24dd4b719df3d04a0bfb7750: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-a39a6add24dd4b719df3d04a0bfb7750.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-524e05f1a9194ba69abfd885fea08180: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-524e05f1a9194ba69abfd885fea08180.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-c3ef44af14604d14ab2229aba6c0dfc5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-c3ef44af14604d14ab2229aba6c0dfc5.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-fe06c238b4954a208aae3c7a379adcfc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-fe06c238b4954a208aae3c7a379adcfc.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-fe250b1945b8453b8f477c30adaf28ca: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-fe250b1945b8453b8f477c30adaf28ca.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-5aaecf2d0b94449f96e1a452754a21db: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-2d48b09c4410495d8afe602273a60462: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-8ba2948e3cf7451ea21df09c93c3626a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-5aaecf2d0b94449f96e1a452754a21db.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-2d48b09c4410495d8afe602273a60462.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-cb46e1f63abb454dae6ac3f53f3044d6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-8ba2948e3cf7451ea21df09c93c3626a.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-cb46e1f63abb454dae6ac3f53f3044d6.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-08dcab734b014f62b41c59bc0e8021a1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-08dcab734b014f62b41c59bc0e8021a1.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-ac7df3f74c1d48359a3acde0be452437: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-0c5ccbb3aa1a4915920b532e47de38d3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-ac7df3f74c1d48359a3acde0be452437.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-0c5ccbb3aa1a4915920b532e47de38d3.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-2ad6cc99b37c43f487c23eaf85a89dbc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-2ad6cc99b37c43f487c23eaf85a89dbc.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-eac79846f8254b33b3cecbfb165a6287: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-eac79846f8254b33b3cecbfb165a6287.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-e7a238cdb34e47b4973ebeb058fa1b6c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-e7a238cdb34e47b4973ebeb058fa1b6c.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-c406bbb6bb5e4af0b22f5280021c86fb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-c406bbb6bb5e4af0b22f5280021c86fb.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-b5b5e4ab75a54ed4b8d6bc303c7aab53: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-b5b5e4ab75a54ed4b8d6bc303c7aab53.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-91f434c19c344ffe873c5f2ea038590f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-91f434c19c344ffe873c5f2ea038590f.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-213e74f640b64e5488f4984fd5082637: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-213e74f640b64e5488f4984fd5082637.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-ff23836918a74585978fd6331e4abfd7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-ff23836918a74585978fd6331e4abfd7.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-c16d056eacab415baf3228d823dbd245: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-c16d056eacab415baf3228d823dbd245.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-a325e44c78994c4db2dcfe3dedcc723a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-a325e44c78994c4db2dcfe3dedcc723a.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-dbb22d6dcf5a4cbea49d5cb1225f8a19: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-dbb22d6dcf5a4cbea49d5cb1225f8a19.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-7213f28e270c4d7992accd870e6944e3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-d78504f0c86f4f3884981a586375b195: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-7213f28e270c4d7992accd870e6944e3.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-d78504f0c86f4f3884981a586375b195.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-de80b19fbe8046abafd5df1374cb98fa: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-de80b19fbe8046abafd5df1374cb98fa.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-5eb508fdab7f4ca5ae66abd48e2f0383: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-5eb508fdab7f4ca5ae66abd48e2f0383.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-33f61b1aa9ad44858fdbdf5dd49601dc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-33f61b1aa9ad44858fdbdf5dd49601dc.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-78e67853be8546ceb86cfeddd194a599: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-78e67853be8546ceb86cfeddd194a599.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-7c099c4c3bf64b6fb038f2cc5c69d947: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-7c099c4c3bf64b6fb038f2cc5c69d947.
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [logger.py:43] Received request chatcmpl-8ba5c29fd2594e21889761cb4a23fdb3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:57 [async_llm.py:270] Added request chatcmpl-8ba5c29fd2594e21889761cb4a23fdb3.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-a4005777c3ba4797b21dc14ba01abb47: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46632 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-a4005777c3ba4797b21dc14ba01abb47.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-8930f433dac74a83b08417364e97641c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-8930f433dac74a83b08417364e97641c.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-6da959f1fbea4abeb4632d22b59f7d90: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-6da959f1fbea4abeb4632d22b59f7d90.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-5cf8772b62c84c2dafb42f50779ac133: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-5cf8772b62c84c2dafb42f50779ac133.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-840bdade3651429f91d515ebae4d9d67: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46656 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-840bdade3651429f91d515ebae4d9d67.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-b210cfb8d70a472f854b949097bc719a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46666 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-b210cfb8d70a472f854b949097bc719a.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-e69c5f6c01e34f6aa5745e481d3e421e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-e69c5f6c01e34f6aa5745e481d3e421e.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-126cf2f46b68431a842673407aa4a5e5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-126cf2f46b68431a842673407aa4a5e5.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-434e415f1e04430bb4ee507f61cce05c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-434e415f1e04430bb4ee507f61cce05c.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-b9422896021c41a0a1545c2895300217: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-b9422896021c41a0a1545c2895300217.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-d3edee018b5e45bc806c59d530391b78: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-d3edee018b5e45bc806c59d530391b78.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-519077f22c524958b621c4ee4568fcc5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-519077f22c524958b621c4ee4568fcc5.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-229f74ba315d4bc6b827de896b030149: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-229f74ba315d4bc6b827de896b030149.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-946448ec5c1e4bee8927c066d61b155a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-946448ec5c1e4bee8927c066d61b155a.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-ecbe6b5438b84a8fbc869bfda259d0bf: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-ecbe6b5438b84a8fbc869bfda259d0bf.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-b81295e90d7a46ff92ee8e251c2a4b8d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-b81295e90d7a46ff92ee8e251c2a4b8d.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-e1c77f652744461e8be16548c2309384: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-e1c77f652744461e8be16548c2309384.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-e0f7bfee8e284735b87c7d4d79d44bfa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-e0f7bfee8e284735b87c7d4d79d44bfa.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-3dedfc7bf67249c185ed4a5fccb34287: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-3dedfc7bf67249c185ed4a5fccb34287.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-b8253262722f401cad9cecc1cd6e487a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-b8253262722f401cad9cecc1cd6e487a.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-08f359b2e37144f494dc6f5970bae108: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-08f359b2e37144f494dc6f5970bae108.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-f3a573ff55f74da4b793d7a70ddc9ef1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-f3a573ff55f74da4b793d7a70ddc9ef1.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-a3861fefb7e743708c0fc37fb2c40e91: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-a3861fefb7e743708c0fc37fb2c40e91.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-52beb52752544bf09705a9142175a20e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-52beb52752544bf09705a9142175a20e.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-f2f34ae45b144b8e870ef5744dafcf42: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-f2f34ae45b144b8e870ef5744dafcf42.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-1cb219c8202b48fd8f9bc27656b9e226: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-1cb219c8202b48fd8f9bc27656b9e226.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-5eed22466bda4f4ea99cfb64d1cbf519: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-5eed22466bda4f4ea99cfb64d1cbf519.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-4fc4f8dcae6f4b06808ed15f0a4235d6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-4fc4f8dcae6f4b06808ed15f0a4235d6.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-3f11d78e77f64ee384236fc8525f180c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-c32305939caf43869d61fb74bb255415: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-eee73f9922104e8db1ea9e3d9040dbf7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-3f11d78e77f64ee384236fc8525f180c.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-c32305939caf43869d61fb74bb255415.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-eee73f9922104e8db1ea9e3d9040dbf7.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-25198e8620c041a79e769315640bc9d3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-4f568c994d124870b7e7e23440a8a8f9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-25198e8620c041a79e769315640bc9d3.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46888 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-4f568c994d124870b7e7e23440a8a8f9.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-a0a4508585664c2ebe8765ca20bfda60: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-a0a4508585664c2ebe8765ca20bfda60.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-c6818fe6c6b3407d94d9dfefe4f8a535: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-c6818fe6c6b3407d94d9dfefe4f8a535.
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [logger.py:43] Received request chatcmpl-8096a7d7cd7d405f8073ce19e8d9f1f0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:58 [async_llm.py:270] Added request chatcmpl-8096a7d7cd7d405f8073ce19e8d9f1f0.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-ccb7b0ab8e5243f0ae4297170080f7c7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-ccb7b0ab8e5243f0ae4297170080f7c7.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-7bd614bafd37402b85f75cfbe89b587a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-7bd614bafd37402b85f75cfbe89b587a.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-7a47ad46523e44e3bb46f8f05f5eb7f0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-7a47ad46523e44e3bb46f8f05f5eb7f0.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-4508ad89b52e478f8411f78e6f7cfb3b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-4508ad89b52e478f8411f78e6f7cfb3b.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-034a1f62f24a4988889630703b6805a6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-034a1f62f24a4988889630703b6805a6.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [loggers.py:118] Engine 000: Avg prompt throughput: 554.2 tokens/s, Avg generation throughput: 1723.2 tokens/s, Running: 101 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.8%, Prefix cache hit rate: 82.4%
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [loggers.py:118] Engine 001: Avg prompt throughput: 558.4 tokens/s, Avg generation throughput: 1717.9 tokens/s, Running: 102 reqs, Waiting: 0 reqs, GPU KV cache usage: 30.7%, Prefix cache hit rate: 82.9%
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [loggers.py:118] Engine 002: Avg prompt throughput: 555.5 tokens/s, Avg generation throughput: 1711.8 tokens/s, Running: 101 reqs, Waiting: 0 reqs, GPU KV cache usage: 30.4%, Prefix cache hit rate: 83.4%
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [loggers.py:118] Engine 003: Avg prompt throughput: 565.0 tokens/s, Avg generation throughput: 1705.5 tokens/s, Running: 100 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.8%, Prefix cache hit rate: 83.1%
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-a7a91bfbcf214e609e0fecdb03f58fda: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-9ed81062a5564943bda9ab0a1ac4e83b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-a7a91bfbcf214e609e0fecdb03f58fda.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-9ed81062a5564943bda9ab0a1ac4e83b.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-4e2e41fcfce34ab194507d9b399292da: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-4e2e41fcfce34ab194507d9b399292da.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-560a5de8da9249daa6aedc09f0421101: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46980 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-560a5de8da9249daa6aedc09f0421101.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-0a01b712480247fbb453a55f01f133c3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:46988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-0a01b712480247fbb453a55f01f133c3.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-ba8786a0d40a42e8a1fbabbf8438a091: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-ba8786a0d40a42e8a1fbabbf8438a091.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-c3a59ab1c43445d8b64c2bd061b672f5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-c3a59ab1c43445d8b64c2bd061b672f5.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-051129ede8e14510b4689730f0136eaf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-051129ede8e14510b4689730f0136eaf.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-905eb13272134949a2cad30582d4e77f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-905eb13272134949a2cad30582d4e77f.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-f5c83e3dd0b844c68a0bf0f6469a3823: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-f5c83e3dd0b844c68a0bf0f6469a3823.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-2ef2170b80ea46ec9236aae0ff8061a3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-2ef2170b80ea46ec9236aae0ff8061a3.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-783be94c3a234e098ab97d8733bd8d9a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-783be94c3a234e098ab97d8733bd8d9a.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-ab11fe614b304015b0715faf5c6e4a94: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-ab11fe614b304015b0715faf5c6e4a94.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-2f3951c0db334b9ea7938ec88aedb504: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-2f3951c0db334b9ea7938ec88aedb504.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-f41e4583ff284fac965321ef36f185e0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47088 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-f41e4583ff284fac965321ef36f185e0.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-2a465db26cd1488eb29355f3aa7076c1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-2a465db26cd1488eb29355f3aa7076c1.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-041c117b05e84e94a921957301d77dfd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-041c117b05e84e94a921957301d77dfd.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-0bda9084c8d34d8e8599cf864806e0c8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47108 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-0bda9084c8d34d8e8599cf864806e0c8.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-08e24ad898684168bbee76f7cf435100: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-08e24ad898684168bbee76f7cf435100.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-cad793557d6341a380e37138d7560f3a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-cad793557d6341a380e37138d7560f3a.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-0929e90e60644bb38a93f415b3dac5b3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-0929e90e60644bb38a93f415b3dac5b3.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-a3214d7ad1c0479ba4dd323d7eccdcbe: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-a3214d7ad1c0479ba4dd323d7eccdcbe.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-4315ed4aeacf41d69598c62f1e59fd78: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-4315ed4aeacf41d69598c62f1e59fd78.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-f586f7209da24a388292b60166ea6ac5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47144 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-f586f7209da24a388292b60166ea6ac5.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-aa935051bb3e4cf4a0c540cfbefa3ed8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-aa935051bb3e4cf4a0c540cfbefa3ed8.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-517261c8f013454ab6c6547c978de7a4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-517261c8f013454ab6c6547c978de7a4.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-c0625a06010c450db95baa0b3b452c13: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-c0625a06010c450db95baa0b3b452c13.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-24dd3926cfac40e2b8e9842200de94f9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-24dd3926cfac40e2b8e9842200de94f9.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-e705b2978e04466a80d6b295ba1dbbee: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-e705b2978e04466a80d6b295ba1dbbee.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-247b9bf270594317b8ce91c1574ff263: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-247b9bf270594317b8ce91c1574ff263.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-819fd10f688248139215c1178fec8580: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-819fd10f688248139215c1178fec8580.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-c6d2b0942f9040b3adae97dcc02f5e85: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-c6d2b0942f9040b3adae97dcc02f5e85.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-3c87a54066ee4da3a5d92ecdc8fe1116: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-3c87a54066ee4da3a5d92ecdc8fe1116.
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [logger.py:43] Received request chatcmpl-da50827cf7e14c08b9af498b29de9722: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:35:59 [async_llm.py:270] Added request chatcmpl-da50827cf7e14c08b9af498b29de9722.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-f38dec1ee79d4b96976a2567b9bdc747: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-f38dec1ee79d4b96976a2567b9bdc747.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-6b3afa756a6a4782bb87ee51ad348ace: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-6b3afa756a6a4782bb87ee51ad348ace.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-0ea2654c903b424cb43916aaf42018fc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-0ea2654c903b424cb43916aaf42018fc.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-a59f818563d641e4aedc14272928f18b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-a59f818563d641e4aedc14272928f18b.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-9cb563154dd24932b4ad4a2067ed44fd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-9cb563154dd24932b4ad4a2067ed44fd.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-7baafb8d3c1045fa86e9034eb04a512f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-7baafb8d3c1045fa86e9034eb04a512f.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-bc920164803c4c31806be0e5144f336c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-bc920164803c4c31806be0e5144f336c.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-7e2af6cc8bae4befb64023ee2ab66e9f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-7e2af6cc8bae4befb64023ee2ab66e9f.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-170f055bb31643088504e674bb770157: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47314 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-170f055bb31643088504e674bb770157.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-09cbcce47a464b09a42bc4a734cb4a72: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-09cbcce47a464b09a42bc4a734cb4a72.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-fdae8b3f5f0f45b19fdc5287b3ea7e45: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-fdae8b3f5f0f45b19fdc5287b3ea7e45.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-5a887de05209414a937f127c658c3e98: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-5a887de05209414a937f127c658c3e98.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-6f286b1c2a7a40c8a499a6114c6a082f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-6f286b1c2a7a40c8a499a6114c6a082f.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-56a4751c987d4ed2bfa0b2c3bc434416: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-56a4751c987d4ed2bfa0b2c3bc434416.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-47050f83cf294980ae5099e66dd7375b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-47050f83cf294980ae5099e66dd7375b.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-8cdf104d71fa4fbca022b516a526efa3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-8cdf104d71fa4fbca022b516a526efa3.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-0b67913e3145408cbadab5f21179f92f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-0b67913e3145408cbadab5f21179f92f.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-d795d422ab73425aba9c12306c8ba84b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-d795d422ab73425aba9c12306c8ba84b.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-cb64f1a51e5442f7af861235363c6479: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-cb64f1a51e5442f7af861235363c6479.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-9e405a0fd32449449a0504416a461afa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-9e405a0fd32449449a0504416a461afa.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-22cc18a5ff184676908f91ed368ecf85: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-22cc18a5ff184676908f91ed368ecf85.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-b61078b5678f49b0881996912cded9aa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-b61078b5678f49b0881996912cded9aa.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-569ce70dcdb447bb84f40d64673b4acf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-569ce70dcdb447bb84f40d64673b4acf.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-45480245c6ec44ae828e58d4c5b19259: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-45480245c6ec44ae828e58d4c5b19259.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-9b8fd8c6d11e476ba86875c58a444e67: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-9b8fd8c6d11e476ba86875c58a444e67.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-32d6fc6ea45f445abc142f3733268d5e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47478 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-32d6fc6ea45f445abc142f3733268d5e.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-55039bc9b82a493185804dff154ebb09: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-55039bc9b82a493185804dff154ebb09.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-e3f7238adac348d4aaf23da85ca414a5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-e3f7238adac348d4aaf23da85ca414a5.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-c49baa94740d46e1b1e509b9cb367ff8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-c49baa94740d46e1b1e509b9cb367ff8.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-699885f15c5f4842aeda9236e4bbfac2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-699885f15c5f4842aeda9236e4bbfac2.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-f8a2dbffbe1a4f25a0c89e33bd8c9980: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-f8a2dbffbe1a4f25a0c89e33bd8c9980.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-8d1a51205c414ada8bd73bd817927b50: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-8d1a51205c414ada8bd73bd817927b50.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-f2101bd94bcb4c7a97623089e4203aad: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-f2101bd94bcb4c7a97623089e4203aad.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-6be4f489ec804e789c2d00dc60d25144: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47534 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-6be4f489ec804e789c2d00dc60d25144.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-80c8c5babd744874ab639034fb3762e1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-80c8c5babd744874ab639034fb3762e1.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-9a8ef2925e7143a1a566642fcae98ea7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-9a8ef2925e7143a1a566642fcae98ea7.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-472a9c4e3b6942aea16890242f4aff1d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-472a9c4e3b6942aea16890242f4aff1d.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-bb0b46ed9e0b4e8bae0c6610c3ed3867: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-bb0b46ed9e0b4e8bae0c6610c3ed3867.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-c6bda02da85e419c9fe8a1d0b167510a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47574 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-c6bda02da85e419c9fe8a1d0b167510a.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-32aefe681d0a4b438e22572a413b1ab4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-32aefe681d0a4b438e22572a413b1ab4.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-8ff54d9f870e4f609d8800350a6a5dc5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-8ff54d9f870e4f609d8800350a6a5dc5.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-7ff68188271746f7a0b09cc2138b4589: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-7ff68188271746f7a0b09cc2138b4589.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-28028d36fb8e4b85ab7071880e6bd0c3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-28028d36fb8e4b85ab7071880e6bd0c3.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-165d5c0d40be4097b5cb36ee2d82531e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-165d5c0d40be4097b5cb36ee2d82531e.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-8684f08f79224687b2a8eecf72fb8c58: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-8684f08f79224687b2a8eecf72fb8c58.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-1eb603f3d6db4dc582316a53ae57e678: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-1eb603f3d6db4dc582316a53ae57e678.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-4964e75525b3449389aa0dad516791dd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-4964e75525b3449389aa0dad516791dd.
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [logger.py:43] Received request chatcmpl-9d301b27bd1e485096de0e9782535281: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47642 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:00 [async_llm.py:270] Added request chatcmpl-9d301b27bd1e485096de0e9782535281.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-8b15ba93a7a043d1aef976fedd2a6092: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-8b15ba93a7a043d1aef976fedd2a6092.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-ce6db118860b4858be895f460665c66e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-ce6db118860b4858be895f460665c66e.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-6160bdeaf3274b56a819cf877d08df8f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47666 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-6160bdeaf3274b56a819cf877d08df8f.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-f8c856065a14460dad3369e2947e1d85: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-f8c856065a14460dad3369e2947e1d85.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-e73eae899a7d43e793cd345ab29adfcb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-e73eae899a7d43e793cd345ab29adfcb.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-074a47c67cef4e768d4190ef95f6693c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-074a47c67cef4e768d4190ef95f6693c.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-8b91cdb4d01f40f5b58aaec5ef75542f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-8b91cdb4d01f40f5b58aaec5ef75542f.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-9ba42e1dfdb14932bc5c9cd62af563db: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-9ba42e1dfdb14932bc5c9cd62af563db.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-c3dfbd34e95d41f4a206bebe0ecfe231: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-c3dfbd34e95d41f4a206bebe0ecfe231.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-1811444d680240c2b1af0fc2e8e256b7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-1811444d680240c2b1af0fc2e8e256b7.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-ac5b108220b54d43b756ccdff84c933e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-3daa7030b0c4458497d3fddc294ea31c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-ac5b108220b54d43b756ccdff84c933e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-3daa7030b0c4458497d3fddc294ea31c.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-4cfb539c62494706b30c1df1f7b6b3aa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-4cfb539c62494706b30c1df1f7b6b3aa.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-789a93308540462390c69880d36cb607: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-789a93308540462390c69880d36cb607.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-986bd6842f024db2bf2e9e3f423ca1a7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-986bd6842f024db2bf2e9e3f423ca1a7.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-d7dd67f4d51849979ecfd6b2fa8d9510: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-36d293ef0dbc40278ce3f276d76ac4ed: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-d7dd67f4d51849979ecfd6b2fa8d9510.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-36d293ef0dbc40278ce3f276d76ac4ed.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-52e552af76554bcc8f1a88639d9c7ec1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-52e552af76554bcc8f1a88639d9c7ec1.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-2eabdfbeb7a64024950d54335bcdb35c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-2eabdfbeb7a64024950d54335bcdb35c.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-40e05d912dc445f9b8311d6013c696dd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-40e05d912dc445f9b8311d6013c696dd.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-176cdb77eacf4351ba0fd398d8c65a50: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-176cdb77eacf4351ba0fd398d8c65a50.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-daf8bc8c38634d78b2074e2386b1361b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-daf8bc8c38634d78b2074e2386b1361b.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-63f6ca94826a448eacc67895e326682c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-63f6ca94826a448eacc67895e326682c.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-71648cc2d8c543b1bf0df409dc9c29da: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-71648cc2d8c543b1bf0df409dc9c29da.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-8b757fb494a841f6ab4c8324edfb004f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-8b757fb494a841f6ab4c8324edfb004f.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-cf875c09e3d9426cb7ccda47fee57c56: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-b4a84cf70a6a4339b711ba709afa603b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-cf875c09e3d9426cb7ccda47fee57c56.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-b4a84cf70a6a4339b711ba709afa603b.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-95f37b58c9aa419ebd6a5dd26e441369: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-baca70c1d1b34847b00285e1f26137b4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-95f37b58c9aa419ebd6a5dd26e441369.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-baca70c1d1b34847b00285e1f26137b4.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-1339976d91ff4d9b93a1d25585e9df8a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-1339976d91ff4d9b93a1d25585e9df8a.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-55503cb3bb0d4c24b3844facb66b7e18: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-55503cb3bb0d4c24b3844facb66b7e18.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-1bc4a9c464f94ff8a2aea6d7b18b70c0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-1bc4a9c464f94ff8a2aea6d7b18b70c0.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-ffbf3e2f8aa04f58a3c65b309c3040fd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-ffbf3e2f8aa04f58a3c65b309c3040fd.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-ea19491f9e764d9b9c0e3367dc6dde80: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [logger.py:43] Received request chatcmpl-749ae753af244dd9af5754d4ef0b81dd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-ea19491f9e764d9b9c0e3367dc6dde80.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:01 [async_llm.py:270] Added request chatcmpl-749ae753af244dd9af5754d4ef0b81dd.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-c5cd5ca4cd0348c083e57942bc749998: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-c5cd5ca4cd0348c083e57942bc749998.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-16f5042ff85d4af7a66fc07cbd215ed7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:47998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-16f5042ff85d4af7a66fc07cbd215ed7.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-c153d9df398e4ebe92f249b0c1a7e5af: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-c153d9df398e4ebe92f249b0c1a7e5af.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-f6eb8c2b03b1463e8f66f78d64b6d7ce: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-f6eb8c2b03b1463e8f66f78d64b6d7ce.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-7b9455f9c3fb4a90bff870b0b676c396: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-7b9455f9c3fb4a90bff870b0b676c396.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-e4f4ec5cc2594603b54afff0b84d52c8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-e4f4ec5cc2594603b54afff0b84d52c8.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-d3f922a0dc8c4ee49de3b7a6c389b48a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-d3f922a0dc8c4ee49de3b7a6c389b48a.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-3c1bf3f99d9a4d4a9618df087983cd81: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-3c1bf3f99d9a4d4a9618df087983cd81.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-85b1d26f96564ea792d8ccd0a0df2a85: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-85b1d26f96564ea792d8ccd0a0df2a85.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-d5043f44cca94f4f9a6feaf07b389b16: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-d5043f44cca94f4f9a6feaf07b389b16.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-8462b1541e0c44609b8b459408d17675: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-8215ab571cd9425687cb52b783207f33: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-8462b1541e0c44609b8b459408d17675.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-24e228e77b34476094363b1ffdc98e04: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-8215ab571cd9425687cb52b783207f33.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-24e228e77b34476094363b1ffdc98e04.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-70de0fb8f7124ec88f85d8729407b572: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48088 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-70de0fb8f7124ec88f85d8729407b572.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-557164dc44b64709a2550608f0e90f13: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-557164dc44b64709a2550608f0e90f13.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-b37e0ff6fe98434dbf0b49fb113036b0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-b37e0ff6fe98434dbf0b49fb113036b0.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-ec3d9c99523b466d87696325dfe0bcbf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-ec3d9c99523b466d87696325dfe0bcbf.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-28886798e493410db2b25da6324712fe: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-28886798e493410db2b25da6324712fe.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-771754e98a894f10b6811d6f88374664: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-771754e98a894f10b6811d6f88374664.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-9189367ed11d40eabf880f3e508d737d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-9189367ed11d40eabf880f3e508d737d.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-256db8076563434ea48e3391aa0b2f5a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-256db8076563434ea48e3391aa0b2f5a.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-0c7681f35afb4b01a10b4754868fa8cd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-0c7681f35afb4b01a10b4754868fa8cd.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-8774667d46904e01abf69829f8e0a6ae: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-8774667d46904e01abf69829f8e0a6ae.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-c4c2bb680ad54c15ba6822bc7c04a5e2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-c4c2bb680ad54c15ba6822bc7c04a5e2.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-6ef2d41246254ac5ab5ec409b082ecf0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-6ef2d41246254ac5ab5ec409b082ecf0.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-3c54a90a136a4c298504c5e8ba52736d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-9084923f72514991a34f6e52530e2a1d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-3c54a90a136a4c298504c5e8ba52736d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-9084923f72514991a34f6e52530e2a1d.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-74d0dcf0b66349b8b6ae83121e76987a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-74d0dcf0b66349b8b6ae83121e76987a.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-7d57de4fd24b4b3f929121e7fafc76a0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-7d57de4fd24b4b3f929121e7fafc76a0.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-e71542da9065436a81d95b7c6ea9e307: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-e71542da9065436a81d95b7c6ea9e307.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-3ce56af572ac4a8cb3692f9b29216d65: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:48248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-3ce56af572ac4a8cb3692f9b29216d65.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-49175ce343a24603bc547839118acb26: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-49175ce343a24603bc547839118acb26.
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [logger.py:43] Received request chatcmpl-2f26c683912842b7ad8adcc59c98772d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60040 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:02 [async_llm.py:270] Added request chatcmpl-2f26c683912842b7ad8adcc59c98772d.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-2ba46a1a845a4893995aa612bc50b1e1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-ed70b42127e3478b9308272df1914044: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-cc8d0e0f80a242b68c73dc7b9ab0c497: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-2ba46a1a845a4893995aa612bc50b1e1.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-4382ad4f0b2741b6aa90b47627eab088: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-ed70b42127e3478b9308272df1914044.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-a2cce0eb6f9343b49b7579e46a3bf0d5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-cc8d0e0f80a242b68c73dc7b9ab0c497.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-4382ad4f0b2741b6aa90b47627eab088.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-a2cce0eb6f9343b49b7579e46a3bf0d5.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-d0f060d61bc84f2ba73cf2b47b71f8e3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-d0f060d61bc84f2ba73cf2b47b71f8e3.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-176ca87d9dfe4f25a9b7e1882b1cee52: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-176ca87d9dfe4f25a9b7e1882b1cee52.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-b0b1389f447d4034a721ab019f7086fd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-b0b1389f447d4034a721ab019f7086fd.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-7ab751c9681540e9840956a44bd95259: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-7ab751c9681540e9840956a44bd95259.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-dd6dbe973c4b424f8a5ecd2f1f7fc95d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-dd6dbe973c4b424f8a5ecd2f1f7fc95d.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-66e52490f1ef4e40949399ac4d03b933: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-66e52490f1ef4e40949399ac4d03b933.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-72578db50a4f484ea807b0e63f45af5e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-72578db50a4f484ea807b0e63f45af5e.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-70ea8666c14c42269f0580aebb21ffee: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-70ea8666c14c42269f0580aebb21ffee.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-00cdab1876ca4950984f07fa05f9382e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-00cdab1876ca4950984f07fa05f9382e.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-95062a67f2e942ca921b745f1d1b7e4c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-95062a67f2e942ca921b745f1d1b7e4c.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-ebb11154600e4546b444c6db0259cfb9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-ebb11154600e4546b444c6db0259cfb9.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-e444d2c0095b4813a0dbdc53af9edf57: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-e444d2c0095b4813a0dbdc53af9edf57.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-693c38d0802f48fe9557d8b43906979e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-693c38d0802f48fe9557d8b43906979e.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-e19e0a20975e44cdb4f8fc111eb68de2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-e19e0a20975e44cdb4f8fc111eb68de2.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-fd31f400331c46df9f070fe7fa6cb858: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-fd31f400331c46df9f070fe7fa6cb858.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-0201071f68934bd4af97e97e4993e969: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-0201071f68934bd4af97e97e4993e969.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-31261e22da3243eab99b601472b2e8c3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60224 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-31261e22da3243eab99b601472b2e8c3.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-544e0b0a67f84eddb46320ed637e4cd2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-544e0b0a67f84eddb46320ed637e4cd2.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-a875d596762541688dbb5598ea4fc21e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-a875d596762541688dbb5598ea4fc21e.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-b0c6ad2730d54c1590025ba8ea30fb26: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-b0c6ad2730d54c1590025ba8ea30fb26.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-13be15e4578345fe941cb2501363b46c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-13be15e4578345fe941cb2501363b46c.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-bba7d542ad234b319ac90bdf04e09c2c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-bba7d542ad234b319ac90bdf04e09c2c.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-2aae76ae571444d7b4375d158e6aaf5a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-2aae76ae571444d7b4375d158e6aaf5a.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-bf252bc0af3245c8b76bbee8d75d471e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-bf252bc0af3245c8b76bbee8d75d471e.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-ff485fadc6b342a39a1dc8efe3b830d9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-ff485fadc6b342a39a1dc8efe3b830d9.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-f0b08260760d4fc385af7220320a99be: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-f0b08260760d4fc385af7220320a99be.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-a029adfe479d4f64ac5c7dc36fc69e74: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-a029adfe479d4f64ac5c7dc36fc69e74.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-6d7d549d0eb84391932aff549b02346b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-6d7d549d0eb84391932aff549b02346b.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-953763cc102f4d42b3be421a6f6e049d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-953763cc102f4d42b3be421a6f6e049d.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-f785de93fc844235a28470a1ce233678: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-f785de93fc844235a28470a1ce233678.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-7508cc8832294b7b96833a1199eb4b40: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-7508cc8832294b7b96833a1199eb4b40.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-18967e8e79c04ee5a6816e13b78132ae: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-18967e8e79c04ee5a6816e13b78132ae.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-79ed49bc04a44a509cd96b58cd6f72f1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-79ed49bc04a44a509cd96b58cd6f72f1.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-3cfb6348691a48cba8dae2b7807b8536: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-3cfb6348691a48cba8dae2b7807b8536.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-402165d531014e719eeadbb836f5e62e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-402165d531014e719eeadbb836f5e62e.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-edb873b80ef443d291e4c51dd89933e9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-edb873b80ef443d291e4c51dd89933e9.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-3014ec1d9ea24b9d87de29720900e910: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-3014ec1d9ea24b9d87de29720900e910.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-4bd93aae3e6648ba8f96ce1a8714b032: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-4bd93aae3e6648ba8f96ce1a8714b032.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-ea26ce1cbf0549e88e011b735ac8009e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-ea26ce1cbf0549e88e011b735ac8009e.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-4dd47e0e1bd245179a2641a8a6525492: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-a2acce512d9b439997f1638b7ffcd5bc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-4dd47e0e1bd245179a2641a8a6525492.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-a2acce512d9b439997f1638b7ffcd5bc.
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [logger.py:43] Received request chatcmpl-c46b0cdec54e4348846414151ae69825: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:03 [async_llm.py:270] Added request chatcmpl-c46b0cdec54e4348846414151ae69825.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-693adac4ce414224a024db4662031a64: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-693adac4ce414224a024db4662031a64.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-fe5d374687ba451cae969e72feb03a58: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-fe5d374687ba451cae969e72feb03a58.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-150e49568fbd4ee89b8180a3f024a129: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-150e49568fbd4ee89b8180a3f024a129.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-6015e65c1cd84ce6901b101892af34f8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-6015e65c1cd84ce6901b101892af34f8.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-c63556aca27c4c9a965739ab913b983a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-c63556aca27c4c9a965739ab913b983a.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-85cbc0e804344c0d8acfb5272001703b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-85cbc0e804344c0d8acfb5272001703b.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-90a60d8fecf74ad0a7d498a9b37b9e05: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-90a60d8fecf74ad0a7d498a9b37b9e05.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-1e46a2b299a84f2b963d5ec1db37a19b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-1e46a2b299a84f2b963d5ec1db37a19b.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-274420c205e046019e6f80a89d41568e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60524 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-274420c205e046019e6f80a89d41568e.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-4cdbbeb24c15418e806a0214d5bac4fa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-4cdbbeb24c15418e806a0214d5bac4fa.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-dda79859f59248a6987372dc9d8d5259: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-297c6ed1441149888e464b8c4663dcd5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-bc9f8644adf74bf5a5a7426fc969b9e3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-dda79859f59248a6987372dc9d8d5259.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-297c6ed1441149888e464b8c4663dcd5.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-bc9f8644adf74bf5a5a7426fc969b9e3.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-e6613fdcf39a4f91b4492b7d40c9c06e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-8b37d0d9d9cf44bd91e098a7890f4395: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-e6613fdcf39a4f91b4492b7d40c9c06e.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-8b37d0d9d9cf44bd91e098a7890f4395.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-a123c4e9ee6c4f9b8fe5f33237707824: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-a123c4e9ee6c4f9b8fe5f33237707824.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-bb0a4ba75bac469db67904b7bb12b816: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-bb0a4ba75bac469db67904b7bb12b816.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-67873ec4ac6d42078d70299bc72c46ad: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-67873ec4ac6d42078d70299bc72c46ad.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-04573070e2ec4b13b69faa0e76bc770c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-04573070e2ec4b13b69faa0e76bc770c.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-ecef7582daf04b929713d26a2ba81df0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-ecef7582daf04b929713d26a2ba81df0.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-da3a1c0b84bc41309ffd3c5b97a27ad4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-da3a1c0b84bc41309ffd3c5b97a27ad4.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-44214c92eb2b449a8a4a224afa2e2739: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-713151b6e9d14a8d8e5bc8a392e0d387: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-44214c92eb2b449a8a4a224afa2e2739.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-713151b6e9d14a8d8e5bc8a392e0d387.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-ea27665a21ca439d862132b8be01b3c6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-ea27665a21ca439d862132b8be01b3c6.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-8703df39d87f4bc5b59a18b9cde6d0ec: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-8703df39d87f4bc5b59a18b9cde6d0ec.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-06e6bad9d4c246489abd9bc2e03ca8fd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-06e6bad9d4c246489abd9bc2e03ca8fd.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-d7ca5b7d42544553b6f97380d10d2804: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-d7ca5b7d42544553b6f97380d10d2804.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-f29e5cf41330431881275f8dcd80ed94: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-f29e5cf41330431881275f8dcd80ed94.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-1890750041e7416cb4950a6af6154936: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-1890750041e7416cb4950a6af6154936.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-f11d9a3ad9394f9c8ce11d9f2c9318ae: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-f11d9a3ad9394f9c8ce11d9f2c9318ae.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-1b9d2c9b9bc54022b2189d9341e84ad8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-1b9d2c9b9bc54022b2189d9341e84ad8.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-0fd3f4489c504ef0ae22ba35d1dec7d3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-9b7dbef1dbe5470aa70e9ffb905f92b3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-1bd80c8612a64cdaa30c1e4db92f3276: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-0fd3f4489c504ef0ae22ba35d1dec7d3.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-9b7dbef1dbe5470aa70e9ffb905f92b3.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-1bd80c8612a64cdaa30c1e4db92f3276.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-71db540002bc410aa01e641f610a784e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-71db540002bc410aa01e641f610a784e.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-b532abf504724ba9b0df7e2fb44ebe2e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-b532abf504724ba9b0df7e2fb44ebe2e.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-20bd4bbb1a3b4a9aaad87c6b9d196bb2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-20bd4bbb1a3b4a9aaad87c6b9d196bb2.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-3300fa82181d48efbf44a889b92a9e93: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-3300fa82181d48efbf44a889b92a9e93.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-7eb610e0e05e41148fbeb920fa72a1de: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-7eb610e0e05e41148fbeb920fa72a1de.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-ef5def41712a4e618655e78e1d624e34: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-ef5def41712a4e618655e78e1d624e34.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-bd43e137b16c4ec3a57fb1ad0f996e9d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-bd43e137b16c4ec3a57fb1ad0f996e9d.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-53444aac9dfc48cf88bae42cefbed82a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-53444aac9dfc48cf88bae42cefbed82a.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-32c85bafebda454194da0ee5df73144f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-2f2ced3d4f7d4af5b1cc6216dcee94ea: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-32c85bafebda454194da0ee5df73144f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-2f2ced3d4f7d4af5b1cc6216dcee94ea.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-0358cb35788e4f168c6e2766277994a5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-0358cb35788e4f168c6e2766277994a5.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-ffa5e05b9daf442dbfa64a3a6d30dae8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-ffa5e05b9daf442dbfa64a3a6d30dae8.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-a222908a9cd34753bffcaebf2cdae594: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-b5a499f519984ddeaf64391930d1cdee: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-a222908a9cd34753bffcaebf2cdae594.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-b5a499f519984ddeaf64391930d1cdee.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-a6b553666f874df9b4b9f1d4aefe44d0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-a6b553666f874df9b4b9f1d4aefe44d0.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-1c93083e1b2e4bd98df58cbc4a8c5800: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-1c93083e1b2e4bd98df58cbc4a8c5800.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-f5e7543ef375451983674ce51c53ffdf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-f5e7543ef375451983674ce51c53ffdf.
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [logger.py:43] Received request chatcmpl-d92b67c7ab1542e192512f6596e643e5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:04 [async_llm.py:270] Added request chatcmpl-d92b67c7ab1542e192512f6596e643e5.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-6fd2cc1049074ae18adb5a61ee04ca5a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-6fd2cc1049074ae18adb5a61ee04ca5a.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-67607e90934d4e3c8b7be500ccd877cf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-67607e90934d4e3c8b7be500ccd877cf.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-cf5c7a6302824c09922dbfbe06970c10: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-45046704601f451f88440f072ee6060f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-cf5c7a6302824c09922dbfbe06970c10.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-45046704601f451f88440f072ee6060f.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-67238da1d7b14476aeced2ef5d128099: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-67238da1d7b14476aeced2ef5d128099.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-90002706247e47ef8e63ad50e6b4b0dc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-90002706247e47ef8e63ad50e6b4b0dc.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-1f0e5eb4019c4c9eb6ee61142ad7d6ae: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-1f0e5eb4019c4c9eb6ee61142ad7d6ae.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-bc4fddaa316e4d76b65542559944e1ec: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:60994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-bc4fddaa316e4d76b65542559944e1ec.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-1e2153191e014603a827bfca01da81ea: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-1e2153191e014603a827bfca01da81ea.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-b39b9736f3954604ad4996834e1e20c4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-b39b9736f3954604ad4996834e1e20c4.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-23935285b7134d76ac1c16d3540ff042: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-23935285b7134d76ac1c16d3540ff042.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-49f3a714e17642c1aeb1651119602cb3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-3abd5b3e0437430a8e286e057c577f3d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-49f3a714e17642c1aeb1651119602cb3.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-3abd5b3e0437430a8e286e057c577f3d.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-d091b1d3aaed4bf78fa4903a98e74b66: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-d091b1d3aaed4bf78fa4903a98e74b66.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-d67420af179741f1824017955eb6d477: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-d67420af179741f1824017955eb6d477.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-25b0a5def87c4c81b02cc1e3e42a51d4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-25b0a5def87c4c81b02cc1e3e42a51d4.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-bb3b1c56d429431293f7549d9d3826d1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-bb3b1c56d429431293f7549d9d3826d1.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-41ee196ab5bb415abdba9c0ebd90aaa2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-92f2c0b34dd642518bbcc9dea254286d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-41ee196ab5bb415abdba9c0ebd90aaa2.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-92f2c0b34dd642518bbcc9dea254286d.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-cf8f622d14944872972b326049c51633: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-cf8f622d14944872972b326049c51633.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-37e672d3f5064b6cb0446586d540a28a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-37e672d3f5064b6cb0446586d540a28a.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-eca4fbabb9ed425287b1268c408161bb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32908 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-eca4fbabb9ed425287b1268c408161bb.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-ee4660dc84cf46c998f671e732e0423f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-ee4660dc84cf46c998f671e732e0423f.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-c81629cce18545f2a77db2935ad51993: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-c81629cce18545f2a77db2935ad51993.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-0e42c05b5f3b43a08bef433bb4c864c9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-0e42c05b5f3b43a08bef433bb4c864c9.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-87f12e207cb64c7486f59347f250ee0d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-87f12e207cb64c7486f59347f250ee0d.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-0d7c812cb05a4cd79b296562ecb532b9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-0d7c812cb05a4cd79b296562ecb532b9.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-9a721b7475954a908844e2864f5f24f1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-9a721b7475954a908844e2864f5f24f1.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-2bb78a5955ad48649bd3c90fbc9233ca: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-2bb78a5955ad48649bd3c90fbc9233ca.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-302b00481bc445e68ae26c697b3b54b2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32980 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-302b00481bc445e68ae26c697b3b54b2.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-314c48b9bee44915941b6928fc49476f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:32990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-314c48b9bee44915941b6928fc49476f.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-2c2addeb03064dea82564f52f4602afe: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-2c2addeb03064dea82564f52f4602afe.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-dc990b6418ec4ddfb619a8fb04850c85: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-dc990b6418ec4ddfb619a8fb04850c85.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-3ae83db537fd468f80b201adbfa780f4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-3ae83db537fd468f80b201adbfa780f4.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-7a8816af9df048a79ef37dec8a8f85b2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-7a8816af9df048a79ef37dec8a8f85b2.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-c649a45bd76e473b974721965f525b55: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-c649a45bd76e473b974721965f525b55.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-848f98ffd38048c8a99e15bb4bd98fba: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-848f98ffd38048c8a99e15bb4bd98fba.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-5ca688c363d24be7b338ac39811d226d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-5ca688c363d24be7b338ac39811d226d.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-c0b159ce028b45af836f7aa2d51193b6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-c0b159ce028b45af836f7aa2d51193b6.
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [logger.py:43] Received request chatcmpl-2330e2784ef24238baad3344c319a3d7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:05 [async_llm.py:270] Added request chatcmpl-2330e2784ef24238baad3344c319a3d7.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-f93a70b392e5431f817f82e1bea18cfc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-f93a70b392e5431f817f82e1bea18cfc.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-3ebd79548a394e0192ce12a7c5b4a482: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-3ebd79548a394e0192ce12a7c5b4a482.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-1286e6070c0e4baeb1a666648697853a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-1286e6070c0e4baeb1a666648697853a.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-e064265ab91d4b20a3410c490ab4c5a0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33108 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-e064265ab91d4b20a3410c490ab4c5a0.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-33e40efb374041458ed435ddf8c441e9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-b619f4c8de844a28a89584d39b307f45: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-33e40efb374041458ed435ddf8c441e9.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-b619f4c8de844a28a89584d39b307f45.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-786a3a2d0b2346648f3cb6837313cb1a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-786a3a2d0b2346648f3cb6837313cb1a.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-7228b1bfa5d54b06a979316aafbc7c6f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-7228b1bfa5d54b06a979316aafbc7c6f.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-3f747191cb3745b3a26483ad57f88a92: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-3f747191cb3745b3a26483ad57f88a92.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-bd77d5ce98ae4c64a4d76c12937cb401: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-bd77d5ce98ae4c64a4d76c12937cb401.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-90fbf80c78bb461b88f1a46f57cc852d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-90fbf80c78bb461b88f1a46f57cc852d.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-04e171f834e24e5a8c9a81b64141f153: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-04e171f834e24e5a8c9a81b64141f153.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-29d2291a0d5b4364967c690196820918: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-29d2291a0d5b4364967c690196820918.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-82fbd4b654b2482c9f52b1ebaef738bd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-82fbd4b654b2482c9f52b1ebaef738bd.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-04d9cec853ff47758fe1ff488b91005d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-04d9cec853ff47758fe1ff488b91005d.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-3a430a57c27d494a9e87ab4776b7e3bd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-3a430a57c27d494a9e87ab4776b7e3bd.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-25cf53010f5c44378d509e2895ab5dde: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-5928a66e499f480b9a066e6c14f26439: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-25cf53010f5c44378d509e2895ab5dde.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-5928a66e499f480b9a066e6c14f26439.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-53da4ddbbfc640b1993d5802e7c9d5f9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-53da4ddbbfc640b1993d5802e7c9d5f9.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-6b3ece92733f4a1e874110a440430c9d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-6b3ece92733f4a1e874110a440430c9d.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-edd8a1aa66b44fdf90337de2d58d2c41: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-edd8a1aa66b44fdf90337de2d58d2c41.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-9017865af3ab4bc8909f73f73744d252: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-9017865af3ab4bc8909f73f73744d252.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-d31f00f20a0b445693a8a8894f92889b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-d31f00f20a0b445693a8a8894f92889b.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-9d2f77970e324aafb4809421d9ff9ed7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-9d2f77970e324aafb4809421d9ff9ed7.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-c2c12bce016c4e33b2948aed051b5edf: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-c2c12bce016c4e33b2948aed051b5edf.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-b4b6250206c447c2b3152b349e4be752: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-b4b6250206c447c2b3152b349e4be752.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-75887274a4da40c8995ce76dd74b9f32: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-75887274a4da40c8995ce76dd74b9f32.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-85db8adfe4e542c984cbd9a622996003: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-85db8adfe4e542c984cbd9a622996003.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-b8db3f1d74744cdb8a04368ec436e81a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-b8db3f1d74744cdb8a04368ec436e81a.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-13b5a34e044946878928408c091c1cda: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-13b5a34e044946878928408c091c1cda.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-0100d0f9074244a6aee6cb53e1e99cfa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-0100d0f9074244a6aee6cb53e1e99cfa.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-46f4dd5ed1f74c1fb26ad233765d5638: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33314 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-46f4dd5ed1f74c1fb26ad233765d5638.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-d7659fea6f28438f9290724a6e4cdf67: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-d7659fea6f28438f9290724a6e4cdf67.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-84b7250cdf72457c924a6c29f8484431: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-84b7250cdf72457c924a6c29f8484431.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-64971ea97aef423cbb83453019b06aa5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-64971ea97aef423cbb83453019b06aa5.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-e7bca6f433354e1a84601014c2739ec3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-e7bca6f433354e1a84601014c2739ec3.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-0090b8fa89a3469ba71dace5c392d6fe: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-0090b8fa89a3469ba71dace5c392d6fe.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-1aa81d919a7642bcb3671d79356d1e59: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-1aa81d919a7642bcb3671d79356d1e59.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-770a80027cea4aa698673c863c2fdc87: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-770a80027cea4aa698673c863c2fdc87.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-bfe66f454ca9427281ff93beb142c032: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-bfe66f454ca9427281ff93beb142c032.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-cf9b6c2692a04b5dbb9f9c2b06f88379: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-cf9b6c2692a04b5dbb9f9c2b06f88379.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-b66d8281638945f98eb59b876ccb280d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-b66d8281638945f98eb59b876ccb280d.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-5b29d96c3d5f40df85bc620283fb1758: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-5b29d96c3d5f40df85bc620283fb1758.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-528ab641e4e346d3afe8449d9a6c29ba: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-528ab641e4e346d3afe8449d9a6c29ba.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-7ad14ca476e84b0f890f8c78dd136685: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-7ad14ca476e84b0f890f8c78dd136685.
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [logger.py:43] Received request chatcmpl-8d2db0f8b6f343d0b34405f2fcd0c057: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:06 [async_llm.py:270] Added request chatcmpl-8d2db0f8b6f343d0b34405f2fcd0c057.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-825bc66dd11f4a6a81c4ffb26a94b20f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-825bc66dd11f4a6a81c4ffb26a94b20f.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-4d17dd0c623c44ebbc252cedbeb26d2e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-4d17dd0c623c44ebbc252cedbeb26d2e.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-6730de8c602842209721f033df207c68: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-6730de8c602842209721f033df207c68.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-375837614be14f9c889523847279d129: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-375837614be14f9c889523847279d129.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-d7d80bb4f8eb4269a7300331a1f4abb5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-d7d80bb4f8eb4269a7300331a1f4abb5.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-6c7b830efb27491fbf28507a4e6753d5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-6c7b830efb27491fbf28507a4e6753d5.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-bef02494710e4ecea020c3df941c30c1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-bef02494710e4ecea020c3df941c30c1.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-7a905d41a5ce4515bca48ac9e5fb77f5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-6077b2f163c640ee8cf68588d5e6c138: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-7a905d41a5ce4515bca48ac9e5fb77f5.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-6077b2f163c640ee8cf68588d5e6c138.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-816b03419b3b4e709a03779fae4c9306: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-816b03419b3b4e709a03779fae4c9306.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-2d5ad0f0a6864616b58aea019fc572a7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-2d5ad0f0a6864616b58aea019fc572a7.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-a85879fcb14247bc913ecfd091b29e3d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-a85879fcb14247bc913ecfd091b29e3d.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-3dfcd2ae7a224b219b618ff421d419d1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-3dfcd2ae7a224b219b618ff421d419d1.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-84e2723f30454943874d89adc2097230: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-84e2723f30454943874d89adc2097230.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-6aab0a4463284e2fad2efaee36015c17: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-6aab0a4463284e2fad2efaee36015c17.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-18e454bd37c24d2a9d6f2992bb907d7a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-18e454bd37c24d2a9d6f2992bb907d7a.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-e7bc7a07864643358ba83ccb0d43440c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-e7bc7a07864643358ba83ccb0d43440c.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-b4fe77e064a34753873b31b2d487d42c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-b4fe77e064a34753873b31b2d487d42c.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-6138566fda35465d9124e50606378c1b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-6138566fda35465d9124e50606378c1b.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-ce177c36ab0c4810bec5c14323643fa3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-ce177c36ab0c4810bec5c14323643fa3.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-328bb4175d31479089c2e8de08e6c927: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-328bb4175d31479089c2e8de08e6c927.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-46c4e27a06924b5e949dd5f1d42103e6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-46c4e27a06924b5e949dd5f1d42103e6.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-e4cdb8c48b69473d8fe8939697cca9c8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-e4cdb8c48b69473d8fe8939697cca9c8.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-e5dc5cc0256a4a5fa626bd217870addc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-e5dc5cc0256a4a5fa626bd217870addc.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-20b9bb95b31b4083856eeed801308a44: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-20b9bb95b31b4083856eeed801308a44.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-8c0eb50a5a814268afa295fcaf042766: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-8c0eb50a5a814268afa295fcaf042766.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-b6f256f7d87e499fa143add46ebf043b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-b6f256f7d87e499fa143add46ebf043b.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-8c646c11bc4f4d54866612e246612866: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-8c646c11bc4f4d54866612e246612866.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-61e865c018c84fdd86c67e6af1620102: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-61e865c018c84fdd86c67e6af1620102.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-0b675398d6c74005a4728d73a57c646a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-0b675398d6c74005a4728d73a57c646a.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-f6f1f711415449f09e48f7597394c29c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-f6f1f711415449f09e48f7597394c29c.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-1b63e99a215a4918959b592dba52268b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-1b63e99a215a4918959b592dba52268b.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-9acf760cac474537be94d10f367fcb1b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-9acf760cac474537be94d10f367fcb1b.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-53c1abe9f29945e4993d52010364b8ee: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-53c1abe9f29945e4993d52010364b8ee.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-489aba109b32406fbb6865f051759b7a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-489aba109b32406fbb6865f051759b7a.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-2b7e43ea5c614b359ab93380d3824814: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-2b7e43ea5c614b359ab93380d3824814.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-0489ee3b938b40a58dd0da8d11df4d5d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-0489ee3b938b40a58dd0da8d11df4d5d.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-711df44732484db996e994ee43221c10: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-711df44732484db996e994ee43221c10.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-53154d1cabab4aa6b13d9c01fe76a5b4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-53154d1cabab4aa6b13d9c01fe76a5b4.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-77fd196d40344f5d950ca23ea3fc9556: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-77fd196d40344f5d950ca23ea3fc9556.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-3e811e75fc6643429500b0939347acce: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-3e811e75fc6643429500b0939347acce.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-26b73f82cc1b40c1921af49270c592f1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [async_llm.py:270] Added request chatcmpl-26b73f82cc1b40c1921af49270c592f1.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-0a2336f2318f49dc9811712b95ce871d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:07 [logger.py:43] Received request chatcmpl-19a7be049b284294b8e14e62272bda38: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-0a2336f2318f49dc9811712b95ce871d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-19a7be049b284294b8e14e62272bda38.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-247fa490aee444be9390fc429aafbd2d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-dae78dd570654d0cb24719ab287420bc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-d3ca6713a23f4956b21de4e47c8ca05d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-247fa490aee444be9390fc429aafbd2d.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-dae78dd570654d0cb24719ab287420bc.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-d3ca6713a23f4956b21de4e47c8ca05d.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-c69994ccd30340a49813258c8f360da6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-c69994ccd30340a49813258c8f360da6.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-3a313efe84434b0987d1b88be37d258a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-3a313efe84434b0987d1b88be37d258a.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-05c8f7d06f4748a481add807d7ed6cd3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-05c8f7d06f4748a481add807d7ed6cd3.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-a7b6f7fb208a44c99fc86ff540c3cb10: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-a7b6f7fb208a44c99fc86ff540c3cb10.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-d613c1efa3ba4df6b002cd6c6a1b1840: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-d613c1efa3ba4df6b002cd6c6a1b1840.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-eaa887dfaee44eb2bc34b6c065eba804: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-eaa887dfaee44eb2bc34b6c065eba804.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-f6f6804047d14993ad51088e9dfced83: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-f6f6804047d14993ad51088e9dfced83.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-743c6fe3343744bb9d154b71126908c2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-743c6fe3343744bb9d154b71126908c2.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-042983defdb7420d8da49ec4302cea1f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-042983defdb7420d8da49ec4302cea1f.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-bdac4f689db345a687884799dce7a8c2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-bdac4f689db345a687884799dce7a8c2.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-ef60e70e744f4d1f8fe77f08f4e6bbdd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-ef60e70e744f4d1f8fe77f08f4e6bbdd.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-a398df0d7ad54c3c9dfde0a500840a57: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-f90f99a220af4a3ea4c1a8df906784ce: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-a398df0d7ad54c3c9dfde0a500840a57.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-f90f99a220af4a3ea4c1a8df906784ce.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-d229cac248fe4478a61853b8e544502f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-5527d82f077b4d448890212ca52e4ef2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-d229cac248fe4478a61853b8e544502f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-5527d82f077b4d448890212ca52e4ef2.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-bc3d193283ed42a0945f166b8563498b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-bc3d193283ed42a0945f166b8563498b.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-41ba9b4795634e02b8a2a2ab884e5b1b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-41ba9b4795634e02b8a2a2ab884e5b1b.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-4ca56f36723444c486cc4a9a28194d96: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-4ca56f36723444c486cc4a9a28194d96.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-3f525c54962c4cd0936449aad130f6e4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-3f525c54962c4cd0936449aad130f6e4.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-3cde0570f38847cc9ad5f6fb429d6fa6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:33998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-3cde0570f38847cc9ad5f6fb429d6fa6.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-b85b76baf5e744599f0d99f336c88cfe: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-b85b76baf5e744599f0d99f336c88cfe.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-c2759a7ece1143079e21013c8b9d3f2a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-c2759a7ece1143079e21013c8b9d3f2a.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-6b1601170ec44941b8c6ba328a8d25c5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-6b1601170ec44941b8c6ba328a8d25c5.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-0ea37ba9e926475aa74c46606ce2fcba: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-0ea37ba9e926475aa74c46606ce2fcba.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-688ec2a30b654b328e3aa0aeb7c444ab: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-688ec2a30b654b328e3aa0aeb7c444ab.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-c12a1b9e0026429d9c1156fd1b05dc07: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-c12a1b9e0026429d9c1156fd1b05dc07.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-30cde97e90fe42dca3119762a42b27c3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-30cde97e90fe42dca3119762a42b27c3.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-eecec6fbb0574eb687a1ed132bd8e54f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-eecec6fbb0574eb687a1ed132bd8e54f.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-04f2fe7ba4c144ab8325624d753ee4c4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-04f2fe7ba4c144ab8325624d753ee4c4.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-e524eba063284a00927bae0652d434ae: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-e524eba063284a00927bae0652d434ae.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-0d6fe07eab0b41969be19d0bd9a99ae4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-0d6fe07eab0b41969be19d0bd9a99ae4.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-e7190e146223488c98fa80926c2861c4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-e7190e146223488c98fa80926c2861c4.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-0e5dd0acf9384a4caa5f1f1fcb26a55b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-0e5dd0acf9384a4caa5f1f1fcb26a55b.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-f18bed18b1ff479d9188495457a72a07: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34148 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-f18bed18b1ff479d9188495457a72a07.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-4fc8ca6ed8b44d1d9ad64d01a2b34e15: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-4fc8ca6ed8b44d1d9ad64d01a2b34e15.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-960f766c49cc46979a4c355f580c2003: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-960f766c49cc46979a4c355f580c2003.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-eff6271966ba40b3a1723e76fbfb17f6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-eff6271966ba40b3a1723e76fbfb17f6.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-9d5e899e8f46425ebc3c01d15f4e8c64: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-9d5e899e8f46425ebc3c01d15f4e8c64.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-f14db06af7ce4a24bd8c494829c690ba: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-f14db06af7ce4a24bd8c494829c690ba.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-200622e02e21424791fc8b3c07ccf3cf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-200622e02e21424791fc8b3c07ccf3cf.
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [logger.py:43] Received request chatcmpl-98f21b79ba624f1997250ce6efee2112: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:08 [async_llm.py:270] Added request chatcmpl-98f21b79ba624f1997250ce6efee2112.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-07c883cc3dac43d7bbf33351246b857f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-07c883cc3dac43d7bbf33351246b857f.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-387e9a6700ab4007afb9295abca3303f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-387e9a6700ab4007afb9295abca3303f.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-b632cc31c15141b7ba6430b94aafaf12: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-b632cc31c15141b7ba6430b94aafaf12.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-50b29760cd914d83b5fdfdbd715b3746: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-50b29760cd914d83b5fdfdbd715b3746.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-d033868fca124400abb2bf12b1f06098: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-d033868fca124400abb2bf12b1f06098.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-077026be7cc94b209c71f9a125c8a631: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-077026be7cc94b209c71f9a125c8a631.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-c2db0b31c3994874a1b983f35ccec4a3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-c2db0b31c3994874a1b983f35ccec4a3.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-bd4b621294f5457dae098a000b6388eb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-bd4b621294f5457dae098a000b6388eb.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-878ab1e8c7214699bc67722a126e99fc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-878ab1e8c7214699bc67722a126e99fc.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-072f38473ccc4754b9436f7956541468: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-072f38473ccc4754b9436f7956541468.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-82e78abf88174076890ebf4b2e1a69d4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-82e78abf88174076890ebf4b2e1a69d4.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-1199bee78f114c5491f7cae93ea251e7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-1199bee78f114c5491f7cae93ea251e7.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-83a6d323d11a49629902c44ed41ad57a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-83a6d323d11a49629902c44ed41ad57a.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-b0ae9659ca014c22b6e0681801c88bc3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-b0ae9659ca014c22b6e0681801c88bc3.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [loggers.py:118] Engine 000: Avg prompt throughput: 545.3 tokens/s, Avg generation throughput: 2427.6 tokens/s, Running: 124 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.0%, Prefix cache hit rate: 84.3%
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [loggers.py:118] Engine 001: Avg prompt throughput: 551.8 tokens/s, Avg generation throughput: 2424.3 tokens/s, Running: 121 reqs, Waiting: 0 reqs, GPU KV cache usage: 34.4%, Prefix cache hit rate: 84.1%
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [loggers.py:118] Engine 002: Avg prompt throughput: 556.3 tokens/s, Avg generation throughput: 2418.1 tokens/s, Running: 124 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.7%, Prefix cache hit rate: 84.9%
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [loggers.py:118] Engine 003: Avg prompt throughput: 560.9 tokens/s, Avg generation throughput: 2414.8 tokens/s, Running: 124 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.1%, Prefix cache hit rate: 84.2%
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-ba3113f0dfcb49b196af7cb74024a6f0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-ba3113f0dfcb49b196af7cb74024a6f0.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-08a3113df79f47b1bc51e66deb5e2f64: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-08a3113df79f47b1bc51e66deb5e2f64.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-c2538c6754f04f588c80e9f424334c1d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-c2538c6754f04f588c80e9f424334c1d.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-47aca5d9c19c4a31a11e3356bdcfa60e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-47aca5d9c19c4a31a11e3356bdcfa60e.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-8fc521085f5846758827ab3dfd2a3afe: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-8fc521085f5846758827ab3dfd2a3afe.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-989d12c9da8f4665ab56d38851fc03e9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-989d12c9da8f4665ab56d38851fc03e9.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-7257070c72944996acda8a1fe03ddb82: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-7257070c72944996acda8a1fe03ddb82.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-5981820123754109a55b77da3a1b61cb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-5981820123754109a55b77da3a1b61cb.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-919147f733dd4dd5a593d5ef961f2379: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-919147f733dd4dd5a593d5ef961f2379.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-83f86898961e4c2e81ce7a57d051f1d6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34418 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-83f86898961e4c2e81ce7a57d051f1d6.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-8a2b7cf62aea4b759a66ae47362b7617: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-8a2b7cf62aea4b759a66ae47362b7617.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-8001cdfd1e2642c3b7c3c5a35393fb9b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-49a60fa28e4b4a43b04338f213192cbc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-8001cdfd1e2642c3b7c3c5a35393fb9b.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-49a60fa28e4b4a43b04338f213192cbc.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-baa6598bdff3496a91ef12a58aa67611: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-baa6598bdff3496a91ef12a58aa67611.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-3e6bb691a322495d9b7aaf485b3c0a7d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-3e6bb691a322495d9b7aaf485b3c0a7d.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-58bd208fe9004945899caa4ebe70d30b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-58bd208fe9004945899caa4ebe70d30b.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-cfa4113423cf48f884347e77bd188750: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-cfa4113423cf48f884347e77bd188750.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-a50d4170124947b69b4342be89de6ce3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-a50d4170124947b69b4342be89de6ce3.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-13959cd9c70942dc87755d80c97e27fa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-13959cd9c70942dc87755d80c97e27fa.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-9ad04ad95d474fee97f2b4abe9e109ed: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-9ad04ad95d474fee97f2b4abe9e109ed.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-7b77a4e7aebd415a896a3d11c29b1143: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-1cb604be874746aabe321d723ca99d28: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-7b77a4e7aebd415a896a3d11c29b1143.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-1cb604be874746aabe321d723ca99d28.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-983d735c0b474c1694a0ce781b8d6591: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-f4afd311749d42d68c82acc1c2d06e79: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-983d735c0b474c1694a0ce781b8d6591.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-f4afd311749d42d68c82acc1c2d06e79.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-9f6074a1cf264c46829bdd4672ca7d1d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-9f6074a1cf264c46829bdd4672ca7d1d.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-1b3beb8ca7b7450fbe27e2d95cb0c3f2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-1b3beb8ca7b7450fbe27e2d95cb0c3f2.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-023adb197e014973b92d7c69dfc4d44c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-023adb197e014973b92d7c69dfc4d44c.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-5f0489054a4a4b97b3e857e5b2e95c9d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-5f0489054a4a4b97b3e857e5b2e95c9d.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-2c49b5496bd1416eacc00d8cfc523834: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-2c49b5496bd1416eacc00d8cfc523834.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-9bfc35b2348a4b2a805aeefbe73e8537: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-9bfc35b2348a4b2a805aeefbe73e8537.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-fcad1ffc14134844ad2d41fe9eb0d3ff: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-fcad1ffc14134844ad2d41fe9eb0d3ff.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-7d10147f551f40e6b2a20fedbb8a6a5d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-7d10147f551f40e6b2a20fedbb8a6a5d.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-9017ae62c84f434b917054f0bab78cb3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-9017ae62c84f434b917054f0bab78cb3.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-260b871d82f743e2b7df09a484a6b7ab: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-260b871d82f743e2b7df09a484a6b7ab.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-33e040de6aca421b9485a35b27a3e438: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-33e040de6aca421b9485a35b27a3e438.
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [logger.py:43] Received request chatcmpl-ef8575ede2d74a69aa2d295e07cd503b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:09 [async_llm.py:270] Added request chatcmpl-ef8575ede2d74a69aa2d295e07cd503b.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-b47db46c851742179f7679e8eedda603: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-b47db46c851742179f7679e8eedda603.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-9cad18b8fe3743ef850b1785d37f3b36: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-9cad18b8fe3743ef850b1785d37f3b36.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-0ae85fe294a0442c8ec8f187b7b5e05c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-0ae85fe294a0442c8ec8f187b7b5e05c.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-13e004da5a504d4abdabe99a901b58ee: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-13e004da5a504d4abdabe99a901b58ee.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-322805660c4a4cc6ac4ab0b39913ee48: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-322805660c4a4cc6ac4ab0b39913ee48.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-a2c0b7f96d164fe2b3cdb9e2e6c94377: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-a2c0b7f96d164fe2b3cdb9e2e6c94377.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-3355902df7604039adb09d071abbf102: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-3355902df7604039adb09d071abbf102.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-990323517dd74ef6a56a316cb1689539: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-990323517dd74ef6a56a316cb1689539.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-d2e9bf7e3e514b978a3520be8090f6c0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-d2e9bf7e3e514b978a3520be8090f6c0.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-0c8df23ccfb5456cb4f1c260a42b3746: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-0c8df23ccfb5456cb4f1c260a42b3746.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-14ff13a6e3e3415d8e9b0fc47690686e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34830 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-14ff13a6e3e3415d8e9b0fc47690686e.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-f5e15fd15e6647a4978fc1ed261a87a3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-f5e15fd15e6647a4978fc1ed261a87a3.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-6f787573d0a84a41bf8ef18ddb8d8bda: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-6f787573d0a84a41bf8ef18ddb8d8bda.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-c5c1cbc3227a4d9d90993fbd021358e5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-c5c1cbc3227a4d9d90993fbd021358e5.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-899adececc504baaac38729c7caea989: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-899adececc504baaac38729c7caea989.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-f5f2b38b4ca3414db38da1b6f6680b9a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-f5f2b38b4ca3414db38da1b6f6680b9a.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-67cde072d6bd41998e2bb971cd9a9878: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-67cde072d6bd41998e2bb971cd9a9878.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-12ecf7af57db4a339d57abeab1477cc2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-12ecf7af57db4a339d57abeab1477cc2.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-5dead08e730a418997f4c99bf753ac99: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-5dead08e730a418997f4c99bf753ac99.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-3299c7ae1717463b94ad161171d41e1d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-3299c7ae1717463b94ad161171d41e1d.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-53600415a4a14072956d3cfd1f59d845: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-53600415a4a14072956d3cfd1f59d845.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-38e4ccb813ed4e68946d2fbec8945143: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-38e4ccb813ed4e68946d2fbec8945143.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-5e8388120e5241ca8a12988522141cef: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-5e8388120e5241ca8a12988522141cef.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-571630c207d6467097efbe238e76a36d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-571630c207d6467097efbe238e76a36d.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-8e3c971d62b642c197a14b497e512a2a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-8e3c971d62b642c197a14b497e512a2a.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-16b509328a194c07ab8cd63a57846af1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-65d81e9956084cad9b5a921276155736: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-16b509328a194c07ab8cd63a57846af1.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-65d81e9956084cad9b5a921276155736.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-1d29cfd17bca46d6ad003181e111642a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-1d29cfd17bca46d6ad003181e111642a.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-468d1c9d7e3b4c18af46016f7dd7e3ef: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-468d1c9d7e3b4c18af46016f7dd7e3ef.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-a50c1f59dc304047ba4b4274a1e0a270: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-a50c1f59dc304047ba4b4274a1e0a270.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-13ce0c1dae5940129e793e6e052f12f1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-13ce0c1dae5940129e793e6e052f12f1.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-e6a7ae2b7e394dda8a35e0fecdc2a745: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-e6a7ae2b7e394dda8a35e0fecdc2a745.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-ceb2cccdfd974ce4964d80370d7cce5f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:34990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-ceb2cccdfd974ce4964d80370d7cce5f.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-2ed351eea6ef4d2cae5c996298889194: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35004 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-2ed351eea6ef4d2cae5c996298889194.
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [logger.py:43] Received request chatcmpl-1fc49eaef6a149d7a6533954e424fb91: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:10 [async_llm.py:270] Added request chatcmpl-1fc49eaef6a149d7a6533954e424fb91.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-ac2d59f5a27a433a88ddb7b6453f55b8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-ac2d59f5a27a433a88ddb7b6453f55b8.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-bbd800f712bb443cb666d6ddc67e1695: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-bbd800f712bb443cb666d6ddc67e1695.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-237b0b9df0b24cb0964e6817e10adc6f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-237b0b9df0b24cb0964e6817e10adc6f.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-e3a3e48ffcaa43199f1e555243a1e457: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-e3a3e48ffcaa43199f1e555243a1e457.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-68bfd1afcbd44062a17a1657390f5df2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-68bfd1afcbd44062a17a1657390f5df2.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-0fe5fc5f6ebd49128737c883ad6c901f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-db2c5933d6e9446ebe6555f42a807a9d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-0fe5fc5f6ebd49128737c883ad6c901f.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-db2c5933d6e9446ebe6555f42a807a9d.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-e64fbf9fec674091b5790e8f70373bc2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-e64fbf9fec674091b5790e8f70373bc2.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-c85a3131970849e4b871656631ef5caa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-c85a3131970849e4b871656631ef5caa.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-03ba1ae74fbd491e9b714b0cfdb9e64b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-03ba1ae74fbd491e9b714b0cfdb9e64b.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-1e89ec95b54e4c8a9a2ee900bdbdd5ad: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-1e89ec95b54e4c8a9a2ee900bdbdd5ad.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-c17fbb77c9cb48b0add5526a0203e788: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-c17fbb77c9cb48b0add5526a0203e788.
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [logger.py:43] Received request chatcmpl-d6fe90e445d5463c8766690afda23517: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.22.0.1:35156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-21 18:36:11 [async_llm.py:270] Added request chatcmpl-d6fe90e445d5463c8766690afda23517.
[36mllm_server_1  |[0m INFO 07-21 18:36:19 [loggers.py:118] Engine 000: Avg prompt throughput: 106.8 tokens/s, Avg generation throughput: 2213.3 tokens/s, Running: 21 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.8%, Prefix cache hit rate: 84.3%
[36mllm_server_1  |[0m INFO 07-21 18:36:19 [loggers.py:118] Engine 001: Avg prompt throughput: 121.0 tokens/s, Avg generation throughput: 2227.7 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.9%, Prefix cache hit rate: 84.2%
[36mllm_server_1  |[0m INFO 07-21 18:36:19 [loggers.py:118] Engine 002: Avg prompt throughput: 118.4 tokens/s, Avg generation throughput: 2233.0 tokens/s, Running: 23 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.7%, Prefix cache hit rate: 85.3%
[36mllm_server_1  |[0m INFO 07-21 18:36:19 [loggers.py:118] Engine 003: Avg prompt throughput: 90.5 tokens/s, Avg generation throughput: 2150.9 tokens/s, Running: 18 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.1%, Prefix cache hit rate: 84.4%

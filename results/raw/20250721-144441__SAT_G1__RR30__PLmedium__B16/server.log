Attaching to vllm_load_test_llm_server_1
[36mllm_server_1  |[0m INFO 07-20 22:55:52 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-20 22:55:57 [api_server.py:1395] vLLM API server version 0.9.2
[36mllm_server_1  |[0m INFO 07-20 22:55:57 [cli_args.py:325] non-default args: {'model': '/llm', 'max_model_len': 32768, 'served_model_name': ['qwen3']}
[36mllm_server_1  |[0m INFO 07-20 22:56:05 [config.py:841] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.
[36mllm_server_1  |[0m INFO 07-20 22:56:05 [config.py:1472] Using max model len 32768
[36mllm_server_1  |[0m INFO 07-20 22:56:06 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36mllm_server_1  |[0m INFO 07-20 22:56:11 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-20 22:56:14 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m INFO 07-20 22:56:14 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m INFO 07-20 22:56:18 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36mllm_server_1  |[0m INFO 07-20 22:56:18 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m INFO 07-20 22:56:18 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m INFO 07-20 22:56:19 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m INFO 07-20 22:56:19 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:06,  1.05it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:05,  1.00it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:03<00:05,  1.02s/it]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.21it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.12it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:05<00:01,  1.06it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:06<00:00,  1.03it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:07<00:00,  1.01it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:07<00:00,  1.04it/s]
[36mllm_server_1  |[0m 
[36mllm_server_1  |[0m INFO 07-20 22:56:27 [default_loader.py:272] Loading weights took 7.73 seconds
[36mllm_server_1  |[0m INFO 07-20 22:56:28 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 7.993788 seconds
[36mllm_server_1  |[0m INFO 07-20 22:56:40 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/33b1081f08/rank_0_0/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m INFO 07-20 22:56:40 [backends.py:519] Dynamo bytecode transform time: 11.77 s
[36mllm_server_1  |[0m INFO 07-20 22:56:46 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m INFO 07-20 22:57:33 [backends.py:193] Compiling a graph for general shape takes 52.18 s
[36mllm_server_1  |[0m INFO 07-20 22:57:56 [monitor.py:34] torch.compile takes 63.95 s in total
[36mllm_server_1  |[0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m   warnings.warn(
[36mllm_server_1  |[0m INFO 07-20 22:57:58 [gpu_worker.py:232] Available KV cache memory: 7.30 GiB
[36mllm_server_1  |[0m INFO 07-20 22:57:58 [kv_cache_utils.py:716] GPU KV cache size: 47,840 tokens
[36mllm_server_1  |[0m INFO 07-20 22:57:58 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.46x
[36mllm_server_1  |[0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|▏         | 1/67 [00:00<00:52,  1.26it/s]Capturing CUDA graph shapes:   3%|▎         | 2/67 [00:02<01:25,  1.31s/it]Capturing CUDA graph shapes:   4%|▍         | 3/67 [00:03<01:02,  1.03it/s]Capturing CUDA graph shapes:   6%|▌         | 4/67 [00:05<01:25,  1.36s/it]Capturing CUDA graph shapes:   7%|▋         | 5/67 [00:05<01:07,  1.08s/it]Capturing CUDA graph shapes:   9%|▉         | 6/67 [00:07<01:23,  1.37s/it]Capturing CUDA graph shapes:  10%|█         | 7/67 [00:08<01:06,  1.11s/it]Capturing CUDA graph shapes:  12%|█▏        | 8/67 [00:09<01:20,  1.36s/it]Capturing CUDA graph shapes:  13%|█▎        | 9/67 [00:10<01:04,  1.11s/it]Capturing CUDA graph shapes:  15%|█▍        | 10/67 [00:12<01:18,  1.37s/it]Capturing CUDA graph shapes:  16%|█▋        | 11/67 [00:13<01:02,  1.12s/it]Capturing CUDA graph shapes:  18%|█▊        | 12/67 [00:13<00:51,  1.07it/s]Capturing CUDA graph shapes:  19%|█▉        | 13/67 [00:14<00:58,  1.08s/it]Capturing CUDA graph shapes:  21%|██        | 14/67 [00:15<00:48,  1.09it/s]Capturing CUDA graph shapes:  22%|██▏       | 15/67 [00:17<01:02,  1.21s/it]Capturing CUDA graph shapes:  24%|██▍       | 16/67 [00:17<00:52,  1.02s/it]Capturing CUDA graph shapes:  25%|██▌       | 17/67 [00:18<00:44,  1.13it/s]Capturing CUDA graph shapes:  27%|██▋       | 18/67 [00:20<00:52,  1.07s/it]Capturing CUDA graph shapes:  28%|██▊       | 19/67 [00:20<00:43,  1.09it/s]Capturing CUDA graph shapes:  30%|██▉       | 20/67 [00:21<00:37,  1.26it/s]Capturing CUDA graph shapes:  31%|███▏      | 21/67 [00:22<00:46,  1.01s/it]Capturing CUDA graph shapes:  33%|███▎      | 22/67 [00:23<00:39,  1.15it/s]Capturing CUDA graph shapes:  34%|███▍      | 23/67 [00:23<00:33,  1.31it/s]Capturing CUDA graph shapes:  36%|███▌      | 24/67 [00:25<00:42,  1.02it/s]Capturing CUDA graph shapes:  37%|███▋      | 25/67 [00:25<00:35,  1.18it/s]Capturing CUDA graph shapes:  39%|███▉      | 26/67 [00:26<00:30,  1.33it/s]Capturing CUDA graph shapes:  40%|████      | 27/67 [00:27<00:39,  1.02it/s]Capturing CUDA graph shapes:  42%|████▏     | 28/67 [00:28<00:33,  1.17it/s]Capturing CUDA graph shapes:  43%|████▎     | 29/67 [00:29<00:39,  1.04s/it]Capturing CUDA graph shapes:  45%|████▍     | 30/67 [00:30<00:37,  1.01s/it]Capturing CUDA graph shapes:  46%|████▋     | 31/67 [00:31<00:30,  1.17it/s]Capturing CUDA graph shapes:  48%|████▊     | 32/67 [00:32<00:38,  1.10s/it]Capturing CUDA graph shapes:  49%|████▉     | 33/67 [00:33<00:31,  1.08it/s]Capturing CUDA graph shapes:  51%|█████     | 34/67 [00:33<00:26,  1.24it/s]Capturing CUDA graph shapes:  52%|█████▏    | 35/67 [00:35<00:32,  1.01s/it]Capturing CUDA graph shapes:  54%|█████▎    | 36/67 [00:35<00:26,  1.15it/s]Capturing CUDA graph shapes:  55%|█████▌    | 37/67 [00:36<00:22,  1.31it/s]Capturing CUDA graph shapes:  57%|█████▋    | 38/67 [00:37<00:28,  1.02it/s]Capturing CUDA graph shapes:  58%|█████▊    | 39/67 [00:38<00:23,  1.19it/s]Capturing CUDA graph shapes:  60%|█████▉    | 40/67 [00:38<00:19,  1.35it/s]Capturing CUDA graph shapes:  61%|██████    | 41/67 [00:40<00:25,  1.03it/s]Capturing CUDA graph shapes:  63%|██████▎   | 42/67 [00:41<00:20,  1.20it/s]Capturing CUDA graph shapes:  64%|██████▍   | 43/67 [00:41<00:17,  1.35it/s]Capturing CUDA graph shapes:  66%|██████▌   | 44/67 [00:43<00:22,  1.03it/s]Capturing CUDA graph shapes:  67%|██████▋   | 45/67 [00:43<00:18,  1.19it/s]Capturing CUDA graph shapes:  69%|██████▊   | 46/67 [00:44<00:15,  1.36it/s]Capturing CUDA graph shapes:  70%|███████   | 47/67 [00:45<00:19,  1.03it/s]Capturing CUDA graph shapes:  72%|███████▏  | 48/67 [00:46<00:15,  1.19it/s]Capturing CUDA graph shapes:  73%|███████▎  | 49/67 [00:46<00:13,  1.34it/s]Capturing CUDA graph shapes:  75%|███████▍  | 50/67 [00:48<00:16,  1.02it/s]Capturing CUDA graph shapes:  76%|███████▌  | 51/67 [00:48<00:13,  1.18it/s]Capturing CUDA graph shapes:  78%|███████▊  | 52/67 [00:49<00:11,  1.34it/s]Capturing CUDA graph shapes:  79%|███████▉  | 53/67 [00:50<00:13,  1.00it/s]Capturing CUDA graph shapes:  81%|████████  | 54/67 [00:51<00:11,  1.17it/s]Capturing CUDA graph shapes:  82%|████████▏ | 55/67 [00:51<00:09,  1.33it/s]Capturing CUDA graph shapes:  84%|████████▎ | 56/67 [00:53<00:10,  1.02it/s]Capturing CUDA graph shapes:  85%|████████▌ | 57/67 [00:53<00:08,  1.19it/s]Capturing CUDA graph shapes:  87%|████████▋ | 58/67 [00:54<00:06,  1.34it/s]Capturing CUDA graph shapes:  88%|████████▊ | 59/67 [00:55<00:07,  1.03it/s]Capturing CUDA graph shapes:  90%|████████▉ | 60/67 [00:56<00:05,  1.20it/s]Capturing CUDA graph shapes:  91%|█████████ | 61/67 [00:56<00:04,  1.37it/s]Capturing CUDA graph shapes:  93%|█████████▎| 62/67 [00:58<00:04,  1.02it/s]Capturing CUDA graph shapes:  94%|█████████▍| 63/67 [00:58<00:03,  1.19it/s]Capturing CUDA graph shapes:  96%|█████████▌| 64/67 [00:59<00:02,  1.35it/s]Capturing CUDA graph shapes:  97%|█████████▋| 65/67 [01:00<00:01,  1.03it/s]Capturing CUDA graph shapes:  99%|█████████▊| 66/67 [01:01<00:00,  1.19it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [01:02<00:00,  1.36it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [01:02<00:00,  1.08it/s]
[36mllm_server_1  |[0m INFO 07-20 22:59:01 [gpu_model_runner.py:2326] Graph capturing finished in 62 secs, took 0.71 GiB
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [core.py:172] init engine (profile, create kv cache, warmup model) took 153.60 seconds
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 2990
[36mllm_server_1  |[0m WARNING 07-20 22:59:02 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:29] Available routes are:
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /docs, Methods: GET, HEAD
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /redoc, Methods: GET, HEAD
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /health, Methods: GET
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /load, Methods: GET
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /ping, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /ping, Methods: GET
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /tokenize, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /detokenize, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /v1/models, Methods: GET
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /version, Methods: GET
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /v1/completions, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /v1/embeddings, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /pooling, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /classify, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /score, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /v1/score, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /v1/audio/translations, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /v1/rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /v2/rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /invocations, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 22:59:02 [launcher.py:37] Route: /metrics, Methods: GET
[36mllm_server_1  |[0m INFO:     Started server process [7]
[36mllm_server_1  |[0m INFO:     Waiting for application startup.
[36mllm_server_1  |[0m INFO:     Application startup complete.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51350 - "GET /health HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:17 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36mllm_server_1  |[0m INFO 07-20 22:59:17 [logger.py:43] Received request chatcmpl-f6173deb9c134c1fa3166b3ea10f32ea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:17 [async_llm.py:270] Added request chatcmpl-f6173deb9c134c1fa3166b3ea10f32ea.
[36mllm_server_1  |[0m INFO 07-20 22:59:22 [loggers.py:118] Engine 000: Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 24.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-6008c1119e034aae9a6660acbd9577b2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-6008c1119e034aae9a6660acbd9577b2.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-75ddc8347fcf45e3be5a3dbb4fdeaa55: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-75ddc8347fcf45e3be5a3dbb4fdeaa55.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-b8b08582cd5c415aa5fad302cb5170ad: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-b8b08582cd5c415aa5fad302cb5170ad.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-4a1a6def60ba4def86fba2459bdd23f6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-4a1a6def60ba4def86fba2459bdd23f6.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-5578b7061e0149c59ff2af2c77797ed2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-5578b7061e0149c59ff2af2c77797ed2.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-42d6224e4a4a45beb6fdcd4cc5d8cdcd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-42d6224e4a4a45beb6fdcd4cc5d8cdcd.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-563fed3b5bae427e8905c4efae7b4b98: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-563fed3b5bae427e8905c4efae7b4b98.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-ebeca6627d5248ef808acf9f7182d4ee: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-ebeca6627d5248ef808acf9f7182d4ee.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-6fd12af665424757886b8ac8d235ddd1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-6fd12af665424757886b8ac8d235ddd1.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-2780efdb6a814009800bd8756c1d46d4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-2780efdb6a814009800bd8756c1d46d4.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-774c53f113ef44b3a8a05dffc6579ef1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-774c53f113ef44b3a8a05dffc6579ef1.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-1e3485303343478589e5ade1db2e64a0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-1e3485303343478589e5ade1db2e64a0.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-6bc436a2e7ff453bb30991fe8d6b8a92: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-6bc436a2e7ff453bb30991fe8d6b8a92.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-ac1b8851a2c94573ba508ccd474a701f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-ac1b8851a2c94573ba508ccd474a701f.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-1ad61f7737b84e70941ccc22600b3791: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-1ad61f7737b84e70941ccc22600b3791.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-7476e66364fd4a3293556a2ff034dac0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-7476e66364fd4a3293556a2ff034dac0.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-bdddaab591ec43eeb24aaf4af6daea63: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-bdddaab591ec43eeb24aaf4af6daea63.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-ae83d0c54dc9416bb64034b1752ac86f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-ae83d0c54dc9416bb64034b1752ac86f.
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [logger.py:43] Received request chatcmpl-7d4b568454b742648cbc1110da452eea: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:23 [async_llm.py:270] Added request chatcmpl-7d4b568454b742648cbc1110da452eea.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-f0527f112b8a40a7a48ce86e181ede88: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-f0527f112b8a40a7a48ce86e181ede88.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-04ad6411f075489cb05974c4280164ab: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35436 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-04ad6411f075489cb05974c4280164ab.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-b75004df09b04de0a6b9f9356ff7bd16: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-b75004df09b04de0a6b9f9356ff7bd16.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-b0ec2dabe13b496184e35d55cf500465: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-b0ec2dabe13b496184e35d55cf500465.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-9a16d211f87f4430b5421ebb708f9e49: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-9a16d211f87f4430b5421ebb708f9e49.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-bdfba4168f01445ca32a345113fe96cb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-bdfba4168f01445ca32a345113fe96cb.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-cda6bc4d37734df0bb5e2e987ecbd545: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-cda6bc4d37734df0bb5e2e987ecbd545.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-af49fd18a6f5404882d3b5b3e5342190: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-af49fd18a6f5404882d3b5b3e5342190.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-50778fbf915e4d10929ad7dc66f1b989: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-50778fbf915e4d10929ad7dc66f1b989.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-939c1fe41f844abeb9133010b11a62d1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-939c1fe41f844abeb9133010b11a62d1.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-410359b0f9d24be283af1ec2309b1cd6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-410359b0f9d24be283af1ec2309b1cd6.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-e3446e465ae94edb95a298f46fb596e1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-e3446e465ae94edb95a298f46fb596e1.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-c6127caa9df04605ad1837c3135205f2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-c6127caa9df04605ad1837c3135205f2.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-87f4e02ad6c94ebd9dcec37d16d1f794: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-87f4e02ad6c94ebd9dcec37d16d1f794.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-8c9a0bddce0b43519e2b54287b408855: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-8c9a0bddce0b43519e2b54287b408855.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-0b5c51dcda514364aca1f7324afeef96: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-0b5c51dcda514364aca1f7324afeef96.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-6d555c4989cd4fa080300f5fa665f441: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-6d555c4989cd4fa080300f5fa665f441.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-38fdbca64efc418db281e86ca7de1ed5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-38fdbca64efc418db281e86ca7de1ed5.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-fe064481f2ae4df3be79d6d44b6ceea6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-fe064481f2ae4df3be79d6d44b6ceea6.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-692eead1096b40d29570eb9e8ae52f62: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-692eead1096b40d29570eb9e8ae52f62.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-a7dff400f6ff4450b5489fbbd53a79e3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-a7dff400f6ff4450b5489fbbd53a79e3.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-23e3a555848b4baa8595bfd17ea9b098: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35616 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-23e3a555848b4baa8595bfd17ea9b098.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-14755aef177048bd8f6875be1b9a8171: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-14755aef177048bd8f6875be1b9a8171.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-1b37caaf974c4a3d9c25965e5666d824: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-1b37caaf974c4a3d9c25965e5666d824.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-12d76344f5e9489eb88222feac0d6f9c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-12d76344f5e9489eb88222feac0d6f9c.
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [logger.py:43] Received request chatcmpl-ad762fc7f6e6495fad99f6025c899955: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:24 [async_llm.py:270] Added request chatcmpl-ad762fc7f6e6495fad99f6025c899955.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-697ce1aa70794161a162790b40fd8e79: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-697ce1aa70794161a162790b40fd8e79.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-b2b0eaaeaa7e4077bbe83fbbf4f2ef36: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-b2b0eaaeaa7e4077bbe83fbbf4f2ef36.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-fc26b0a55e944e279af649de555876cb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-fc26b0a55e944e279af649de555876cb.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-8422a474ca0b4c5a84247d007f54ba20: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-8422a474ca0b4c5a84247d007f54ba20.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-1dd27c5f1c6442458a605bbcb0eb6926: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-1dd27c5f1c6442458a605bbcb0eb6926.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-905106a1b63f4ffe9c5fb807f54c02af: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-905106a1b63f4ffe9c5fb807f54c02af.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-abca1a10f1854b1295583f137adc98e5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-abca1a10f1854b1295583f137adc98e5.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-0cdc77cd1c0f458686ff0e8fd898c1e4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-0cdc77cd1c0f458686ff0e8fd898c1e4.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-8f676ac184ef455e93dbdb09d0dca89f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-8f676ac184ef455e93dbdb09d0dca89f.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-3091685dd3744a3ea499f6e30f5b6898: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-3091685dd3744a3ea499f6e30f5b6898.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-f46c4167bdb44f4e97158d2e2806605f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-f46c4167bdb44f4e97158d2e2806605f.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-bb09a62e966748a9aaa70b449105ec93: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-bb09a62e966748a9aaa70b449105ec93.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-fb61319eb1914a8fac7f6a47f1baabb5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35778 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-fb61319eb1914a8fac7f6a47f1baabb5.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-0998d66b0a7647a592cb257b780762eb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-0998d66b0a7647a592cb257b780762eb.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-4748992cb0ed4cbb9322e9582a102fbf: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-4748992cb0ed4cbb9322e9582a102fbf.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-6d43bbbe31ac481cb1ffca1134a27f60: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-6d43bbbe31ac481cb1ffca1134a27f60.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-4d1c3facb12f4ecd8ad56daa513754e7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-4d1c3facb12f4ecd8ad56daa513754e7.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-7a1022e39d8643868d54bdb0f52b0bde: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-7a1022e39d8643868d54bdb0f52b0bde.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-e4782b73dee54ac7adde7c5955887a2e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-e4782b73dee54ac7adde7c5955887a2e.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-e2903bea64184d8a860a46b5278aaf98: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-e2903bea64184d8a860a46b5278aaf98.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-9a50927647ce4c49861294b5327177e8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-9a50927647ce4c49861294b5327177e8.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-2ba9d575f9244d1ca398acff910e78f8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-2ba9d575f9244d1ca398acff910e78f8.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-6b8a26e29feb483aa0e54fb15f5320e0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-6b8a26e29feb483aa0e54fb15f5320e0.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-8596a275440942eca58558315230b1ee: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-8596a275440942eca58558315230b1ee.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-b1d0cfa8c0294e8e911d0ea7b60b4a6d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-b1d0cfa8c0294e8e911d0ea7b60b4a6d.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-b07c5bcc1cb14fc792a552d51fc7f8b1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-b07c5bcc1cb14fc792a552d51fc7f8b1.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-10415b91075c4912b80bd6ea9023ea1e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-10415b91075c4912b80bd6ea9023ea1e.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-0a19afce88004f2390351b3cb381db19: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-0a19afce88004f2390351b3cb381db19.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-8d18ba876ded4aac8d3099b7c6c1be0a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35900 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-8d18ba876ded4aac8d3099b7c6c1be0a.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-85847e9775e14c5c87662c8317189bd8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-85847e9775e14c5c87662c8317189bd8.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-1f64c0beefc94314af4f8e733eb48089: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-1f64c0beefc94314af4f8e733eb48089.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-5c102ce62c8f48bfa260219f849dd229: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-5c102ce62c8f48bfa260219f849dd229.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-2b0dfb83397f44689bec1f28f433c004: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-2b0dfb83397f44689bec1f28f433c004.
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [logger.py:43] Received request chatcmpl-41b466acac5648faa2db374a307aff05: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:25 [async_llm.py:270] Added request chatcmpl-41b466acac5648faa2db374a307aff05.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-c6da638304c1434484b68ae22b4b9895: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-c6da638304c1434484b68ae22b4b9895.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-91508bdf8d96423fab49a16c6828594c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-91508bdf8d96423fab49a16c6828594c.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-ced04756959f43a6997b96e527eb938b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-ced04756959f43a6997b96e527eb938b.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-9c04caf190d540fcba21371d64e56a4a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-9c04caf190d540fcba21371d64e56a4a.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-b5a7283268f540c8bfe83ffa9dc3e407: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:35990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-b5a7283268f540c8bfe83ffa9dc3e407.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-42b1bc00dd0d4e22b3d8abd258deabbd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-417a4f21dc0f4c18b54cb2bd2f3cf752: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-42b1bc00dd0d4e22b3d8abd258deabbd.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-bdfc80e9e591411bae975dc976a02a67: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-417a4f21dc0f4c18b54cb2bd2f3cf752.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-626dc3b91ad1469481ba0421e88e32e2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-bdfc80e9e591411bae975dc976a02a67.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-10be9b494419425981b1c85d2974413c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-626dc3b91ad1469481ba0421e88e32e2.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-10be9b494419425981b1c85d2974413c.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-f6055457fea44e7395972ca4151ffc37: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-f6055457fea44e7395972ca4151ffc37.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-ba458fd8dc534f74849c2b55ddd25da3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-ba458fd8dc534f74849c2b55ddd25da3.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-dd935c41a77e415ba43f360e8d00f0bb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-dd935c41a77e415ba43f360e8d00f0bb.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-9ebd6dd1234448d29ece94bbe8edf8bd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-9ebd6dd1234448d29ece94bbe8edf8bd.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-d2c099391b734d77b3a260ce48eca09d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36088 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-d2c099391b734d77b3a260ce48eca09d.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-5a343b169bed46a58353e4e6e94fc551: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-5a343b169bed46a58353e4e6e94fc551.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-99a0e6cbf9a04869ba2507e53e0feee7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-99a0e6cbf9a04869ba2507e53e0feee7.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-a019b9ab91aa479e9d7da1bd42809dd4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-a019b9ab91aa479e9d7da1bd42809dd4.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-4ba2dc44417b40b58783c5ca5c0e379e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-4ba2dc44417b40b58783c5ca5c0e379e.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-2e3aac48683040b0bd7794557359b33f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-2e3aac48683040b0bd7794557359b33f.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-351eb6e24f5f403783ecfc019e225aa6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36144 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-351eb6e24f5f403783ecfc019e225aa6.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-6f6e5fd7263748cfb26b8ed2cb9c7798: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36148 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-6f6e5fd7263748cfb26b8ed2cb9c7798.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-e1322fd430604f5bb3045770894a3cea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-e1322fd430604f5bb3045770894a3cea.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-0e44dfc090af4dd6a0b244f3e355d508: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-0e44dfc090af4dd6a0b244f3e355d508.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-bec52fbc94e0450babaa226d97c8fecc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-bec52fbc94e0450babaa226d97c8fecc.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-71d723b1fb1f40e1b0e7783b1344cf62: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-71d723b1fb1f40e1b0e7783b1344cf62.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-91ea3717d0f74731b104d7100cca5246: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-91ea3717d0f74731b104d7100cca5246.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-ddccffbec9c74bfaa4525bf7d53e2ef4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-ddccffbec9c74bfaa4525bf7d53e2ef4.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-95b8eec037cd4133b0eb73af320c6a54: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-95b8eec037cd4133b0eb73af320c6a54.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-0dc96b3920d441229b9eca247f805a2e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-0dc96b3920d441229b9eca247f805a2e.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-8268eb006e9346258e2f1b0a01082bc2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-8268eb006e9346258e2f1b0a01082bc2.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-9bce7782aeed449392dd22817dd7199e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-9bce7782aeed449392dd22817dd7199e.
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [logger.py:43] Received request chatcmpl-42a4e8a04995445bb745f7f9aeb3a838: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:26 [async_llm.py:270] Added request chatcmpl-42a4e8a04995445bb745f7f9aeb3a838.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-b3ce77d4eed949168f7abb9bb210b5a4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-b3ce77d4eed949168f7abb9bb210b5a4.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-3f4db7fe195543268c7245da149fc54e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36260 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-3f4db7fe195543268c7245da149fc54e.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-e501d8f7737c4e4dae2bab4fa00d0915: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-e501d8f7737c4e4dae2bab4fa00d0915.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-4c36a690279a429096223d311a247bfa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-4c36a690279a429096223d311a247bfa.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-f94cc1b5982241cfbb2661661c8abdf5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-f94cc1b5982241cfbb2661661c8abdf5.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-5fae61b48bfd41c9b9a769daa7fe14ee: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-5fae61b48bfd41c9b9a769daa7fe14ee.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-7330c2fb55cc4b0a9af4d947faef212e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-7330c2fb55cc4b0a9af4d947faef212e.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-f4149a6e5f70498f884cedda9675ba1b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-f4149a6e5f70498f884cedda9675ba1b.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-ceb0d017dcf442f082abae6f7e1fd327: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-ceb0d017dcf442f082abae6f7e1fd327.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-ed0fc31c49d545888e552e539c827179: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36314 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-ed0fc31c49d545888e552e539c827179.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-3cece2de23284895aeff3952dd3bbf7c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-3cece2de23284895aeff3952dd3bbf7c.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-8bed0e94c32f43468735be069cd0d581: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-8bed0e94c32f43468735be069cd0d581.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-40165021dcf14e7cbd902d09fa471d6c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-40165021dcf14e7cbd902d09fa471d6c.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-e438e41e18124bc6b70aee8658192aa3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-e438e41e18124bc6b70aee8658192aa3.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-f9869a4770a648bb803e456f451fb9d4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-ae72c39393a149fda8c811b1cae95c27: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-f82ad3d10b934169b2ccf6d63a8aa53a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-f9869a4770a648bb803e456f451fb9d4.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-ae72c39393a149fda8c811b1cae95c27.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-f82ad3d10b934169b2ccf6d63a8aa53a.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-69931db0db9e43a4b4f23068f64b20f5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-69931db0db9e43a4b4f23068f64b20f5.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-14763a88b2844857bdc92a815f60d610: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-14763a88b2844857bdc92a815f60d610.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-81d49bb7c9bd40ebb8293efe09e483f7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36418 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-81d49bb7c9bd40ebb8293efe09e483f7.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-321e8cea2dab4bdd8a9e2620c436f72a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-e6c14157a94f41458fce7b279284251e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-321e8cea2dab4bdd8a9e2620c436f72a.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-e6c14157a94f41458fce7b279284251e.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-37f968e3c4974478924c5388368c5a90: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-37f968e3c4974478924c5388368c5a90.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-16cabc3f758b42bb952b64bbcb67d2c2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-16cabc3f758b42bb952b64bbcb67d2c2.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-ba68baa1d421477fb789ff3e1e970e64: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-ba68baa1d421477fb789ff3e1e970e64.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-cc30b1bd0a3d4291b76d8ba9b1a9bc09: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-cc30b1bd0a3d4291b76d8ba9b1a9bc09.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-253c94cd4077456689e88d00fbb8be19: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-253c94cd4077456689e88d00fbb8be19.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-53d273b96aca42fba15c3710f3f36f76: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-53d273b96aca42fba15c3710f3f36f76.
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [logger.py:43] Received request chatcmpl-3852369b68df4c84af15fe471000e16a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:27 [async_llm.py:270] Added request chatcmpl-3852369b68df4c84af15fe471000e16a.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-628ae61f7dae4fb7b65af3f7a7f1bb88: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-628ae61f7dae4fb7b65af3f7a7f1bb88.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-953554049bf34aac83affeefbcd29ada: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-953554049bf34aac83affeefbcd29ada.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-89124ed0d7d3463fa8f240b9778494d6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-89124ed0d7d3463fa8f240b9778494d6.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-45f8c8e47e574c11b1b9a1d079613476: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-45f8c8e47e574c11b1b9a1d079613476.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-569cff2db1d942dfa1884f561b783076: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-569cff2db1d942dfa1884f561b783076.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-412884eb052c44ecac91924803af181e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-412884eb052c44ecac91924803af181e.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-76196ba813a548fa8f09e0da9f2a5e1c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-76196ba813a548fa8f09e0da9f2a5e1c.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-38e878d150a0418bbe361be95d85633b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-38e878d150a0418bbe361be95d85633b.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-599613fb21d54ab893811eb2e2cebe4d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-599613fb21d54ab893811eb2e2cebe4d.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-ab1044fe22ea4ec2b0b44d75c185dd16: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-ab1044fe22ea4ec2b0b44d75c185dd16.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-dc1babaed4df4cf893669f8981118a13: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-dc1babaed4df4cf893669f8981118a13.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-f8c7612db03346aca4166059fa79bdc9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-b904d090221b459bbfb4181d5a2598b6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-f8c7612db03346aca4166059fa79bdc9.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-b904d090221b459bbfb4181d5a2598b6.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-17408e8a8541436ab7b4a164c77a7371: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-17408e8a8541436ab7b4a164c77a7371.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-07f7c702d7524bbd8495eaac32108099: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36642 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-07f7c702d7524bbd8495eaac32108099.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-4fb8a195052e4c6399751573a67c63d6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-4fb8a195052e4c6399751573a67c63d6.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-20d3491d5866467caac833b14ca0dcaa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36656 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-20d3491d5866467caac833b14ca0dcaa.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-484cdcf060744d4a986b75820ef9b72e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-484cdcf060744d4a986b75820ef9b72e.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-0d713985bf8f4f1197d491b46157cf8a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-0d713985bf8f4f1197d491b46157cf8a.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-90d3ad9b06174e77a77c3d97cd6df507: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-90d3ad9b06174e77a77c3d97cd6df507.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-5e322b5c41bc431b93a64694e5835500: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-5e322b5c41bc431b93a64694e5835500.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-6782041d18d547e6b45811736a2a811b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-6782041d18d547e6b45811736a2a811b.
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [logger.py:43] Received request chatcmpl-ea4e01a2e3284722a7698fac30f0dec8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:28 [async_llm.py:270] Added request chatcmpl-ea4e01a2e3284722a7698fac30f0dec8.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-795edf5e131c42c78ecb06c1f0e6727a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-795edf5e131c42c78ecb06c1f0e6727a.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-14512015848d44ac879a554b917ec578: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-14512015848d44ac879a554b917ec578.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-6cbfee90e2ad48898de7eb1f58fe7931: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-6cbfee90e2ad48898de7eb1f58fe7931.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-db7c28c9ceb84b408f3683d2f18fcc4e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-db7c28c9ceb84b408f3683d2f18fcc4e.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-af0d8967b6bd4f8597331f5b887521db: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-af0d8967b6bd4f8597331f5b887521db.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-9f9ca88f02e74592b48c9eeee4d935fc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-90cf01c0062647d0878579805417d29e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-9f9ca88f02e74592b48c9eeee4d935fc.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-90cf01c0062647d0878579805417d29e.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-efe574ddf0a241029194b7612dfa5352: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-ebfa97e69a0a4c929f502756be270ab0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-a40707c2eac34fd4aaeabf0c602133ac: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-efe574ddf0a241029194b7612dfa5352.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-ebfa97e69a0a4c929f502756be270ab0.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-a40707c2eac34fd4aaeabf0c602133ac.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-4aa7bda18bd3427a880b072e70252d38: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36830 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-4aa7bda18bd3427a880b072e70252d38.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-f08f4cad1e47413ca20d177c80123837: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-f08f4cad1e47413ca20d177c80123837.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-5b174bbae0ce4b799954a67fd9b4be32: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-5b174bbae0ce4b799954a67fd9b4be32.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-559d84904c9648c3b82cc2be588dcfb9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-559d84904c9648c3b82cc2be588dcfb9.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-ed16a58ae6c048899af128b97c1ff1d4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-ed16a58ae6c048899af128b97c1ff1d4.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-f998ce1d04534d2cbe22c64451f7bfed: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-f4d4b7b0dcf04e85a37c2223b9ece2ab: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-f998ce1d04534d2cbe22c64451f7bfed.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-f4d4b7b0dcf04e85a37c2223b9ece2ab.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-a702698bedc94a76983d71bcf1eae116: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-a702698bedc94a76983d71bcf1eae116.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-0830aa3b849845a5913e699f6037ff19: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-0830aa3b849845a5913e699f6037ff19.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-85f2bfefc7574b5da3bc4e20938cf6c4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-85f2bfefc7574b5da3bc4e20938cf6c4.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-52746f8221f6475db8f53d2af44fddbf: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-52746f8221f6475db8f53d2af44fddbf.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-c562bc1045bb4b7caadf940af9eea3d2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-c562bc1045bb4b7caadf940af9eea3d2.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-b5f1a8148c414ab4a094133e6f0bbb81: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-b5f1a8148c414ab4a094133e6f0bbb81.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-973e58fdb0ed49dda022297116830d24: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-973e58fdb0ed49dda022297116830d24.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-6fa1131a60ba48638d545271f7fde656: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-6fa1131a60ba48638d545271f7fde656.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-887e74cca4414ecebe538365e22c4015: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:36994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-887e74cca4414ecebe538365e22c4015.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-e8a2df83ab4a42338b4163e42df8fd03: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-e8a2df83ab4a42338b4163e42df8fd03.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-b2e8c194408b4ac98bfa64410e7f6a01: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-b2e8c194408b4ac98bfa64410e7f6a01.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-b276549665114199aed2bafe1f80bbe9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-b276549665114199aed2bafe1f80bbe9.
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [logger.py:43] Received request chatcmpl-9dbd8880e9084d6dba80c17f02658c74: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:29 [async_llm.py:270] Added request chatcmpl-9dbd8880e9084d6dba80c17f02658c74.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-da3436c2a9454617a0ea9d0f377dfdbb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-da3436c2a9454617a0ea9d0f377dfdbb.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-35fd2587b1f8434d8580511021403b9e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-35fd2587b1f8434d8580511021403b9e.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-5f9fc5b473e24e828b7432864ceeb11b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-5f9fc5b473e24e828b7432864ceeb11b.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-4704798211674f8faedd1e16de2e1d2b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-4704798211674f8faedd1e16de2e1d2b.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-6ba969a25b3040e982f93aaace3fcd23: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-8a381cee895444a1a48ad895d1093f57: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-6ba969a25b3040e982f93aaace3fcd23.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-8a381cee895444a1a48ad895d1093f57.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-f06968aa3bfb48c2a7119f9d0a288595: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-f06968aa3bfb48c2a7119f9d0a288595.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-ed09258c3bc74f199b092abd76f68093: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-ed09258c3bc74f199b092abd76f68093.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-0d6960917b0b4723b693092791d7cf99: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-0d6960917b0b4723b693092791d7cf99.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-cbea211b0a524b8aa7423189d57ddc02: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-cbea211b0a524b8aa7423189d57ddc02.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-41ceac704feb4388b720fe25940c0594: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-41ceac704feb4388b720fe25940c0594.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-694ada1fc8d24b088845f24ca596c0d7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-035aa2692dfa483f997c943f2d453089: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37144 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-694ada1fc8d24b088845f24ca596c0d7.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-035aa2692dfa483f997c943f2d453089.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-e3abe04eae44441b99205f1d2a2ade29: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-e3abe04eae44441b99205f1d2a2ade29.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-9ef2b8296e16406b9afd01fef15eb730: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-9ef2b8296e16406b9afd01fef15eb730.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-38b50adf472f4aceba6a990935ef7707: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-38b50adf472f4aceba6a990935ef7707.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-b56a5358b6c744f8bd12309f30fe90b5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-b56a5358b6c744f8bd12309f30fe90b5.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-8ef28fcaf3174961b7119d609d90e392: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-8ef28fcaf3174961b7119d609d90e392.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-9deb79ec4ab44bc991874625c3f97ba8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-9deb79ec4ab44bc991874625c3f97ba8.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-a29f2e2f34ee47debf4aa3210aadcb83: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-164df01365124df598cf7f95076bb351: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-bf2bf9c16d114d26a25a587619a90ac2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-a29f2e2f34ee47debf4aa3210aadcb83.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-164df01365124df598cf7f95076bb351.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-bf2bf9c16d114d26a25a587619a90ac2.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-ffbd03a340a44db1ac2f3e4e0cc067fc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-ffbd03a340a44db1ac2f3e4e0cc067fc.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-1db348e799f341da82ba6b09b2462528: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-1db348e799f341da82ba6b09b2462528.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-b832435a09444ff38d0144c3bc7360ce: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-b832435a09444ff38d0144c3bc7360ce.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-ebfeaf88748a48969786d96960546d80: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-ebfeaf88748a48969786d96960546d80.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-0dc83ea539c5438eb9df986a53865fb3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-0dc83ea539c5438eb9df986a53865fb3.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-a310b453bb774cef8fd90c460a8d4d28: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-a310b453bb774cef8fd90c460a8d4d28.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-927e378d712e466a845d8f69d2ac55a9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-927e378d712e466a845d8f69d2ac55a9.
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [logger.py:43] Received request chatcmpl-e71bebe9fd2f411685d8b8facc8665c2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:30 [async_llm.py:270] Added request chatcmpl-e71bebe9fd2f411685d8b8facc8665c2.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-719e8d59a1b94ad2aafd8b9d1832d155: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-719e8d59a1b94ad2aafd8b9d1832d155.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-7567b77a123241d0bae6220e3950c685: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37312 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-7567b77a123241d0bae6220e3950c685.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-08d4fd3ff525452c81779dc4ce71cc29: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-08d4fd3ff525452c81779dc4ce71cc29.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-d9c17cac6ca842b1812e7cb9c98ff127: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-d9c17cac6ca842b1812e7cb9c98ff127.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-268dd60b91c04cf3ab820586a6a140df: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-268dd60b91c04cf3ab820586a6a140df.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-dfc7dd5ceb8d4b948a6b9878e966b8ff: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-dfc7dd5ceb8d4b948a6b9878e966b8ff.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-dd89e8f9427745d08f762a7de1359858: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-dd89e8f9427745d08f762a7de1359858.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-4b5006e35cd0428bb8d7c67896ece462: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-4b5006e35cd0428bb8d7c67896ece462.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-9b9fce7c893040259aa42bf55d16e4ce: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-9b9fce7c893040259aa42bf55d16e4ce.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-0818a666665d4ed8a8b11f3d948c2fc4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-0818a666665d4ed8a8b11f3d948c2fc4.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-f03785e326f7481ca64eff84f12096cc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-f03785e326f7481ca64eff84f12096cc.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-3e557585e7944edba3cf8c979f474662: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-3e557585e7944edba3cf8c979f474662.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-d24b649e24344849bd0cbc6095223a85: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-996aa98eaefd4ce68f1f0a561b2b14b7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-d24b649e24344849bd0cbc6095223a85.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-996aa98eaefd4ce68f1f0a561b2b14b7.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-f631d9fe74734e33afb8ac67c0319fc9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-f631d9fe74734e33afb8ac67c0319fc9.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-dddb05c5c0434cb083d90ad12da7867f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-dddb05c5c0434cb083d90ad12da7867f.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-ee54b50786bf4dd591d813069e12846d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-ee54b50786bf4dd591d813069e12846d.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-e9f450748220471684fa5ff2783cbd49: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-e9f450748220471684fa5ff2783cbd49.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-9a7d62badd3a4440aae69dc0e331695d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-9a7d62badd3a4440aae69dc0e331695d.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-e59a95cfca074ccbb93c24d1cc19355a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-e59a95cfca074ccbb93c24d1cc19355a.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-f1c0a5714cdf48448eadb2f33563be2f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-f1c0a5714cdf48448eadb2f33563be2f.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-0996caaf6db547d08a9f57c8bbfbf783: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-0996caaf6db547d08a9f57c8bbfbf783.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-c38a0cb150f34081bf4309177ec94605: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-c38a0cb150f34081bf4309177ec94605.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-9f1d21023b084565b05d42a256fb025c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-9f1d21023b084565b05d42a256fb025c.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-7b09da4af58241e1bf1bcde36e05577a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-7b09da4af58241e1bf1bcde36e05577a.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-796104cc5baf401e9a1b237ee1bdccd5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-796104cc5baf401e9a1b237ee1bdccd5.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-0b399c33739f47519dbd49ad6e81ba23: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37536 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-0b399c33739f47519dbd49ad6e81ba23.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-deb0fcd1d66841bab13d4cc8a9cbd78e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-deb0fcd1d66841bab13d4cc8a9cbd78e.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-07586185fd3f4c49bbcce5f6ecaf151d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-07586185fd3f4c49bbcce5f6ecaf151d.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-678e6aea872f4092bdea96b05a9bd55b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-678e6aea872f4092bdea96b05a9bd55b.
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [logger.py:43] Received request chatcmpl-b32c27ffab6745bc8afbad541d5fbcc6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:31 [async_llm.py:270] Added request chatcmpl-b32c27ffab6745bc8afbad541d5fbcc6.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-17046ad9ad534980a6d92193a6c3ea04: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-17046ad9ad534980a6d92193a6c3ea04.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-1fe768ddee2b4eb087a33e45b79b0628: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-1fe768ddee2b4eb087a33e45b79b0628.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-2c406f0692cd440fab992fc979463545: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-2c406f0692cd440fab992fc979463545.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-aa0d2816d44549869a786af291c9600e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37616 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-aa0d2816d44549869a786af291c9600e.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-ad88bf6bf93a4c05a4df9a2d084635b7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-ad88bf6bf93a4c05a4df9a2d084635b7.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-4acbd1f187874046b3d7fb36186b14b8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-4acbd1f187874046b3d7fb36186b14b8.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-9496a14005f54fbb9e2b9c15e0b78107: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-9496a14005f54fbb9e2b9c15e0b78107.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-546bb5047cfe4d1aa0a0967ced2f313d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-97a1379852f84478a55dc2a16a0fa9c7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-546bb5047cfe4d1aa0a0967ced2f313d.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-97a1379852f84478a55dc2a16a0fa9c7.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-3e4d844f19904bfabb6cc976c6352871: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-3e4d844f19904bfabb6cc976c6352871.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-12793c2e823247bf8bb1bc4aa1397231: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-12793c2e823247bf8bb1bc4aa1397231.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-b9507e0507d14e03b5cf903145975ab9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-b9507e0507d14e03b5cf903145975ab9.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-241beb7e0b7d41fdacf9373ccc83cdee: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-241beb7e0b7d41fdacf9373ccc83cdee.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-5c2b3b850eda4fd9b493a1d888d39ff0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-5c2b3b850eda4fd9b493a1d888d39ff0.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-b83460e0d2d24ed9ad68e0052d512d24: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-b83460e0d2d24ed9ad68e0052d512d24.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-78a91a397db14a469f7cd59efcbad2b2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-78a91a397db14a469f7cd59efcbad2b2.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-892e001b4821477daab07f55a5cfc234: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-892e001b4821477daab07f55a5cfc234.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-02a5974b4dc846f49858aaa1d8acbbe5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-02a5974b4dc846f49858aaa1d8acbbe5.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-e5b90a065027458ea628e177c77e4861: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-e5b90a065027458ea628e177c77e4861.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-f609df02c8194d88a5260ff06d7d5df6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-f609df02c8194d88a5260ff06d7d5df6.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-04661c4f487746d0afb5eced3f24f86c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:37752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-04661c4f487746d0afb5eced3f24f86c.
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [logger.py:43] Received request chatcmpl-5eba2f75f2d34af6a5666c04c8b401e4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:32 [async_llm.py:270] Added request chatcmpl-5eba2f75f2d34af6a5666c04c8b401e4.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [loggers.py:118] Engine 000: Avg prompt throughput: 1327.6 tokens/s, Avg generation throughput: 2520.5 tokens/s, Running: 256 reqs, Waiting: 20 reqs, GPU KV cache usage: 60.7%, Prefix cache hit rate: 84.6%
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-933846e67ad94cb6b6f01ac3f0faaea7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-933846e67ad94cb6b6f01ac3f0faaea7.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-b4d0208caf1546959d80389d01cbaa4a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-b4d0208caf1546959d80389d01cbaa4a.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-e4d050532f2b43e39f123692dadfb6c4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-e4d050532f2b43e39f123692dadfb6c4.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-712a90fb87d84674a10d0d439b68e56e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-712a90fb87d84674a10d0d439b68e56e.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-acf33bdcd3ac44d6870f548d9638d98e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-acf33bdcd3ac44d6870f548d9638d98e.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-5a2811f1e9374f3f9ce34ccdd8332ca4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-5a2811f1e9374f3f9ce34ccdd8332ca4.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-7337520444c64b9696b12e4ce7aa9f61: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-b4d608651aa0445099c9f174bfbab96a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-7337520444c64b9696b12e4ce7aa9f61.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-b4d608651aa0445099c9f174bfbab96a.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-2c442611203c48988e2537dd8c15af58: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-2c442611203c48988e2537dd8c15af58.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-b2c0f4cc3dd64da6936ed7576157890f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-b2c0f4cc3dd64da6936ed7576157890f.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-1bc5e3d5379b46a9883a4875958d6734: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-1bc5e3d5379b46a9883a4875958d6734.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-9b0cbf1310ec4693b8a566ce397f4970: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-9b0cbf1310ec4693b8a566ce397f4970.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-290936bf661c439d90f9afaf9ea9429e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-290936bf661c439d90f9afaf9ea9429e.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-713fad52a7904ee3a872627dfce1647e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-713fad52a7904ee3a872627dfce1647e.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-37381462a1ce4702be79bf8de0132e54: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-37381462a1ce4702be79bf8de0132e54.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-d54ad1ea93d8436bb7f89a35e37c6fc2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-d54ad1ea93d8436bb7f89a35e37c6fc2.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-2d10fd1824d44b91b86be0135768a0f1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-2d10fd1824d44b91b86be0135768a0f1.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-73e886d8afa445859c8e490c2446fbdc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-73e886d8afa445859c8e490c2446fbdc.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-8be255dad1564ed080d7d7fdd2ca49c2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52908 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-8be255dad1564ed080d7d7fdd2ca49c2.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-50cd3c953eb44c579e6c7272a6247812: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-50cd3c953eb44c579e6c7272a6247812.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-568dcece8e3d4f93a32ac8a7f98e008c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-568dcece8e3d4f93a32ac8a7f98e008c.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-71c7fef16be04c9c864e0d78081425f6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-71c7fef16be04c9c864e0d78081425f6.
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [logger.py:43] Received request chatcmpl-cb42b46008754c8f80b839e7ffa70a56: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:33 [async_llm.py:270] Added request chatcmpl-cb42b46008754c8f80b839e7ffa70a56.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-a880ec5132fe442f961a4f75c9958733: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-a880ec5132fe442f961a4f75c9958733.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-70df6f74e1264aeb978ad5317a971445: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-70df6f74e1264aeb978ad5317a971445.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-c2dbb6ac741a4a6e8bcdcbc2e6dc2f48: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:52994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-c2dbb6ac741a4a6e8bcdcbc2e6dc2f48.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-4e167fb693ba40faa6c132b66e24f7a1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-4e167fb693ba40faa6c132b66e24f7a1.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-b9df2d908a1a462d901c0d1c38511271: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-b9df2d908a1a462d901c0d1c38511271.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-69dc36960f1a423697a00b220506aa54: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-69dc36960f1a423697a00b220506aa54.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-a568296ce76d4814acf79d73ad39ac63: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-a568296ce76d4814acf79d73ad39ac63.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-2429e293d79d4a23821fbf3edd49b102: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-2429e293d79d4a23821fbf3edd49b102.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-8e7d830b67474aaaaa6d9b0548e9071c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-8e7d830b67474aaaaa6d9b0548e9071c.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-1252e806ea244f7c950eed1cd1506a8c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-1252e806ea244f7c950eed1cd1506a8c.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-46bdb7606d1241f2a7a2ba17a7f9c49f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-46bdb7606d1241f2a7a2ba17a7f9c49f.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-2cdf6157bf0646129b928a4fe13ec254: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-2cdf6157bf0646129b928a4fe13ec254.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-794dd144231d46d28eec8f6659809465: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-794dd144231d46d28eec8f6659809465.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-040ed16d33db4bca91ba2ea3409e00a4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-040ed16d33db4bca91ba2ea3409e00a4.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-7eb344afa7814a01908ffe4b5b9a19ab: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-7eb344afa7814a01908ffe4b5b9a19ab.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-21beefaa5fe24afb8230e657710554d4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-21beefaa5fe24afb8230e657710554d4.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-f1434dfdd74242389c0142b7df8475d3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-f1434dfdd74242389c0142b7df8475d3.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-44dceb2b4db0403faf1cbc5a905b0b5b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-44dceb2b4db0403faf1cbc5a905b0b5b.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-db3989d60d534fe0b97955fb7fa76c95: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-db3989d60d534fe0b97955fb7fa76c95.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-10ebe468f9c9477997f8dfb184f7ed25: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-10ebe468f9c9477997f8dfb184f7ed25.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-35ced319c5c4458d9a2194959d672dcc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-35ced319c5c4458d9a2194959d672dcc.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-ad8396c5949b4957a2fbaf0a54defb6b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-ad8396c5949b4957a2fbaf0a54defb6b.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-e5ab15abf968499386fba76d78fc01d2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-e5ab15abf968499386fba76d78fc01d2.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-722fe2a5f2ff4fb895da09953be0ed0f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-722fe2a5f2ff4fb895da09953be0ed0f.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-c25a2df0e9ae4538a1225f30fd423ba9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-c25a2df0e9ae4538a1225f30fd423ba9.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-43d37743650246ccbb5b02538d9080dc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-43d37743650246ccbb5b02538d9080dc.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-7b46ca89914b409a92401dc76b248558: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-7b46ca89914b409a92401dc76b248558.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-d9437dd5ba40499e90dc37b70d5657fc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-d9437dd5ba40499e90dc37b70d5657fc.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-bd79659344944c61b5ecb1a68be1ead7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-bd79659344944c61b5ecb1a68be1ead7.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-cbdb1cc216ca427c8b0f956fd665490b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-cbdb1cc216ca427c8b0f956fd665490b.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-83b84fb3de9046269699c4e212e969bd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-83b84fb3de9046269699c4e212e969bd.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-0ca6708129d4439197cfebb560080a32: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-0ca6708129d4439197cfebb560080a32.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-d4cf19730b534debb6a6687d30b808b2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-d4cf19730b534debb6a6687d30b808b2.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-32a621b3d32643e5877d8b0e2c333d44: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-32a621b3d32643e5877d8b0e2c333d44.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-1ea04f57f3d941d58d60e500cffe3b5d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-1ea04f57f3d941d58d60e500cffe3b5d.
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [logger.py:43] Received request chatcmpl-49fa47686cda415aaf43b770bd9b6c49: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:34 [async_llm.py:270] Added request chatcmpl-49fa47686cda415aaf43b770bd9b6c49.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-3437daaa738d42b2bb1e038f19c8d4a2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-3437daaa738d42b2bb1e038f19c8d4a2.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-83f920bbdb54400f8425ea1e48957650: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-83f920bbdb54400f8425ea1e48957650.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-b90c67d537984cb09e2d4d3d6e3f8d80: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-b90c67d537984cb09e2d4d3d6e3f8d80.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-61c0ac9f50cc4112bc2c426a2201430a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-61c0ac9f50cc4112bc2c426a2201430a.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-5fe2d7df652743f3961b2f42d84379ad: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-5fe2d7df652743f3961b2f42d84379ad.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-a2c00f87d06d4cb48ff619416f603705: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-a2c00f87d06d4cb48ff619416f603705.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-6da9f3aa683d46609398dbf39f0ece05: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-6da9f3aa683d46609398dbf39f0ece05.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-60972c04c4d14862ae97b2c288855f0d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-383a6221123b4b9c86fae4964abf2209: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-60972c04c4d14862ae97b2c288855f0d.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-383a6221123b4b9c86fae4964abf2209.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-83773673a7354524aba16da127461573: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-83773673a7354524aba16da127461573.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-303a2b7cc7284818a3100d83c6917501: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-303a2b7cc7284818a3100d83c6917501.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-a7a81ebefdaa4fb8b8e216159f251ec3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-a7a81ebefdaa4fb8b8e216159f251ec3.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-1829fea0b7cb4f4081247e740711f3e6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-1829fea0b7cb4f4081247e740711f3e6.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-c2ae51f8de7e43739dce77505d4ea2e4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-c2ae51f8de7e43739dce77505d4ea2e4.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-34c1cd7ed9374b4192e847377e16b107: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-34c1cd7ed9374b4192e847377e16b107.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-29f986437c6246ebb62c552ecd429eaf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-29f986437c6246ebb62c552ecd429eaf.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-46e1ef5ca4a04f1495ca90ea4b9a3f1d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-46e1ef5ca4a04f1495ca90ea4b9a3f1d.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-27758112675049218db2f217a44a6c20: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-27758112675049218db2f217a44a6c20.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-cb95a9843fe14ff6949008fbc147f6c5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-cb95a9843fe14ff6949008fbc147f6c5.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-1758691e139c4762aed69cf7c51e6f3b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-1758691e139c4762aed69cf7c51e6f3b.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-86ab35826978482184e4eff68fc6b843: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-86ab35826978482184e4eff68fc6b843.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-710ed53b90af41a89789133d1dfffcc6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-710ed53b90af41a89789133d1dfffcc6.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-3bbfa5bc2a8a4a7aac69b77437900917: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-3bbfa5bc2a8a4a7aac69b77437900917.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-2375c9bc397f477f98c510411e3673a9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-2375c9bc397f477f98c510411e3673a9.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-4d6f61f6ea6a426b813f0b272b990ed4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53478 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-4d6f61f6ea6a426b813f0b272b990ed4.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-e7f51d3a78134d14bd89dd58464d41a0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-e7f51d3a78134d14bd89dd58464d41a0.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-581161c6181a4fc09ecf94d7a4672839: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-581161c6181a4fc09ecf94d7a4672839.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-60ad38fc121b4faea16b48e904686cc2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-60ad38fc121b4faea16b48e904686cc2.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-ae17e532af714e289415f34ced200bff: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-ae17e532af714e289415f34ced200bff.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-93320a789ab447cb9f025c1c88eb49b9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-93320a789ab447cb9f025c1c88eb49b9.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-f5408b435f6942c682e15e610ed4b652: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-f5408b435f6942c682e15e610ed4b652.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-0a75a599a5d34e52a89a72a8bf1a23b6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-0a75a599a5d34e52a89a72a8bf1a23b6.
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [logger.py:43] Received request chatcmpl-726720e955844ae0886263f00a300e19: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:35 [async_llm.py:270] Added request chatcmpl-726720e955844ae0886263f00a300e19.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-15ded1d5565d49109ae4b222d143d014: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-15ded1d5565d49109ae4b222d143d014.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-eb3254f9502d4b3199276c4042683410: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-eb3254f9502d4b3199276c4042683410.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-82b12bb01f17463cb9aaa1a8b050787e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-82b12bb01f17463cb9aaa1a8b050787e.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-96292a5bdd9b4708bbf2ae9f1359e170: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-96292a5bdd9b4708bbf2ae9f1359e170.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-f536d00012cd4996a50abd231ccd7ae8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-f536d00012cd4996a50abd231ccd7ae8.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-80d99e4a28e5471097c3ce05fc763423: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-80d99e4a28e5471097c3ce05fc763423.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-36862e22908147adb6b1f76e01022fc7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-31aa40e944b94570b98918a002c7a288: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-36862e22908147adb6b1f76e01022fc7.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-31aa40e944b94570b98918a002c7a288.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-eb1da19ead90466f9cc7a91b5846547b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-eb1da19ead90466f9cc7a91b5846547b.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-30d71b464d1243f493fbd4881f358973: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-30d71b464d1243f493fbd4881f358973.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-cc9b8b66c9484f53884c6153afa5e6bc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-cc9b8b66c9484f53884c6153afa5e6bc.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-8e997699c8e64558875ff95d3450d484: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53642 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-8e997699c8e64558875ff95d3450d484.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-8728e2dbe5d3494282dedd13319d54ba: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-8728e2dbe5d3494282dedd13319d54ba.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-a6115db84cd5425bba68720f1e44d67b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-a6115db84cd5425bba68720f1e44d67b.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-ff3f71341a36411d824a32b72f1e84b8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53666 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-ff3f71341a36411d824a32b72f1e84b8.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-774f8a6848224a9dbb17b7c0d3960db3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-774f8a6848224a9dbb17b7c0d3960db3.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-92e86ba805ce4342976352196e07fe5c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-92e86ba805ce4342976352196e07fe5c.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-d50aaad7322e47fc8ea7c1cdd88a75d7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-d50aaad7322e47fc8ea7c1cdd88a75d7.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-80d9d50929f8408aa400b70b95d85a2b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-80d9d50929f8408aa400b70b95d85a2b.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-66a9c9a6b6ef481c95758ed385018420: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-66a9c9a6b6ef481c95758ed385018420.
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [logger.py:43] Received request chatcmpl-7410e5a504f1419e8c8092917a8cbabe: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53712 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:36 [async_llm.py:270] Added request chatcmpl-7410e5a504f1419e8c8092917a8cbabe.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-48a924d613f44b0396554d0997d580a2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-48a924d613f44b0396554d0997d580a2.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-8fe7217cefe04c4b9fbbf29413a4386e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-8fe7217cefe04c4b9fbbf29413a4386e.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-4da13b116009423c9f4ba0735485932e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-4da13b116009423c9f4ba0735485932e.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-645b449800c94838b3c79a9f4353cba5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-645b449800c94838b3c79a9f4353cba5.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-188a5c3bd0954fc18a4aa0f69352232a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-188a5c3bd0954fc18a4aa0f69352232a.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-d00dfebb330747f68b8852daea92ab94: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-d00dfebb330747f68b8852daea92ab94.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-8ecf58d955ab45c89a937036c59ae843: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-8ecf58d955ab45c89a937036c59ae843.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-02fc9862215e4da59956b13025b49ffc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-02fc9862215e4da59956b13025b49ffc.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-62a7a6b998f04257993fd28aa8095008: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-62a7a6b998f04257993fd28aa8095008.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-f9e3efa9abe745c6ac27bbd692fc85ae: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-f9e3efa9abe745c6ac27bbd692fc85ae.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-a8f4bb55627f400b95f59b9352fdb4e7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-a8f4bb55627f400b95f59b9352fdb4e7.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-c93d7c0a86a14b6dabf23562257a3b2d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-c93d7c0a86a14b6dabf23562257a3b2d.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-2609fec5e9e04f8ebc5640b09a755b5d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-66c35a951bd141b19dd14a22acbdf18f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-2609fec5e9e04f8ebc5640b09a755b5d.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-66c35a951bd141b19dd14a22acbdf18f.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-0516041fe1a44a49ae738a1fb599d5f9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-0516041fe1a44a49ae738a1fb599d5f9.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-fbf8a302d6ec4e72a4211a6e6ca6d054: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-fbf8a302d6ec4e72a4211a6e6ca6d054.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-a1355141ffd54527a2af4eff26005a9b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-a1355141ffd54527a2af4eff26005a9b.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-8e3fb01f48474f42ae22ec563b118360: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-8e3fb01f48474f42ae22ec563b118360.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-f1c303abdd9045d196607160086f156d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-f1c303abdd9045d196607160086f156d.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-5465756912ea4f00b305f2ac6588cae6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-5465756912ea4f00b305f2ac6588cae6.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-729189e2f3394c10a4f16665ee9da454: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-45a8757b85d142caa82526e4b54364a2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-729189e2f3394c10a4f16665ee9da454.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-45a8757b85d142caa82526e4b54364a2.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-58194d6d41034eb288e6fcdca3a36f51: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-c0c57404ab064d6e942e9e1e6f57d96a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-58194d6d41034eb288e6fcdca3a36f51.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-c0c57404ab064d6e942e9e1e6f57d96a.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-f22511b240f44de1b264e5391ade7109: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-f22511b240f44de1b264e5391ade7109.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-681b7c9cef0741908231ba8e8ee715bc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-681b7c9cef0741908231ba8e8ee715bc.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-54ca5bd6e9a14fdca7f56b9270bd8e41: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-78ecbb05f9ae449aae2f3abc73b1d30e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-54ca5bd6e9a14fdca7f56b9270bd8e41.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-78ecbb05f9ae449aae2f3abc73b1d30e.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-e8500d2889654586b1743545d3707290: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-55e895b6be7f4d61ba0fbfcc884b30b4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-e8500d2889654586b1743545d3707290.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-55e895b6be7f4d61ba0fbfcc884b30b4.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-731b3536f7b1429eb8706d6a4c7f5fd6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:53992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-731b3536f7b1429eb8706d6a4c7f5fd6.
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [logger.py:43] Received request chatcmpl-c78a13114f1a43c68f9f029d861e2749: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:37 [async_llm.py:270] Added request chatcmpl-c78a13114f1a43c68f9f029d861e2749.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-3d0cdb3ef70b4dd7b77e05866c0ea1b2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-3d0cdb3ef70b4dd7b77e05866c0ea1b2.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-b5ac07a7613349f787a3c385831b050e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-b5ac07a7613349f787a3c385831b050e.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-bf8981c2b2a7436fb47fa1df6a6da029: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-bf8981c2b2a7436fb47fa1df6a6da029.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-eb75af37efdc433caedde7cc57a27f05: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54040 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-eb75af37efdc433caedde7cc57a27f05.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-4fb8d553f592414b928b6807521db6c1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-4fb8d553f592414b928b6807521db6c1.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-554d46748f724ad09d76842894f23373: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-554d46748f724ad09d76842894f23373.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-0ebf513e219f4c6282db76571ded0337: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-0ebf513e219f4c6282db76571ded0337.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-9b837770b062458b845a52109acd3e65: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-9b837770b062458b845a52109acd3e65.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-18bc6392b70d4d38afbc0b86e7ae09b0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-18bc6392b70d4d38afbc0b86e7ae09b0.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-9048709e977c4211ab7d99cbe774ea2b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-9048709e977c4211ab7d99cbe774ea2b.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-a6a855464f1d476cbf13d6c165d947a1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-a6a855464f1d476cbf13d6c165d947a1.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-a4ce04dd1f1d4cc6a8531eb86341d65b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-a4ce04dd1f1d4cc6a8531eb86341d65b.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-a2a315a069dd4721846b6182ae099064: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-a2a315a069dd4721846b6182ae099064.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-968381347b4043aabf08ec766eb0680e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-968381347b4043aabf08ec766eb0680e.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-9f0138bdc9654bffbffc7b96509b578b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-9f0138bdc9654bffbffc7b96509b578b.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-0c9ae79cc5614e6d9b07461fd2647e71: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-0c9ae79cc5614e6d9b07461fd2647e71.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-007ad4d8395d469f90ac821be6614449: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-007ad4d8395d469f90ac821be6614449.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-791a4ecc79bd43ec94a9017752912ccb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-791a4ecc79bd43ec94a9017752912ccb.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-180b96eeec4a458fbb18086959e86225: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-180b96eeec4a458fbb18086959e86225.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-3349655c5c254c9db63033b49e802700: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-3349655c5c254c9db63033b49e802700.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-b85bee39e81e4c0281557bb6a37aa1fd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-b85bee39e81e4c0281557bb6a37aa1fd.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-8146037c75bc4956ac366ea752f1b520: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-8146037c75bc4956ac366ea752f1b520.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-825f861268d742a3a16024b4eb1cecd6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-825f861268d742a3a16024b4eb1cecd6.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-e639b78a83c4486cb60f8ea9b770dace: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-e639b78a83c4486cb60f8ea9b770dace.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-82165345c6174c6ba5e001b00f4cc68e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-82165345c6174c6ba5e001b00f4cc68e.
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [logger.py:43] Received request chatcmpl-abcbe51ac06c42aeb7f9b73917ddf69e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:38 [async_llm.py:270] Added request chatcmpl-abcbe51ac06c42aeb7f9b73917ddf69e.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-29455bf164d744739356d84a9fc5cecb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-29455bf164d744739356d84a9fc5cecb.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-8e6b72c57b0248a7bb7f50b1317b2cef: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-8e6b72c57b0248a7bb7f50b1317b2cef.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-b226700afb10433fb272cf19e3654598: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-b226700afb10433fb272cf19e3654598.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-d65304cebb3a4486bb3f06819d16e876: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-d65304cebb3a4486bb3f06819d16e876.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-b8bd13bed18144e4afae5120862df785: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54314 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-b8bd13bed18144e4afae5120862df785.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-92ebfda36a954477870ed217c8d9122f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-50bea6a1ab31442e987f9c1ad4ae3211: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-92ebfda36a954477870ed217c8d9122f.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-50bea6a1ab31442e987f9c1ad4ae3211.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-43c9c7abbc3d423aa4428a8cd4327514: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-43c9c7abbc3d423aa4428a8cd4327514.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-96871d49e42d4330af3da689ac5e6e3d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-96871d49e42d4330af3da689ac5e6e3d.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-25d966e9869a4a5a8272ba3636a9d1fa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-25d966e9869a4a5a8272ba3636a9d1fa.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-4dd63686f662453abadb367749ee7ec5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-4dd63686f662453abadb367749ee7ec5.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-1cacd99dc0a746038786efe5e312dfbc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-1cacd99dc0a746038786efe5e312dfbc.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-eac3d5b1e42f408888659dc6b06b344e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-e99461c4d5954fdc93ea5c79efcb66c7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-eac3d5b1e42f408888659dc6b06b344e.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-e99461c4d5954fdc93ea5c79efcb66c7.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-23b5c7b478d3459b81c65cd43cd49017: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-23b5c7b478d3459b81c65cd43cd49017.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-7f7df2f4e92843ea8a4d65abf6ce11d4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-7f7df2f4e92843ea8a4d65abf6ce11d4.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-c9abc9a17d48456e92f0f6af89607379: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-c9abc9a17d48456e92f0f6af89607379.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-c04b2fb0061f429fb33c994fd624b624: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-c04b2fb0061f429fb33c994fd624b624.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-0c028627cf444d93855574fad80a960b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-0c028627cf444d93855574fad80a960b.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-d0126a7ad5064667bd0889acc4bafa0c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-d0126a7ad5064667bd0889acc4bafa0c.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-c6d46181ddb54a909e519ae74f5c3dec: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-c6d46181ddb54a909e519ae74f5c3dec.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-8a6e59b3e2334c46882635c9121976cb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-8a6e59b3e2334c46882635c9121976cb.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-2771063861b54ec3bca30ee432e21931: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-2771063861b54ec3bca30ee432e21931.
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [logger.py:43] Received request chatcmpl-31264c1d212140bc9bc40b95fe3551e9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:39 [async_llm.py:270] Added request chatcmpl-31264c1d212140bc9bc40b95fe3551e9.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-041735ff33514e278fd4595d57e385a8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-041735ff33514e278fd4595d57e385a8.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-6fcc2f58c5c24a5c8174655063cae87b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-6fcc2f58c5c24a5c8174655063cae87b.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-3a6e7e0e08ae45bd91597b06ed42a786: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-3a6e7e0e08ae45bd91597b06ed42a786.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-da96bf852336431ab48a6437238aee40: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-da96bf852336431ab48a6437238aee40.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-0987a4b7414146f7be3ab6411dd8992e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-0987a4b7414146f7be3ab6411dd8992e.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-7ed1bd99190a4d71ae0a4949bfd39ec7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-7ed1bd99190a4d71ae0a4949bfd39ec7.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-b74a036c624b4aa0b45a34516466e386: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-b74a036c624b4aa0b45a34516466e386.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-373a6a8e50404358a3c3ac041e897560: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-373a6a8e50404358a3c3ac041e897560.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-dd5ddbbaf6574e838cec05a6ba7279e8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-dd5ddbbaf6574e838cec05a6ba7279e8.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-d819a88ba21a4aa0b30f89eaf29bcf78: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-d819a88ba21a4aa0b30f89eaf29bcf78.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-f1141171120e4c53a816d4f7d3b2b1db: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-f1141171120e4c53a816d4f7d3b2b1db.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-d2e393687565444aac3102ba322d9f71: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-d2e393687565444aac3102ba322d9f71.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-7f4988a5b89748768e6d3eb0b1d0466a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-7f4988a5b89748768e6d3eb0b1d0466a.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-ba1ee31896034f11a43bc124348f8bca: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-ba1ee31896034f11a43bc124348f8bca.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-65fdcb7afcce4f9da9fb03b982c542c8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-65fdcb7afcce4f9da9fb03b982c542c8.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-24587cbfec9e4a2f9edd53bcefd7cf0b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-24587cbfec9e4a2f9edd53bcefd7cf0b.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-fd8ac5713f1a4fa1bf45c6382217c78c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-fd8ac5713f1a4fa1bf45c6382217c78c.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-062a1de676a240db87e7541fa1d07f68: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-062a1de676a240db87e7541fa1d07f68.
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [logger.py:43] Received request chatcmpl-e2c45b886a9f453ca5d57c401c75001c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:40 [async_llm.py:270] Added request chatcmpl-e2c45b886a9f453ca5d57c401c75001c.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-176bb5190e7f45faa6ce6f3eb81db899: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-176bb5190e7f45faa6ce6f3eb81db899.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-e84877e96bce45ddac0fe1c340884048: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-e84877e96bce45ddac0fe1c340884048.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-dab6dc689ab74f30b22a687aa9c0ff33: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54686 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-dab6dc689ab74f30b22a687aa9c0ff33.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-82542b69e3994a07bbfe14b5d2f5f148: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-82542b69e3994a07bbfe14b5d2f5f148.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-180f2f8f7a86429a882668eb7450cf2c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-180f2f8f7a86429a882668eb7450cf2c.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-83e932c2e06b4a58b2a24e82f8bc34dd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-83e932c2e06b4a58b2a24e82f8bc34dd.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-d01539c42ffc4c3792b013f35e2c132c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-d01539c42ffc4c3792b013f35e2c132c.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-802b28000149491a8e4035f8d220f099: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-802b28000149491a8e4035f8d220f099.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-cbf931d7a625405cbe5b9a5958058bc8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-cbf931d7a625405cbe5b9a5958058bc8.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-cb60cba3200b41b0ade73c47ff0bf0d4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-e2a2665dffd84ea5927e2d7883590158: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-cb60cba3200b41b0ade73c47ff0bf0d4.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-e2a2665dffd84ea5927e2d7883590158.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-49e6a74ec8f6415c996d6f0c47efafb1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-49e6a74ec8f6415c996d6f0c47efafb1.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-96b4a3c784664c3185d3b61d514413c6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-96b4a3c784664c3185d3b61d514413c6.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-ad41f6ece40f455e948062576de2a64b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-ad41f6ece40f455e948062576de2a64b.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-34f4c5d674d84526935b50a3f39d49bf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-34f4c5d674d84526935b50a3f39d49bf.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-db76906833ae4e1d9e44914d4b163b7f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-db76906833ae4e1d9e44914d4b163b7f.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-5dfdb8c7b0464ca3aeabec592dd003c1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-5dfdb8c7b0464ca3aeabec592dd003c1.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-72b8d4d0fa6a48809d516dd0b9814f4b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-72b8d4d0fa6a48809d516dd0b9814f4b.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-cd4d2945821246539f3ec97a0c050787: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-cd4d2945821246539f3ec97a0c050787.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-1518f07032aa405ab12d90354d3a6df0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-1518f07032aa405ab12d90354d3a6df0.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-d0cb59cf2cbc4de883ebe08bef0279ec: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-d0cb59cf2cbc4de883ebe08bef0279ec.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-8a8f8cc4b03f455bbd2751dc97167a73: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-8a8f8cc4b03f455bbd2751dc97167a73.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-9024659808b140b387ed76dd354282a4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-9024659808b140b387ed76dd354282a4.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-5175e0d4fd414a78aaa6f3fe6d6c6d30: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-5175e0d4fd414a78aaa6f3fe6d6c6d30.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-c3b8cd5375d743f791a5d351d2ba9c9c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-c3b8cd5375d743f791a5d351d2ba9c9c.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-0f506c5c71284005bb18def45d71fe66: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-0f506c5c71284005bb18def45d71fe66.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-0cb8c750b0c047109deeb7c002815a20: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-0cb8c750b0c047109deeb7c002815a20.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-b8f7835fd0e4497ba62ce47a6122fae1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-b8f7835fd0e4497ba62ce47a6122fae1.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-3d547e6d76344a61ab38433d79a2e257: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-3d547e6d76344a61ab38433d79a2e257.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-4fa5b02caf2c4ccc8418f17fdbebcf1d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-4fa5b02caf2c4ccc8418f17fdbebcf1d.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-b84abf6f77644dc9bafa5dcb7ff40035: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-b84abf6f77644dc9bafa5dcb7ff40035.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-66fd9eac47ed4f15a8419d3f7886b443: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-66fd9eac47ed4f15a8419d3f7886b443.
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [logger.py:43] Received request chatcmpl-152ece438a204addb5b78a301ff1b660: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:41 [async_llm.py:270] Added request chatcmpl-152ece438a204addb5b78a301ff1b660.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-9f39d3f9be674a1d965d4bb9c4ad6b27: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-9f39d3f9be674a1d965d4bb9c4ad6b27.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-b81a7d31377e44a5a1fbfa29572a45c6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-b81a7d31377e44a5a1fbfa29572a45c6.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-5477833fc28149b1b81ee60c3cd15963: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:54998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-5477833fc28149b1b81ee60c3cd15963.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-429173b6003843ba8215529e1ed4d872: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-429173b6003843ba8215529e1ed4d872.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-2c1731677a494d8386103ec6cf9d2dec: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-2c1731677a494d8386103ec6cf9d2dec.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-041c0e6a3078468796a510a8e3974621: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-041c0e6a3078468796a510a8e3974621.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-04db294b0b3945d89c375b3e42e1f27e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-04db294b0b3945d89c375b3e42e1f27e.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-454f94b4d8804f7bbb949b16ac5dcf1e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-454f94b4d8804f7bbb949b16ac5dcf1e.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-3e8ba821b2754616b4f6c8fbbd3a4bed: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-3e8ba821b2754616b4f6c8fbbd3a4bed.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-cd72f2aa21fc4fd7acb87e05ce3db258: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-cd72f2aa21fc4fd7acb87e05ce3db258.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-9e7564d9fad243d4b966c38b009bcd00: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-9e7564d9fad243d4b966c38b009bcd00.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-6982fc7659e145b9967ff82169740cf5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-6982fc7659e145b9967ff82169740cf5.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-7b2859daddef4df5a03a4bb92d25091a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-7b2859daddef4df5a03a4bb92d25091a.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-4927de56fc8e4f5cb07ec85808410d08: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-4927de56fc8e4f5cb07ec85808410d08.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-58516a6c1f1c4126b054c16f328989af: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-58516a6c1f1c4126b054c16f328989af.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-3366b321e8804250b831548cae8bec9e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-3366b321e8804250b831548cae8bec9e.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-e734599991f74fa68e0aa2b1ef7ec6fa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-8719ffae84a146329040660804b7e0cc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-e734599991f74fa68e0aa2b1ef7ec6fa.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-8719ffae84a146329040660804b7e0cc.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-357e501f64ef47f0b08142666fe87283: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-357e501f64ef47f0b08142666fe87283.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-331178fa204b4180b935f1462486f9c0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-331178fa204b4180b935f1462486f9c0.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-34b48322adea46b189099e63307c0d5d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-34b48322adea46b189099e63307c0d5d.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-e20ca1710b11455485dce599da9a5714: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-e20ca1710b11455485dce599da9a5714.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-eac12f5cceb545cc9968d2c7a9602df6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-eac12f5cceb545cc9968d2c7a9602df6.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-bab591ac47e94b679f3b3f01e1191319: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-bab591ac47e94b679f3b3f01e1191319.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-1d9f4930e911443f89f761c068db0fb4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-1d9f4930e911443f89f761c068db0fb4.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-fb1c9d07ed0141feb5beaa28421f0374: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-fb1c9d07ed0141feb5beaa28421f0374.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-6368a2ed17574ebd87b2c3aa40408056: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-6368a2ed17574ebd87b2c3aa40408056.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-16aeb52736354a9f8ff236dca813996b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-16aeb52736354a9f8ff236dca813996b.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-66dd8b7c3d864771b39edfbccd5ae3e0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:55208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-66dd8b7c3d864771b39edfbccd5ae3e0.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-4ab4a8aaa38b4de8a0ec7f92ada3b004: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-4ab4a8aaa38b4de8a0ec7f92ada3b004.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-19ff304ae91c42a7ac74722ed92eea82: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-19ff304ae91c42a7ac74722ed92eea82.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-72c5c4684d8b4c599c3ee50bdeb7f665: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-72c5c4684d8b4c599c3ee50bdeb7f665.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-25e20ce184f74f1aa0b9dce3945513c3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43574 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-25e20ce184f74f1aa0b9dce3945513c3.
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [logger.py:43] Received request chatcmpl-9620ee16e3f84a1ea040a7070a178a42: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:42 [async_llm.py:270] Added request chatcmpl-9620ee16e3f84a1ea040a7070a178a42.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [loggers.py:118] Engine 000: Avg prompt throughput: 520.7 tokens/s, Avg generation throughput: 3558.1 tokens/s, Running: 254 reqs, Waiting: 193 reqs, GPU KV cache usage: 79.4%, Prefix cache hit rate: 84.9%
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-41cb32e1e25a4617811885d5c09cac7f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-41cb32e1e25a4617811885d5c09cac7f.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-2230f3b6f9734c2ea51a1e440909625d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-2230f3b6f9734c2ea51a1e440909625d.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-a273e6db525d4862af586264453d4dea: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-2f6bd30c74ad40778bd519ed207a3f49: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-67aedc6bd26a4391a21ddf4d54add210: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-a273e6db525d4862af586264453d4dea.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-2f6bd30c74ad40778bd519ed207a3f49.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-67aedc6bd26a4391a21ddf4d54add210.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-fcc71c16cc754a71af23cecfaf558c72: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-fcc71c16cc754a71af23cecfaf558c72.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-e37dd4163dc0488f84731e983864fd32: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-e37dd4163dc0488f84731e983864fd32.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-0e211a6d961440f0b7d3d4abca174b90: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-0e211a6d961440f0b7d3d4abca174b90.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-7ab0a3ed994a45cfa7ecca1047a97dda: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-7ab0a3ed994a45cfa7ecca1047a97dda.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-179123f26a59499086aefd06271ebc2e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43662 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-179123f26a59499086aefd06271ebc2e.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-649d10162b714587bd91dae8436596a8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-649d10162b714587bd91dae8436596a8.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-8477242a587a47b792a25e6d739a71d3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-8477242a587a47b792a25e6d739a71d3.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-215319f7506743a3bed9aa72edbfe741: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-215319f7506743a3bed9aa72edbfe741.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-b42fd71f64fd4e6390c7b35149664994: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-41d649a3e9794610b92439c9256d7743: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-b42fd71f64fd4e6390c7b35149664994.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43712 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-41d649a3e9794610b92439c9256d7743.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-a57495ae522a40f3848a0b6eb90e4e8f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-a57495ae522a40f3848a0b6eb90e4e8f.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-7cdf5ee420d34849838ad74d62ae6da7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-7cdf5ee420d34849838ad74d62ae6da7.
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [logger.py:43] Received request chatcmpl-f7bca265c322465eb4ea680cb3b43435: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:43 [async_llm.py:270] Added request chatcmpl-f7bca265c322465eb4ea680cb3b43435.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-64fa94bf6e3545fd9bffd8f07ad2d583: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-64fa94bf6e3545fd9bffd8f07ad2d583.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-ecca735e841d4fffbc9c2c148d071bc8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-ecca735e841d4fffbc9c2c148d071bc8.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-dee4566f975c4ac1bcd2a6bb8b1c2020: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-dee4566f975c4ac1bcd2a6bb8b1c2020.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-2f4cf9ce41e84aebac1c03591c84a3c8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-2f4cf9ce41e84aebac1c03591c84a3c8.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-5053d62224294c2388f9fcb7a1bab129: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-5053d62224294c2388f9fcb7a1bab129.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-81f6d230f8234e3d943eb5f9bf65f8eb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-81f6d230f8234e3d943eb5f9bf65f8eb.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-4d21e0cbfab443eb921819631382d7af: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-4d21e0cbfab443eb921819631382d7af.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-34f66f43fdee433f80b72415766e9aa0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-d5868d086e7441719f24b88745b3bfdb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-34f66f43fdee433f80b72415766e9aa0.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-d5868d086e7441719f24b88745b3bfdb.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-59ce8fa119114e1c9812638c9d2230cc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-59ccb4db69784f6cac81344ad9e9d19b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-81ef6c5960a3467e9f88fe244b8c28c9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-59ce8fa119114e1c9812638c9d2230cc.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-59ccb4db69784f6cac81344ad9e9d19b.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-a4f7df1a13d34ec6a8a137ca31aad169: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-81ef6c5960a3467e9f88fe244b8c28c9.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-81711a28b37c45488e3d2e45bd7c461e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-a4f7df1a13d34ec6a8a137ca31aad169.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-81711a28b37c45488e3d2e45bd7c461e.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-fe62e30a039a4ddcb0c2dcdff941dfe4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-fe62e30a039a4ddcb0c2dcdff941dfe4.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-865062c1d173443e98e28c21c95315a3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-865062c1d173443e98e28c21c95315a3.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-c08dc2568e294354821bfcdbf7e8baa7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-c08dc2568e294354821bfcdbf7e8baa7.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-f9f3dfd8ad4d49389ba8ccf37947c69e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-f9f3dfd8ad4d49389ba8ccf37947c69e.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-54a2575fbe9542e6a1cf7d41ec7ed227: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-54a2575fbe9542e6a1cf7d41ec7ed227.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-00e8b363f44a4ddc90d2fd6017fbbff4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-00e8b363f44a4ddc90d2fd6017fbbff4.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-48389faff542459693d446360e9e597f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-48389faff542459693d446360e9e597f.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-685af73dd62d4303a455db1bd2c0d82a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-685af73dd62d4303a455db1bd2c0d82a.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-d5a56f5d9b2345d19f4da8ad00095e5d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-d5a56f5d9b2345d19f4da8ad00095e5d.
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [logger.py:43] Received request chatcmpl-c079d55ff4c64d1b850ad9157f09bf96: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:44 [async_llm.py:270] Added request chatcmpl-c079d55ff4c64d1b850ad9157f09bf96.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-4b62e19819824f109af90aaad469fd14: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-4b62e19819824f109af90aaad469fd14.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-ffd23c2a9e1144a891ce032e021a5ae9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-6b336ae7d9ee4efe9b8010802861f2cc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-ffd23c2a9e1144a891ce032e021a5ae9.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-6b336ae7d9ee4efe9b8010802861f2cc.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-2a469fc03599403e948dd44753b924f1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:43998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-2a469fc03599403e948dd44753b924f1.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-56237d01031747fbad3cabee29a68541: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-56237d01031747fbad3cabee29a68541.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-47533ad8724a479197598d8216b3c77a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-47533ad8724a479197598d8216b3c77a.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-263b8536979a4980b7c97e64e5532724: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-0b126b49658e4b7ba57819d57f19bc37: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-263b8536979a4980b7c97e64e5532724.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-0b126b49658e4b7ba57819d57f19bc37.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-95cf642e34134c3988af074442df1947: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-95cf642e34134c3988af074442df1947.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-908e72e63feb4abc924739c82a06abb3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-908e72e63feb4abc924739c82a06abb3.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-a8746d6e69e54ff7a55a47f9829acc41: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-a8746d6e69e54ff7a55a47f9829acc41.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-c0b85d3c22084f1580d9dae8214fdc50: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-c0b85d3c22084f1580d9dae8214fdc50.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-9f0ff12256ee488f8513628083fd7b77: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-9f0ff12256ee488f8513628083fd7b77.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-816132ac82e6482d9332f729be418b56: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-816132ac82e6482d9332f729be418b56.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-6ea0351dfbd749b2ab089f15f020546b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-6ea0351dfbd749b2ab089f15f020546b.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-e64daa5619c24ee0b1503bb07b0aacf5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-e64daa5619c24ee0b1503bb07b0aacf5.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-494719f94ee64579854235e1e8c7039c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-494719f94ee64579854235e1e8c7039c.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-7b67be1d8954408b8da53fecf609d4e4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-7b67be1d8954408b8da53fecf609d4e4.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-d5219c4d6f5a4943984ed4e6a1246382: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-d5219c4d6f5a4943984ed4e6a1246382.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-2bba320af1d34279bd569dad2f0775c6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-2bba320af1d34279bd569dad2f0775c6.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-2348629fe83c4280829cbd46a65bf4aa: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-2348629fe83c4280829cbd46a65bf4aa.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-1c34a889c798457094269ae4ee5bee04: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44144 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-1c34a889c798457094269ae4ee5bee04.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-485215ca48ce4eb581cc8d9ae9624d70: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44148 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-485215ca48ce4eb581cc8d9ae9624d70.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-1e18762c109d497abdbedf043a9e452a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-1e18762c109d497abdbedf043a9e452a.
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [logger.py:43] Received request chatcmpl-a8018516925b4fc492efcc809084bd37: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:45 [async_llm.py:270] Added request chatcmpl-a8018516925b4fc492efcc809084bd37.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-1083e067122b41ac9dc8cd17ec020f8a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-1083e067122b41ac9dc8cd17ec020f8a.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-8257d14bb50b4c98b5c901fe833b3f54: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-8257d14bb50b4c98b5c901fe833b3f54.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-725bf93420a64fd5a6f35f9d73335a1f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-725bf93420a64fd5a6f35f9d73335a1f.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-d3032e0348724a60aa0971ff795a8c26: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-d3032e0348724a60aa0971ff795a8c26.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-4de288edfa1544c8b355d684d89ec21a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-0d759a0dbc8544c6aedb948c2a966902: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-7bbf65ccbf6d44848fd0d54ae54b991f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-4de288edfa1544c8b355d684d89ec21a.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-e83f8178c2ca481c8faee604255d2fa3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-0d759a0dbc8544c6aedb948c2a966902.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-7bbf65ccbf6d44848fd0d54ae54b991f.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-e83f8178c2ca481c8faee604255d2fa3.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-f65bb28186be4f7fb1b48d5e7ac48b80: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-f65bb28186be4f7fb1b48d5e7ac48b80.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-880d40714305492d91aa049f386e748f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-880d40714305492d91aa049f386e748f.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-676839285de245b1b7a1230955fd8301: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-676839285de245b1b7a1230955fd8301.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-76ed8647e63b4a2fa6d777f6bddad887: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-76ed8647e63b4a2fa6d777f6bddad887.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-87cf7ee0bc2a4aaabd10c23cc3a181e1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-87cf7ee0bc2a4aaabd10c23cc3a181e1.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-1e0a81c090484f4e8d5038bef72f3060: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-1e0a81c090484f4e8d5038bef72f3060.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-5070e5b1663d4b35a8b422c2f7cc7e84: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-5070e5b1663d4b35a8b422c2f7cc7e84.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-22def87ed7a7401498509743b5a0c2fe: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-22def87ed7a7401498509743b5a0c2fe.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-7b93bdbb1c9242418333a623f29845b1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-7b93bdbb1c9242418333a623f29845b1.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-ffdbe79d24424be7b86f6ba74d1faf8c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-ffdbe79d24424be7b86f6ba74d1faf8c.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-643706563e1749deb9cccf5a00fea4be: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-643706563e1749deb9cccf5a00fea4be.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-258d3cf6071b4f72bac169762f0b7849: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44314 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-258d3cf6071b4f72bac169762f0b7849.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-65ce5ff0db4a40979377dfb130613b8f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-65ce5ff0db4a40979377dfb130613b8f.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-a9a76125a60e4de6b5d988634778bd2c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-a9a76125a60e4de6b5d988634778bd2c.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-9b6ddeabf9fb40abaf7fe010abcac0c0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-9b6ddeabf9fb40abaf7fe010abcac0c0.
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [logger.py:43] Received request chatcmpl-58103b71c4e44713b4883577485f63d7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:46 [async_llm.py:270] Added request chatcmpl-58103b71c4e44713b4883577485f63d7.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-6c5721d3b5b14aeb86817fddba7df96a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-6c5721d3b5b14aeb86817fddba7df96a.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-cdab21cc01294d75998d3965c848f922: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-cdab21cc01294d75998d3965c848f922.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-b7cb38ffd64f4bda93b188cd7d15e4b0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-b7cb38ffd64f4bda93b188cd7d15e4b0.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-23c5378cc174463298d9379f95e2ead2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-23c5378cc174463298d9379f95e2ead2.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-084dde3b47af4df593e9aeeb317b1da9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-084dde3b47af4df593e9aeeb317b1da9.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-977b3be1c41b4378903e2c85e4df0049: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-977b3be1c41b4378903e2c85e4df0049.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-f10a79d698d345b18e5840b572c3a780: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-c85043f6ca2143038db520e2289e0ba1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-f10a79d698d345b18e5840b572c3a780.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-c85043f6ca2143038db520e2289e0ba1.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-3f6f38e7627e483d81a9ffa994b96a95: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44418 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-3f6f38e7627e483d81a9ffa994b96a95.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-b76a827d1ad341eca236f76048a0695a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-7c52920e88d2429ab1105c651c04dcb1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-b76a827d1ad341eca236f76048a0695a.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-7c52920e88d2429ab1105c651c04dcb1.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-1c0604809f384088aec4a04bc063343e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-1c0604809f384088aec4a04bc063343e.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-5b6b055e7a3142bfb4cd73e34fb313d8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-5b6b055e7a3142bfb4cd73e34fb313d8.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-97cee28fffaf4e49825069f74ebaf064: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-97cee28fffaf4e49825069f74ebaf064.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-771b3bd976b347628a66fdba9fa18a16: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44478 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-771b3bd976b347628a66fdba9fa18a16.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-94381bce4473409b94bb7de98f82775f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-94381bce4473409b94bb7de98f82775f.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-ed6554fc400146429d2b1b6971542ce3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-ed6554fc400146429d2b1b6971542ce3.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-58821b7c77bd48c7b1d16c67eda027fa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-58821b7c77bd48c7b1d16c67eda027fa.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-5097acc11cdb4aae8b40e783ca8c3a08: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-8c50554351de4495bed29e7199a7476e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-5097acc11cdb4aae8b40e783ca8c3a08.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-8c50554351de4495bed29e7199a7476e.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-7c743a0c9f224087a528c53b734edd7a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-7c743a0c9f224087a528c53b734edd7a.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-071d087725f549f4a66e299b927e5153: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-071d087725f549f4a66e299b927e5153.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-c8d1c9af1f35495794941938334c183e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-c8d1c9af1f35495794941938334c183e.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-fb3d0d6870fe43429f7de74050b1c81b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44552 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-fb3d0d6870fe43429f7de74050b1c81b.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-c58033062eb7477fae6d0ba55b569b8f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-34a2ae47de42409891c80784fc5db534: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-c58033062eb7477fae6d0ba55b569b8f.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-34a2ae47de42409891c80784fc5db534.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-1cd57a4e26f74e38bd2ebf0517d25136: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-1cd57a4e26f74e38bd2ebf0517d25136.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-8eb5b287cab24728880c9747ab8f94f3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-8eb5b287cab24728880c9747ab8f94f3.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-037260afe50647299d681e0b953604d8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-037260afe50647299d681e0b953604d8.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-90bdedf54dc94b33a8394600e82a58e6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-90bdedf54dc94b33a8394600e82a58e6.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-f16fd29e2bdc4509901e8e62f81fa904: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-f16fd29e2bdc4509901e8e62f81fa904.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-57a0908fbdba42568399f8b12727e40d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-57a0908fbdba42568399f8b12727e40d.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-9009779dbcf84793b6cf00201195c33b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-9009779dbcf84793b6cf00201195c33b.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-3ebcf0f82d0a46ff9bfb961d8bc4d6e3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-3ebcf0f82d0a46ff9bfb961d8bc4d6e3.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-2309d524b164481d8da06330ce01523d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-2309d524b164481d8da06330ce01523d.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-7a4da6c2357a4c8281f7ebd38b7fb875: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-7a4da6c2357a4c8281f7ebd38b7fb875.
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [logger.py:43] Received request chatcmpl-9de031cfbda541f1b10f145346e57c66: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:47 [async_llm.py:270] Added request chatcmpl-9de031cfbda541f1b10f145346e57c66.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-29b66b1951224657a4ca54a57c81ce96: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-29b66b1951224657a4ca54a57c81ce96.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-24bb8e5d07964dc4b09e5ee4f9f20e15: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-24bb8e5d07964dc4b09e5ee4f9f20e15.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-c5ac48978ac34d28b811d8d8fc52f435: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44712 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-c5ac48978ac34d28b811d8d8fc52f435.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-e4818a2efef742d4a6c0d1e92ed34389: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-e4818a2efef742d4a6c0d1e92ed34389.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-7b5727a677fe4d71b9d7d197058f3b58: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-7b5727a677fe4d71b9d7d197058f3b58.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-417ce28ddd0c40669ccf18ffb34765a7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-a535c475f29c49f486c56a000dda3caa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-417ce28ddd0c40669ccf18ffb34765a7.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-a535c475f29c49f486c56a000dda3caa.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-d9c4ba8f841948ffaf4bc3626fcd1781: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-d9c4ba8f841948ffaf4bc3626fcd1781.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-6088406291274202a148ec841406749b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-6088406291274202a148ec841406749b.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-4fdb0048fb8a4ce48a5b5403f57d3d60: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-4fdb0048fb8a4ce48a5b5403f57d3d60.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-4850b0c4c8c543098160062927f557a8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-4850b0c4c8c543098160062927f557a8.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-a210a35dfd59463cb52b4e1f1178ec9e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-349369cf49c74c98b2c394e27f058c3d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-a210a35dfd59463cb52b4e1f1178ec9e.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-349369cf49c74c98b2c394e27f058c3d.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-93203d982c1949968d580caa718e46a8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-93203d982c1949968d580caa718e46a8.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-2fe6710acca8429ab367ddf9807540c5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-2fe6710acca8429ab367ddf9807540c5.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-2e32e672aabb4da7a37f594d38894afe: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-2e32e672aabb4da7a37f594d38894afe.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-6e422e6b618c4b249011cc6ca91d2b3f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-4d05b4e3e4bd40578c6a46ed616349ae: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-6e422e6b618c4b249011cc6ca91d2b3f.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-7f9403df6845414fb9f9af8bd90567aa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-4d05b4e3e4bd40578c6a46ed616349ae.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-7f9403df6845414fb9f9af8bd90567aa.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-daf8b3b95d064455b55dfc479854d3d9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-daf8b3b95d064455b55dfc479854d3d9.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-5f448739218f4c7390fd041a6eb86183: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-5f448739218f4c7390fd041a6eb86183.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-2e50db3a6ee843f5a5b9ae555e11a5d7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-2e50db3a6ee843f5a5b9ae555e11a5d7.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-dde8d94802b443c29d901a5357d746fa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-9ae1f479a380445db2c3f1be1bc09f4b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-dde8d94802b443c29d901a5357d746fa.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-9ae1f479a380445db2c3f1be1bc09f4b.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-d995664a3a8643d0bdbdbc56566b36a1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-d995664a3a8643d0bdbdbc56566b36a1.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-fa868846d62b480ba82fdfd3e9d02616: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-fa868846d62b480ba82fdfd3e9d02616.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-f564c38333314ab1ada811dc29dbd187: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-f564c38333314ab1ada811dc29dbd187.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-11afc29cad474f6ba0834a7ec222f276: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-11afc29cad474f6ba0834a7ec222f276.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-ee937bdb82124ab7b13dc4bcb3578751: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-0ace5a048b65495c82fd3e5554520556: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-ee937bdb82124ab7b13dc4bcb3578751.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-0ace5a048b65495c82fd3e5554520556.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-21e3bcb4a9bb4864b85a10a994544726: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-21e3bcb4a9bb4864b85a10a994544726.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-0b4a7d8d9aa644a6bef6217c557a1fd2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-0b4a7d8d9aa644a6bef6217c557a1fd2.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-4aa81db413fb4fd28617d5f93429a7b0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:44992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-4aa81db413fb4fd28617d5f93429a7b0.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-cc716df11d54497c8fa2ecc517f21675: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-cc716df11d54497c8fa2ecc517f21675.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-fcd03fd1abfa41868cb37d36766b533b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-fcd03fd1abfa41868cb37d36766b533b.
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [logger.py:43] Received request chatcmpl-7904be6309e14f6e973c5413d235e3cc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:48 [async_llm.py:270] Added request chatcmpl-7904be6309e14f6e973c5413d235e3cc.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-bee22331870146759532497928c887e3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-0f4a5eb914bd4a42844fe3e7e73de74e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-bee22331870146759532497928c887e3.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-0f4a5eb914bd4a42844fe3e7e73de74e.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-d5757e21126949f5a75d09a232b378c9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-d5757e21126949f5a75d09a232b378c9.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-458ed8272522461590980ef4a07219dc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-458ed8272522461590980ef4a07219dc.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-5ae9d3c9840042e6872128b14e41fa57: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-5ae9d3c9840042e6872128b14e41fa57.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-2fe33554824f49d3baa5edc6103167c3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-2fe33554824f49d3baa5edc6103167c3.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-f120e4e856bd4e80af7016395ba776d7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-f120e4e856bd4e80af7016395ba776d7.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-5ff6f93648674c0dacd8e791925cbee4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-5ff6f93648674c0dacd8e791925cbee4.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-7fe27a129c024af0be841ec9df8461aa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-7fe27a129c024af0be841ec9df8461aa.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-48222cb19a8840bdac0dd382278267ec: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-48222cb19a8840bdac0dd382278267ec.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-af056881d32141399bfb7754adfe33ca: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-af056881d32141399bfb7754adfe33ca.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-6ad5aeeb45ff4975818c972cd5cfafac: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-6ad5aeeb45ff4975818c972cd5cfafac.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-35ae0fa8e4974983b620d4ef01a3c795: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-35ae0fa8e4974983b620d4ef01a3c795.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-ee019104a6d64ef59b3d7cb77caadecd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-ee019104a6d64ef59b3d7cb77caadecd.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-dd23a069a9c24319bdae95404406378e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-dd23a069a9c24319bdae95404406378e.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-5294367a969747a6bff67367fec3a594: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-5294367a969747a6bff67367fec3a594.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-27ca746d690340e4b149ac19cf783611: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-81d13b82a2994c20882b82ecd4452ea7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-2cf6da728f364b1882d44ed2ae88dd27: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-27ca746d690340e4b149ac19cf783611.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-81d13b82a2994c20882b82ecd4452ea7.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-b43b0407057747c18716eac2f5d98452: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-2cf6da728f364b1882d44ed2ae88dd27.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-b43b0407057747c18716eac2f5d98452.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-e41941c9dadf4f8ab9f46ff7fca89537: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-bd4f50e510c4470192b130ddb2c629c4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-e41941c9dadf4f8ab9f46ff7fca89537.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-bd4f50e510c4470192b130ddb2c629c4.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-7f022db0cec84ad7a868fa33a8792edf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45224 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-7f022db0cec84ad7a868fa33a8792edf.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-48152cf7bf934bb69e2f6ef9226913eb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-48152cf7bf934bb69e2f6ef9226913eb.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-857119994a274011b7b11589d669bfe1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-857119994a274011b7b11589d669bfe1.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-69b8737da78543d388b68819eeb310bf: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-69b8737da78543d388b68819eeb310bf.
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [logger.py:43] Received request chatcmpl-62726df868e542179c2f8fad2ac4a4e6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:49 [async_llm.py:270] Added request chatcmpl-62726df868e542179c2f8fad2ac4a4e6.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-a76432a211664156aeea8a64710ff1d4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-a76432a211664156aeea8a64710ff1d4.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-2aa99fdb45dc4b68ac660f7741b21ac4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-2aa99fdb45dc4b68ac660f7741b21ac4.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-33f16bd0401e4fd3ae6d1a35467baa65: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-33f16bd0401e4fd3ae6d1a35467baa65.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-b8d13322074043e7b5b6d4dd10e80352: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-b8d13322074043e7b5b6d4dd10e80352.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-4282e9f8418e4a669c637c52121b2940: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-4282e9f8418e4a669c637c52121b2940.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-3da5546c2b5b4be1970cdef844b21387: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-611297ddd0cc4d7d9abac70062497869: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-3da5546c2b5b4be1970cdef844b21387.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-611297ddd0cc4d7d9abac70062497869.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-c285f51b1326424a9ad2f2532218fb0c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-c1cb26b99fd045e2b85093c0ffc7c3e9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-9c4da9bf2a7948f79c2b88f61a190adf: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-c285f51b1326424a9ad2f2532218fb0c.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-c1cb26b99fd045e2b85093c0ffc7c3e9.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-9c4da9bf2a7948f79c2b88f61a190adf.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-d925a2e1f2e340ca86d9ac3bf5a77036: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-d925a2e1f2e340ca86d9ac3bf5a77036.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-31f04c824fc84ac584f7ad1bd2c110d4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-31f04c824fc84ac584f7ad1bd2c110d4.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-b755a3c321144ca49824a1eab742d987: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-b755a3c321144ca49824a1eab742d987.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-8103e85adfcf4188befa5ce47f9394b7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-8103e85adfcf4188befa5ce47f9394b7.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-a1da5c8c16ca4e55accc994807f12264: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-a1da5c8c16ca4e55accc994807f12264.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-3a6a4091d6024a1f9e304d3f8906eff6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-3a6a4091d6024a1f9e304d3f8906eff6.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-4d684fd5d372485ea4644f1ebaf3a52a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-4d684fd5d372485ea4644f1ebaf3a52a.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-edb4cc5193464b40bd9b172029b580a1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-edb4cc5193464b40bd9b172029b580a1.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-6e58e56601224fb58da81ef0b989ab39: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-6e58e56601224fb58da81ef0b989ab39.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-bf9a4bc9f1c6479389d705a58116c9c4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [async_llm.py:270] Added request chatcmpl-bf9a4bc9f1c6479389d705a58116c9c4.
[36mllm_server_1  |[0m INFO 07-20 22:59:50 [logger.py:43] Received request chatcmpl-b094e4af3a824826819671890131ac16: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-b094e4af3a824826819671890131ac16.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-e9dd9f92ab4e48f1a65d0095543a2500: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-e9dd9f92ab4e48f1a65d0095543a2500.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-4222d18895564a3c90a35b7e9eb4747d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-4222d18895564a3c90a35b7e9eb4747d.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-c82b7f2904f849d6a9344a4948e43334: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-c82b7f2904f849d6a9344a4948e43334.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-eb6702178db34967ad12f6f1da3f7625: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-eb6702178db34967ad12f6f1da3f7625.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-862debaa89524e6f92b554600dcb4b04: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-862debaa89524e6f92b554600dcb4b04.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-97d73208449840bea0bc90618c1b728b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-97d73208449840bea0bc90618c1b728b.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-d8ddf7e97df44f7d9cd4c4bb258195ce: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-d8ddf7e97df44f7d9cd4c4bb258195ce.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-3f14375b18884df7aae8a49ddf061a24: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-3f14375b18884df7aae8a49ddf061a24.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-86a07cf3092248d68d57f4ccbe0d3ed2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-86a07cf3092248d68d57f4ccbe0d3ed2.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-3d8ea133d53f4263ae9f4264061a9a84: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-3d8ea133d53f4263ae9f4264061a9a84.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-92cc9cf8ee854bf9bc5190b3840eac42: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-92cc9cf8ee854bf9bc5190b3840eac42.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-169d3343442e4745b14e387c71c6b1b7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-169d3343442e4745b14e387c71c6b1b7.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-ea387d168d0f42ac978c3b6c27d8c3c7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-ea387d168d0f42ac978c3b6c27d8c3c7.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-5911dbb9152e4b1fb58362bd48ad78b9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-5911dbb9152e4b1fb58362bd48ad78b9.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-880f88d238d1499093a184a33bd736cf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-880f88d238d1499093a184a33bd736cf.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-6b1f0b62616e41d4bb88ddbccfb118f7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-6b1f0b62616e41d4bb88ddbccfb118f7.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-6de28876abe04e3686ea5d97d237877b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-6de28876abe04e3686ea5d97d237877b.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-4632424d74534362b0affe5aa4c8a363: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-bf07d7aab47140c78d2eb17e3c41f92c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-4632424d74534362b0affe5aa4c8a363.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-bf07d7aab47140c78d2eb17e3c41f92c.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-5e45c495dfc444648fb4976abd14146b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-1a473dc16cba4cf193f030f348c0ed4a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-5e45c495dfc444648fb4976abd14146b.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-1a473dc16cba4cf193f030f348c0ed4a.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-1c6d0d9c2b274da8a3ec30a57034c6e6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-1c6d0d9c2b274da8a3ec30a57034c6e6.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-c46dcd6cf2c449859a57a88eb9ccd715: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-c46dcd6cf2c449859a57a88eb9ccd715.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-757f296ff0004fcb8f8b8296092836d3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-757f296ff0004fcb8f8b8296092836d3.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-048facbdd5904dea81c8580724a6e49f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-048facbdd5904dea81c8580724a6e49f.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-9f2f34b96e874a72bd172c4574113732: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-9f2f34b96e874a72bd172c4574113732.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-ff405f9018f34a2f9ff4e4f379ee6d31: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-ff405f9018f34a2f9ff4e4f379ee6d31.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-11a90e541ef941ef8250e41e8705df49: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-11a90e541ef941ef8250e41e8705df49.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-758cf30c1f094cbbbf6e6d14ce845a04: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-758cf30c1f094cbbbf6e6d14ce845a04.
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [logger.py:43] Received request chatcmpl-8aaf7dca59244bf0ac1965dfaec63d7a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:51 [async_llm.py:270] Added request chatcmpl-8aaf7dca59244bf0ac1965dfaec63d7a.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-9289f69cebc74818a8201e3840d346ea: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-9289f69cebc74818a8201e3840d346ea.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-710a15d2c82840a0b37fa46ba6f11202: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-710a15d2c82840a0b37fa46ba6f11202.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-01ce3838cbd44d3db612ba609ff4aa83: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-01ce3838cbd44d3db612ba609ff4aa83.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-06092cca237b413f96a21efa6772c60a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45814 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-06092cca237b413f96a21efa6772c60a.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-aa362f7c351247b5a5965df306d3fa6b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-aa362f7c351247b5a5965df306d3fa6b.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-cf9f2b669a534c5987e5cd1bb9fcf412: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-cf9f2b669a534c5987e5cd1bb9fcf412.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-95ee1576bf2e4c44a626641a88e10091: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-95ee1576bf2e4c44a626641a88e10091.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-d1f8c15e427849ef9a44d948cee42fc1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-d1f8c15e427849ef9a44d948cee42fc1.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-f0fb36c65aa340238d045268676a7ea8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-f0fb36c65aa340238d045268676a7ea8.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-a6eb089cc5fe44aba425a94fdf837446: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-a6eb089cc5fe44aba425a94fdf837446.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-0e4c10b28f7d4febb0db26049d343318: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-0e4c10b28f7d4febb0db26049d343318.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-088f2ebf7aa74fb2a57a8f9c4bf864e6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-088f2ebf7aa74fb2a57a8f9c4bf864e6.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-100315ff67bf43cca0abda5e0a09482c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-100315ff67bf43cca0abda5e0a09482c.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-3fcde3f2aebf41d18ccc0713d1058f11: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-3fcde3f2aebf41d18ccc0713d1058f11.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-1a4325c509bc4ed99d49b41cb1a40ff8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-1a4325c509bc4ed99d49b41cb1a40ff8.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-9eef5efbd26c4a78a96d99cbf23937fe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-9eef5efbd26c4a78a96d99cbf23937fe.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-39e93c498f3a4888823693caf4881aea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-39e93c498f3a4888823693caf4881aea.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-b364ba92e6544a3f9a433db5494b8768: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-b364ba92e6544a3f9a433db5494b8768.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-d9b8f696d864409cba76b3d32bb13bec: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-d9b8f696d864409cba76b3d32bb13bec.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-cace7d2031b145a2a6db009ef56916a9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-cace7d2031b145a2a6db009ef56916a9.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-7573bfece41f490393467d3a3e2ec1d0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-7573bfece41f490393467d3a3e2ec1d0.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-ce79e904dd03487e87e8cc8195bf558a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-ce79e904dd03487e87e8cc8195bf558a.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-145b1db861e444fc8c7fba1ead17274f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-145b1db861e444fc8c7fba1ead17274f.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-1f743bad78fa4249a89a000b10468682: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-1f743bad78fa4249a89a000b10468682.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-eee73bccc2ea4ee0bf022bcf19f93d05: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-eee73bccc2ea4ee0bf022bcf19f93d05.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-9d52ac2c14e245ca96c0fb596d1bd615: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-9d52ac2c14e245ca96c0fb596d1bd615.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-690293ce0abd48a79c966b81f968fce7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-690293ce0abd48a79c966b81f968fce7.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-55db672e365640bb92a82f7934acc459: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-55db672e365640bb92a82f7934acc459.
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [logger.py:43] Received request chatcmpl-368ce554eb0a4111aac205f5ad487aa7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:52 [async_llm.py:270] Added request chatcmpl-368ce554eb0a4111aac205f5ad487aa7.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-55498f60dbd34ee098c81337a425e8a0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-55498f60dbd34ee098c81337a425e8a0.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [loggers.py:118] Engine 000: Avg prompt throughput: 792.7 tokens/s, Avg generation throughput: 3378.9 tokens/s, Running: 256 reqs, Waiting: 316 reqs, GPU KV cache usage: 68.7%, Prefix cache hit rate: 85.1%
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-22766dda256d4473854b6b3fe428a485: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-22766dda256d4473854b6b3fe428a485.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-b5370f9cf4bb44858b4a556bbf5f0e01: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-b5370f9cf4bb44858b4a556bbf5f0e01.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-f9b342a87ae9464899055e623a58179b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-f9b342a87ae9464899055e623a58179b.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-dc440a67bd5d4d69aceb6a9f0fcd9970: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-dc440a67bd5d4d69aceb6a9f0fcd9970.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-e8c45ada041b460397cbbf877a36718d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-e8c45ada041b460397cbbf877a36718d.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-6f722d77b639425eb000d4d201908139: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-6f722d77b639425eb000d4d201908139.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-86c26deb07f5436596a799d27962bc6a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-86c26deb07f5436596a799d27962bc6a.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-f9044361556d47ad813347ad926978c7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50148 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-f9044361556d47ad813347ad926978c7.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-947b41476e6046c581e24231985f76fa: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-947b41476e6046c581e24231985f76fa.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-bf0e73de9cba4553b1704fc771605dc3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-bf0e73de9cba4553b1704fc771605dc3.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-137a40cdc4454c099553f901a7d526b0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-137a40cdc4454c099553f901a7d526b0.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-b0931a03b1584edcbde0994ea4b8f980: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-b0931a03b1584edcbde0994ea4b8f980.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-29d480a6197943c59e4a198fb8a64e82: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-29d480a6197943c59e4a198fb8a64e82.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-8164872f311b4daabad7e3cbdd1edde8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-8164872f311b4daabad7e3cbdd1edde8.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-384a56233727419b9cde4017f403290c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-384a56233727419b9cde4017f403290c.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-5bbe3d5b3f004a6f82e3b855c25c6b57: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-5bbe3d5b3f004a6f82e3b855c25c6b57.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-02f0ae08ef0144209844e1dee16bb9ea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-02f0ae08ef0144209844e1dee16bb9ea.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-2777e738b4b54c27bc043bfb29fb5aec: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-2777e738b4b54c27bc043bfb29fb5aec.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-7753e7be1f2a4542aba78293da2950e4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-7753e7be1f2a4542aba78293da2950e4.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-73a2ebc5c66a4bbbb4d550c481f123c5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-73a2ebc5c66a4bbbb4d550c481f123c5.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-b215e090bf9640bc8abb9ae566a5bff4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-b215e090bf9640bc8abb9ae566a5bff4.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-8f9afbc29ad5411e9b93aa8bd1fa1e4f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-8f9afbc29ad5411e9b93aa8bd1fa1e4f.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-b6bbcc06b244478dbaed8bbeceb2dbca: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-b6bbcc06b244478dbaed8bbeceb2dbca.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-02bafa12f3844cf78327afa17d9f17f5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-02bafa12f3844cf78327afa17d9f17f5.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-4f30b4b076c44d7289c232ec21680b8a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-4f30b4b076c44d7289c232ec21680b8a.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-0032164a7dfa412da4144678529dd9ce: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-0032164a7dfa412da4144678529dd9ce.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-fa9af90071ca4f05900a343c527c6973: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [async_llm.py:270] Added request chatcmpl-fa9af90071ca4f05900a343c527c6973.
[36mllm_server_1  |[0m INFO 07-20 22:59:53 [logger.py:43] Received request chatcmpl-48cc25e59e9d42e68463ecf159b08e28: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-48cc25e59e9d42e68463ecf159b08e28.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-d7a3090a17dc41d7bebf48ab01a9c71d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-d7a3090a17dc41d7bebf48ab01a9c71d.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-850f4e2552e244b58ed202be2a0b1dfd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-850f4e2552e244b58ed202be2a0b1dfd.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-f52c156046764099a5ffa1feca648309: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-f52c156046764099a5ffa1feca648309.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-abf40900e7c2439eb44b64ce236c27ce: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-abf40900e7c2439eb44b64ce236c27ce.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-5b4ca9c17cf0469aae07594c419810e3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-5b4ca9c17cf0469aae07594c419810e3.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-e82bf45de6a245d99ede832816044ed2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-e82bf45de6a245d99ede832816044ed2.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-7c7ae6b4d93e443e8df9296c9bb70516: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-7c7ae6b4d93e443e8df9296c9bb70516.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-de99f8f55a344a7d872f98853d02b32f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-de99f8f55a344a7d872f98853d02b32f.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-210f3f93e077448ea32f0ad46eac89c5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-210f3f93e077448ea32f0ad46eac89c5.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-fb43916319e1481982b5c299ddb3bba9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-fb43916319e1481982b5c299ddb3bba9.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-af9ffbb0b3b246b8a98d3aa58cc9fa6e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50436 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-af9ffbb0b3b246b8a98d3aa58cc9fa6e.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-3a1b267dccf14eb8a3a94c1cff7055a5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-3a1b267dccf14eb8a3a94c1cff7055a5.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-0c326bbd8e0940de96721842179efbf5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-0c326bbd8e0940de96721842179efbf5.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-0dae8db1780a4cdc951cb101e881dd86: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-0dae8db1780a4cdc951cb101e881dd86.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-0e14f27446714923b6cd8ead720b191b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-0e14f27446714923b6cd8ead720b191b.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-bc3992d76a3e4403a8a5b5e38c08f549: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-bc3992d76a3e4403a8a5b5e38c08f549.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-8c3c14f220b6442bb18d6d0f35010d0a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-8c3c14f220b6442bb18d6d0f35010d0a.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-13ce80056fc046fcbffaf1ef3e98bc84: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-13ce80056fc046fcbffaf1ef3e98bc84.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-ca4bded814fb4d4682311e8f297f62a6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-ca4bded814fb4d4682311e8f297f62a6.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-e0c04c564cba409fb1055b3c251b2270: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-e0c04c564cba409fb1055b3c251b2270.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-93cdba7034624ba09e128cd039d242ab: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-93cdba7034624ba09e128cd039d242ab.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-7a0cea517c594459ae336e9e0483449b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-7a0cea517c594459ae336e9e0483449b.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-9a7b2e185b70413db11fefe8829dc157: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-9a7b2e185b70413db11fefe8829dc157.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-90bfbb4bfc454d65885b503e87c8829c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-90bfbb4bfc454d65885b503e87c8829c.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-f2b49ddc6d6345ceba7debdc2db66aa8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-f2b49ddc6d6345ceba7debdc2db66aa8.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-ac3ec5e0beac42308358659f5381f278: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-ac3ec5e0beac42308358659f5381f278.
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [logger.py:43] Received request chatcmpl-5ac7ead57cc847dbbad391962d9ae33c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:54 [async_llm.py:270] Added request chatcmpl-5ac7ead57cc847dbbad391962d9ae33c.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-9f9c74ad8dd346d6b96c8a1e7861269c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-9f9c74ad8dd346d6b96c8a1e7861269c.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-4b26232fe5264f78a2e817e8e551c3d2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-4b26232fe5264f78a2e817e8e551c3d2.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-da071cd55fe84d0e969d95286071d901: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-da071cd55fe84d0e969d95286071d901.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-27d9eff0501c416b8a1227cbc462d162: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-27d9eff0501c416b8a1227cbc462d162.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-ab3770b5806a4750b457b51405307958: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-ab3770b5806a4750b457b51405307958.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-2f38abed7b14462c984231bf9ec780a5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-2f38abed7b14462c984231bf9ec780a5.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-f64aefec1bcf488a8f8f17220948b267: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-f64aefec1bcf488a8f8f17220948b267.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-91e9c500d976482894e0df075d924273: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-91e9c500d976482894e0df075d924273.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-d5953893870e446c9a1a1bbd884d5530: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-d5953893870e446c9a1a1bbd884d5530.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-e394cd4eaf814aca9cf08384cc7099cb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-e394cd4eaf814aca9cf08384cc7099cb.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-dfb826db457c41e4945389f3642bcae7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-dfb826db457c41e4945389f3642bcae7.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-2688a8c041344e9cbec292eb2fa1ae52: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-2688a8c041344e9cbec292eb2fa1ae52.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-ad18fbcae403429b97022074afdfdfd4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-ad18fbcae403429b97022074afdfdfd4.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-4c97a5f88c2e4acc999de367c0bff24c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-4c97a5f88c2e4acc999de367c0bff24c.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-bba601debcda4009a33049d50165bb8b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-bba601debcda4009a33049d50165bb8b.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-ff88e26f130744f0afd7a2b0a7f68de1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-ff88e26f130744f0afd7a2b0a7f68de1.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-0d669c6b33f948079f1235b51624c0c5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-0d669c6b33f948079f1235b51624c0c5.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-98b9bca8a7024140becb4bbade630a7c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-98b9bca8a7024140becb4bbade630a7c.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-a8c172cdcd27413a98c3c6e9b883172e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-50574c08c49347c1b60aebeb53f9e970: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-a8c172cdcd27413a98c3c6e9b883172e.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-50574c08c49347c1b60aebeb53f9e970.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-31faea3e0d184f378136cbcf7a44f1e8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-31faea3e0d184f378136cbcf7a44f1e8.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-d5db0857e2b3498eaa0b4ce1b1a11b7f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-d5db0857e2b3498eaa0b4ce1b1a11b7f.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-ea0fe9564b6e4009b40e3b87ec1b5afc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-ea0fe9564b6e4009b40e3b87ec1b5afc.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-39ea89d87d84437e97b48c24ddfdad8a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-39ea89d87d84437e97b48c24ddfdad8a.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-d0d72c730dcd4aafa34efc05fe27e8b3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-d0d72c730dcd4aafa34efc05fe27e8b3.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-1785953c6d9c463680b274fa1d44073f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-1785953c6d9c463680b274fa1d44073f.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-5f6059887d7e4f0e9b77baa0f36f5a02: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-5f6059887d7e4f0e9b77baa0f36f5a02.
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [logger.py:43] Received request chatcmpl-2c0c17deb58246e3948da0ebff300eae: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:55 [async_llm.py:270] Added request chatcmpl-2c0c17deb58246e3948da0ebff300eae.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-41ac96119c1f47c8a32d46a92c39e751: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-41ac96119c1f47c8a32d46a92c39e751.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-95625501007f49d2b4f5291f8e6f1cb2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-95625501007f49d2b4f5291f8e6f1cb2.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-91071438696f4cc7ac48f1ee40c51275: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-91071438696f4cc7ac48f1ee40c51275.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-5ea15ccca8e64ddd83c9796f90ef4cee: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-5ea15ccca8e64ddd83c9796f90ef4cee.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-68526ed6d72b44ab8712946b3e854850: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-68526ed6d72b44ab8712946b3e854850.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-ccf64fb262dd45679eb0c2f49a892bf0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-ccf64fb262dd45679eb0c2f49a892bf0.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-6332807e768c462aa40b47f91d3ad7f7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-6332807e768c462aa40b47f91d3ad7f7.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-c9f81df39e294c788776a089e66de4f8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-c9f81df39e294c788776a089e66de4f8.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-369928b72e00416fa57bf4613e4c1c1b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-369928b72e00416fa57bf4613e4c1c1b.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-a7d59926cd7440cb908a2c65b321e8fd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-a7d59926cd7440cb908a2c65b321e8fd.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-c4e864333e8042f0a2476807dd1689a3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-c4e864333e8042f0a2476807dd1689a3.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-c463e24d32454459a3f3f52aa4ae1c08: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-c463e24d32454459a3f3f52aa4ae1c08.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-76030387d0b346f5b7db88d4ba117245: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-76030387d0b346f5b7db88d4ba117245.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-e5e512c625514224a2b33751e36ca8c8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:50998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-e5e512c625514224a2b33751e36ca8c8.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-399411d32cc04b19b69103965b477e0d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-399411d32cc04b19b69103965b477e0d.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-bcada5f2074c4293acba5081579a150b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-bcada5f2074c4293acba5081579a150b.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-05fd4e909b804c23881cfb05269c7ddc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-05fd4e909b804c23881cfb05269c7ddc.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-70238ea1805f49df9abb35b7546ada46: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-70238ea1805f49df9abb35b7546ada46.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-7c3f2889140b4239b0c09f0c8ec28fa4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-e648bcc6cbe44dd7b28fb6b9070c80cd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-7c3f2889140b4239b0c09f0c8ec28fa4.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-e648bcc6cbe44dd7b28fb6b9070c80cd.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-ec4a846b611d4da18d95b692984f41e7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-ec4a846b611d4da18d95b692984f41e7.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-e3130499ce66448496ddc7669a87d79a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-e3130499ce66448496ddc7669a87d79a.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-39111743e8d84c9e982169fc5defbc7c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-39111743e8d84c9e982169fc5defbc7c.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-e879d6fffe3a4bc7a53dc57d52d55de4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-e879d6fffe3a4bc7a53dc57d52d55de4.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-2e140a6524dd4bb3acc0d6214b864a6a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-2e140a6524dd4bb3acc0d6214b864a6a.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-130d42e612604a10add869ded6ee718a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-130d42e612604a10add869ded6ee718a.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-4c01fbd82baa4b7f8778bb520375fff6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-4c01fbd82baa4b7f8778bb520375fff6.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-c4079cd96e2b473a9d20e3b2e3fd2f2c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-c4079cd96e2b473a9d20e3b2e3fd2f2c.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-02cac0dd9e8e4acbbfbaf7fc5307084f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-02cac0dd9e8e4acbbfbaf7fc5307084f.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-8c5add02d1554f6183c9b67a861240b6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-8c5add02d1554f6183c9b67a861240b6.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-8731dad9fafe4a28a3edb53c00df3f7b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-8731dad9fafe4a28a3edb53c00df3f7b.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-dc304ca6dd92408cb7a785ca020e1181: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-dc304ca6dd92408cb7a785ca020e1181.
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [logger.py:43] Received request chatcmpl-aeb7ffc1dc0a432fbdb86563193bf93f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:56 [async_llm.py:270] Added request chatcmpl-aeb7ffc1dc0a432fbdb86563193bf93f.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-66319234f38844289d3cb4f7dd1dde37: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-66319234f38844289d3cb4f7dd1dde37.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-f0568e594cfe43fb864b69e90a5fe36e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-f0568e594cfe43fb864b69e90a5fe36e.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-4bc7240d92844fb29ad971ec09b0a359: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-4bc7240d92844fb29ad971ec09b0a359.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-3ce2b52602c74279a6170461aa586362: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-3ce2b52602c74279a6170461aa586362.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-b70ca10bfbd341eeb00d3647f64d6695: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-b70ca10bfbd341eeb00d3647f64d6695.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-1fd52ae91d4842f68591facb49ad43dc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-1fd52ae91d4842f68591facb49ad43dc.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-d032d82e784447bb8981e237ce47bcc2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-d032d82e784447bb8981e237ce47bcc2.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-0fafc8d25bed44c291790a865408bd7a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-0fafc8d25bed44c291790a865408bd7a.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-b544d6b2fa2048008b638d8b9d1237da: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-b544d6b2fa2048008b638d8b9d1237da.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-ee8c2517ff1840df9ea4ecc9e8a2bf02: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-ee8c2517ff1840df9ea4ecc9e8a2bf02.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-3d08d8a3928f41b4a80bfb59f3dec2f8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-3d08d8a3928f41b4a80bfb59f3dec2f8.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-2bfb5e3e6c8d468b894f69eec595611d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-2bfb5e3e6c8d468b894f69eec595611d.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-ddd122d61fe04477a32c038eaedee4be: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-ddd122d61fe04477a32c038eaedee4be.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-4226ccffbcf741c19924569ce0d22440: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-4226ccffbcf741c19924569ce0d22440.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-9e07436fd0cf4f0d82ae7675fee5ec7c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-9e07436fd0cf4f0d82ae7675fee5ec7c.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-68e7213d170b46bc8aa89a011aee4786: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-68e7213d170b46bc8aa89a011aee4786.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-b3c4bf981b7345689d89248ad0e74011: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-b3c4bf981b7345689d89248ad0e74011.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-4e71f917b88748c997c6a3e600a41473: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-4e71f917b88748c997c6a3e600a41473.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-4ea0c5675d084a0bb74a5b3065dac67d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-4ea0c5675d084a0bb74a5b3065dac67d.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-c232e82adcd346f39a0f9fb3ed62a0a0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-c232e82adcd346f39a0f9fb3ed62a0a0.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-648f5e9559ed48b89fd3308d55180a30: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-648f5e9559ed48b89fd3308d55180a30.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-43f9037e7e04444c88a85cc339cd5da8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-43f9037e7e04444c88a85cc339cd5da8.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-96f229e58fb840c1bfe8f891d9d811ed: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-96f229e58fb840c1bfe8f891d9d811ed.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-5fff9d57e36d4dd484db100e9c548534: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-5fff9d57e36d4dd484db100e9c548534.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-1d3ffc83ae1a43aea7d21a27cf2a2261: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-1d3ffc83ae1a43aea7d21a27cf2a2261.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-b1661665095749eda1d091a34854a7ec: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-b1661665095749eda1d091a34854a7ec.
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [logger.py:43] Received request chatcmpl-8e1bf9f4890140228c72f7bb91deaf13: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:57 [async_llm.py:270] Added request chatcmpl-8e1bf9f4890140228c72f7bb91deaf13.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-4fa2317e03b740d59442444ba30cf52f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-4fa2317e03b740d59442444ba30cf52f.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-28ebecb76b0a4ccf85f727681af596eb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-28ebecb76b0a4ccf85f727681af596eb.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-293a6db0d7df4505804a872345fc54aa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-293a6db0d7df4505804a872345fc54aa.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-d01ec9076fea4ddfa9473ff1db306f70: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-d01ec9076fea4ddfa9473ff1db306f70.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-8f841ce171e74684aad08d2c98f522a3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-8f841ce171e74684aad08d2c98f522a3.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-6be61a7025924447ba26c74b1c49bc5b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51436 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-6be61a7025924447ba26c74b1c49bc5b.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-7a7353ae69154871baa0a708e8565ba3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-7a7353ae69154871baa0a708e8565ba3.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-f3d919f95cc043e3a2022e9b720ec335: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-f3d919f95cc043e3a2022e9b720ec335.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-a9833c83a0bf45a8927c7fd37959f744: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-a9833c83a0bf45a8927c7fd37959f744.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-08ce002fde3a45de8f6a953bf14e5866: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-08ce002fde3a45de8f6a953bf14e5866.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-428d44359c104f14a14fb54c85dffe2a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-428d44359c104f14a14fb54c85dffe2a.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-aea200ffa752489a8dc60a97a0fb8f5b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-aea200ffa752489a8dc60a97a0fb8f5b.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-7e56060acf5a49d398104c85460d2c32: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-7e56060acf5a49d398104c85460d2c32.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-1e739400607943bcb57fa02b466836f0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-1e739400607943bcb57fa02b466836f0.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-b199945770e3473cb8f7b18e295b1a3e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-b199945770e3473cb8f7b18e295b1a3e.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-2d1b61f182424222b8ae27642fd5abba: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-2d1b61f182424222b8ae27642fd5abba.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-091b37a2ea52408e8e43aab2115d16f4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-091b37a2ea52408e8e43aab2115d16f4.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-a2cabe87b685436085c46fb5e0ac1ba7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-a2cabe87b685436085c46fb5e0ac1ba7.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-804f327abe164d87a9c0a714c54db5c6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-804f327abe164d87a9c0a714c54db5c6.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-2685aff650f1492a8f864a81056b1817: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-2685aff650f1492a8f864a81056b1817.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-445d71d7f2d24e1cb4779481466fd60c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-445d71d7f2d24e1cb4779481466fd60c.
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [logger.py:43] Received request chatcmpl-256763452a8d477bb384b25b4e4c1df1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:58 [async_llm.py:270] Added request chatcmpl-256763452a8d477bb384b25b4e4c1df1.
[36mllm_server_1  |[0m INFO 07-20 22:59:59 [logger.py:43] Received request chatcmpl-d2244be063e04777b4df85ed9b3a5f58: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:59 [async_llm.py:270] Added request chatcmpl-d2244be063e04777b4df85ed9b3a5f58.
[36mllm_server_1  |[0m INFO 07-20 22:59:59 [logger.py:43] Received request chatcmpl-ef60de57357344a9845d110e1224c570: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:59 [async_llm.py:270] Added request chatcmpl-ef60de57357344a9845d110e1224c570.
[36mllm_server_1  |[0m INFO 07-20 22:59:59 [logger.py:43] Received request chatcmpl-05750e6809584368ace6650ea3093cff: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:59 [async_llm.py:270] Added request chatcmpl-05750e6809584368ace6650ea3093cff.
[36mllm_server_1  |[0m INFO 07-20 22:59:59 [logger.py:43] Received request chatcmpl-e5ec5afb1de34c4f9a262980abd558cd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:59 [async_llm.py:270] Added request chatcmpl-e5ec5afb1de34c4f9a262980abd558cd.
[36mllm_server_1  |[0m INFO 07-20 22:59:59 [logger.py:43] Received request chatcmpl-229ee6cbcff74cd2ac79defb2d2415f4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:51604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 22:59:59 [async_llm.py:270] Added request chatcmpl-229ee6cbcff74cd2ac79defb2d2415f4.
[36mllm_server_1  |[0m INFO 07-20 23:00:03 [loggers.py:118] Engine 000: Avg prompt throughput: 561.6 tokens/s, Avg generation throughput: 3430.1 tokens/s, Running: 255 reqs, Waiting: 378 reqs, GPU KV cache usage: 81.0%, Prefix cache hit rate: 85.1%
[36mllm_server_1  |[0m INFO 07-20 23:00:13 [loggers.py:118] Engine 000: Avg prompt throughput: 739.7 tokens/s, Avg generation throughput: 3478.9 tokens/s, Running: 256 reqs, Waiting: 232 reqs, GPU KV cache usage: 76.1%, Prefix cache hit rate: 85.3%
[36mllm_server_1  |[0m INFO 07-20 23:00:23 [loggers.py:118] Engine 000: Avg prompt throughput: 602.0 tokens/s, Avg generation throughput: 3302.1 tokens/s, Running: 255 reqs, Waiting: 113 reqs, GPU KV cache usage: 80.6%, Prefix cache hit rate: 85.5%
[36mllm_server_1  |[0m INFO 07-20 23:00:33 [loggers.py:118] Engine 000: Avg prompt throughput: 580.7 tokens/s, Avg generation throughput: 3542.4 tokens/s, Running: 223 reqs, Waiting: 0 reqs, GPU KV cache usage: 76.0%, Prefix cache hit rate: 85.6%

Attaching to vllm_load_test_llm_server_1
[36mllm_server_1  |[0m INFO 07-20 23:01:08 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-20 23:01:13 [api_server.py:1395] vLLM API server version 0.9.2
[36mllm_server_1  |[0m INFO 07-20 23:01:13 [cli_args.py:325] non-default args: {'model': '/llm', 'max_model_len': 32768, 'served_model_name': ['qwen3']}
[36mllm_server_1  |[0m INFO 07-20 23:01:21 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
[36mllm_server_1  |[0m INFO 07-20 23:01:21 [config.py:1472] Using max model len 32768
[36mllm_server_1  |[0m INFO 07-20 23:01:21 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36mllm_server_1  |[0m INFO 07-20 23:01:27 [__init__.py:244] Automatically detected platform cuda.
[36mllm_server_1  |[0m INFO 07-20 23:01:30 [core.py:526] Waiting for init message from front-end.
[36mllm_server_1  |[0m INFO 07-20 23:01:30 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/llm', speculative_config=None, tokenizer='/llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36mllm_server_1  |[0m INFO 07-20 23:01:34 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36mllm_server_1  |[0m INFO 07-20 23:01:34 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36mllm_server_1  |[0m INFO 07-20 23:01:34 [gpu_model_runner.py:1770] Starting to load model /llm...
[36mllm_server_1  |[0m INFO 07-20 23:01:34 [gpu_model_runner.py:1775] Loading model from scratch...
[36mllm_server_1  |[0m INFO 07-20 23:01:34 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:06,  1.04it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:02<00:06,  1.01s/it]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:03<00:05,  1.03s/it]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.20it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.12it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:05<00:01,  1.06it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:06<00:00,  1.02it/s]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:07<00:00,  1.00s/it]
[36mllm_server_1  |[0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:07<00:00,  1.03it/s]
[36mllm_server_1  |[0m 
[36mllm_server_1  |[0m INFO 07-20 23:01:42 [default_loader.py:272] Loading weights took 7.80 seconds
[36mllm_server_1  |[0m INFO 07-20 23:01:44 [gpu_model_runner.py:1801] Model loading took 27.5185 GiB and 8.106918 seconds
[36mllm_server_1  |[0m INFO 07-20 23:01:55 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/33b1081f08/rank_0_0/backbone for vLLM's torch.compile
[36mllm_server_1  |[0m INFO 07-20 23:01:55 [backends.py:519] Dynamo bytecode transform time: 11.26 s
[36mllm_server_1  |[0m INFO 07-20 23:02:02 [backends.py:181] Cache the graph of shape None for later use
[36mllm_server_1  |[0m INFO 07-20 23:02:49 [backends.py:193] Compiling a graph for general shape takes 53.30 s
[36mllm_server_1  |[0m INFO 07-20 23:03:14 [monitor.py:34] torch.compile takes 64.55 s in total
[36mllm_server_1  |[0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36mllm_server_1  |[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36mllm_server_1  |[0m   warnings.warn(
[36mllm_server_1  |[0m INFO 07-20 23:03:16 [gpu_worker.py:232] Available KV cache memory: 7.30 GiB
[36mllm_server_1  |[0m INFO 07-20 23:03:17 [kv_cache_utils.py:716] GPU KV cache size: 47,840 tokens
[36mllm_server_1  |[0m INFO 07-20 23:03:17 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 1.46x
[36mllm_server_1  |[0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|â–         | 1/67 [00:00<00:35,  1.84it/s]Capturing CUDA graph shapes:   3%|â–Ž         | 2/67 [00:01<00:35,  1.84it/s]Capturing CUDA graph shapes:   4%|â–         | 3/67 [00:02<01:00,  1.05it/s]Capturing CUDA graph shapes:   6%|â–Œ         | 4/67 [00:03<00:49,  1.27it/s]Capturing CUDA graph shapes:   7%|â–‹         | 5/67 [00:03<00:42,  1.45it/s]Capturing CUDA graph shapes:   9%|â–‰         | 6/67 [00:04<00:56,  1.07it/s]Capturing CUDA graph shapes:  10%|â–ˆ         | 7/67 [00:05<00:48,  1.24it/s]Capturing CUDA graph shapes:  12%|â–ˆâ–        | 8/67 [00:06<00:41,  1.41it/s]Capturing CUDA graph shapes:  13%|â–ˆâ–Ž        | 9/67 [00:07<00:54,  1.06it/s]Capturing CUDA graph shapes:  15%|â–ˆâ–        | 10/67 [00:08<00:46,  1.21it/s]Capturing CUDA graph shapes:  16%|â–ˆâ–‹        | 11/67 [00:09<01:03,  1.14s/it]Capturing CUDA graph shapes:  18%|â–ˆâ–Š        | 12/67 [00:10<00:53,  1.03it/s]Capturing CUDA graph shapes:  19%|â–ˆâ–‰        | 13/67 [00:11<00:45,  1.19it/s]Capturing CUDA graph shapes:  21%|â–ˆâ–ˆ        | 14/67 [00:12<00:54,  1.03s/it]Capturing CUDA graph shapes:  22%|â–ˆâ–ˆâ–       | 15/67 [00:13<00:45,  1.13it/s]Capturing CUDA graph shapes:  24%|â–ˆâ–ˆâ–       | 16/67 [00:13<00:39,  1.30it/s]Capturing CUDA graph shapes:  25%|â–ˆâ–ˆâ–Œ       | 17/67 [00:14<00:48,  1.03it/s]Capturing CUDA graph shapes:  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:15<00:41,  1.19it/s]Capturing CUDA graph shapes:  28%|â–ˆâ–ˆâ–Š       | 19/67 [00:17<00:50,  1.05s/it]Capturing CUDA graph shapes:  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:17<00:47,  1.01s/it]Capturing CUDA graph shapes:  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:18<00:39,  1.17it/s]Capturing CUDA graph shapes:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 22/67 [00:20<00:48,  1.08s/it]Capturing CUDA graph shapes:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/67 [00:20<00:40,  1.09it/s]Capturing CUDA graph shapes:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:21<00:34,  1.24it/s]Capturing CUDA graph shapes:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:22<00:42,  1.01s/it]Capturing CUDA graph shapes:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 26/67 [00:23<00:35,  1.16it/s]Capturing CUDA graph shapes:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:23<00:30,  1.30it/s]Capturing CUDA graph shapes:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/67 [00:25<00:37,  1.03it/s]Capturing CUDA graph shapes:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/67 [00:25<00:31,  1.19it/s]Capturing CUDA graph shapes:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:26<00:27,  1.33it/s]Capturing CUDA graph shapes:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 31/67 [00:27<00:34,  1.03it/s]Capturing CUDA graph shapes:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 32/67 [00:28<00:29,  1.19it/s]Capturing CUDA graph shapes:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:28<00:25,  1.35it/s]Capturing CUDA graph shapes:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/67 [00:30<00:32,  1.03it/s]Capturing CUDA graph shapes:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/67 [00:30<00:26,  1.20it/s]Capturing CUDA graph shapes:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:31<00:22,  1.35it/s]Capturing CUDA graph shapes:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 37/67 [00:32<00:29,  1.03it/s]Capturing CUDA graph shapes:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 38/67 [00:33<00:24,  1.19it/s]Capturing CUDA graph shapes:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:33<00:20,  1.36it/s]Capturing CUDA graph shapes:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [00:35<00:26,  1.03it/s]Capturing CUDA graph shapes:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/67 [00:35<00:21,  1.20it/s]Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:36<00:18,  1.34it/s]Capturing CUDA graph shapes:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/67 [00:37<00:23,  1.03it/s]Capturing CUDA graph shapes:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 44/67 [00:38<00:19,  1.20it/s]Capturing CUDA graph shapes:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:38<00:16,  1.33it/s]Capturing CUDA graph shapes:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/67 [00:40<00:20,  1.03it/s]Capturing CUDA graph shapes:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 47/67 [00:40<00:16,  1.19it/s]Capturing CUDA graph shapes:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:41<00:14,  1.33it/s]Capturing CUDA graph shapes:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 49/67 [00:43<00:17,  1.03it/s]Capturing CUDA graph shapes:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:43<00:14,  1.20it/s]Capturing CUDA graph shapes:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:44<00:11,  1.35it/s]Capturing CUDA graph shapes:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 52/67 [00:45<00:14,  1.02it/s]Capturing CUDA graph shapes:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 53/67 [00:46<00:11,  1.18it/s]Capturing CUDA graph shapes:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:46<00:09,  1.33it/s]Capturing CUDA graph shapes:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/67 [00:48<00:11,  1.03it/s]Capturing CUDA graph shapes:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 56/67 [00:48<00:09,  1.20it/s]Capturing CUDA graph shapes:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:49<00:07,  1.38it/s]Capturing CUDA graph shapes:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 58/67 [00:50<00:08,  1.01it/s]Capturing CUDA graph shapes:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 59/67 [00:51<00:06,  1.18it/s]Capturing CUDA graph shapes:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:51<00:05,  1.33it/s]Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 61/67 [00:53<00:05,  1.02it/s]Capturing CUDA graph shapes:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 62/67 [00:53<00:04,  1.20it/s]Capturing CUDA graph shapes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:54<00:02,  1.37it/s]Capturing CUDA graph shapes:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:55<00:02,  1.02it/s]Capturing CUDA graph shapes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 65/67 [00:56<00:01,  1.19it/s]Capturing CUDA graph shapes:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:56<00:00,  1.37it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:58<00:00,  1.00s/it]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:58<00:00,  1.15it/s]
[36mllm_server_1  |[0m INFO 07-20 23:04:16 [gpu_model_runner.py:2326] Graph capturing finished in 58 secs, took 0.71 GiB
[36mllm_server_1  |[0m INFO 07-20 23:04:16 [core.py:172] init engine (profile, create kv cache, warmup model) took 152.28 seconds
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 2990
[36mllm_server_1  |[0m WARNING 07-20 23:04:17 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:29] Available routes are:
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /docs, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /health, Methods: GET
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /load, Methods: GET
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /ping, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /ping, Methods: GET
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /tokenize, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /detokenize, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /v1/models, Methods: GET
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /version, Methods: GET
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /v1/completions, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /v1/embeddings, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /pooling, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /classify, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /score, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /v1/score, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /v1/audio/translations, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /v1/rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /v2/rerank, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /invocations, Methods: POST
[36mllm_server_1  |[0m INFO 07-20 23:04:17 [launcher.py:37] Route: /metrics, Methods: GET
[36mllm_server_1  |[0m INFO:     Started server process [7]
[36mllm_server_1  |[0m INFO:     Waiting for application startup.
[36mllm_server_1  |[0m INFO:     Application startup complete.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59550 - "GET /health HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:32 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36mllm_server_1  |[0m INFO 07-20 23:04:32 [logger.py:43] Received request chatcmpl-9c40384b415a4535a1aab561f8a8ff39: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:56182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:32 [async_llm.py:270] Added request chatcmpl-9c40384b415a4535a1aab561f8a8ff39.
[36mllm_server_1  |[0m INFO 07-20 23:04:37 [loggers.py:118] Engine 000: Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 22.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-59d3fb598b8b4ddead80c9c62966e2d6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-59d3fb598b8b4ddead80c9c62966e2d6.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-bff0123e83064708a43d3c697d567994: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-bff0123e83064708a43d3c697d567994.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-c947c17388ed4a198b72d7085e081a51: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-c947c17388ed4a198b72d7085e081a51.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-5aceef06a3884c3789eb3ff978257908: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-5aceef06a3884c3789eb3ff978257908.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-c65bc3d1ee2e4ae3831b972e3fcc7cec: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-c65bc3d1ee2e4ae3831b972e3fcc7cec.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-f2a8dd5e557b4ff98631df4c3b5b067d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-f2a8dd5e557b4ff98631df4c3b5b067d.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-1d3eac2353514c89af09ef07b6249d60: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-1d3eac2353514c89af09ef07b6249d60.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-2a2e0289b0334ad7b0aaeb8b1dcaa2e6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-2a2e0289b0334ad7b0aaeb8b1dcaa2e6.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-5f73385ab2d04117ac5a617a562288d2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-5f73385ab2d04117ac5a617a562288d2.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-72abd9287b794593a27ce8b0358636da: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-72abd9287b794593a27ce8b0358636da.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-4dcf55cd460e4b209d8066a614311dbc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-4dcf55cd460e4b209d8066a614311dbc.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-cfde969c90f34214a3814e9895a5b797: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-cfde969c90f34214a3814e9895a5b797.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-b94f8644f0c648378ea5de2fa2f7b5d7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-b94f8644f0c648378ea5de2fa2f7b5d7.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-11a4a32973de42108d38f2bfb5f3bcb6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-11a4a32973de42108d38f2bfb5f3bcb6.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-fce904020a624616ad4421a99d67246a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-fce904020a624616ad4421a99d67246a.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-ae7ceb07171845bb929ae77b4dd64522: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-560ba590901b4319b1c37fb8005c835e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45980 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-ae7ceb07171845bb929ae77b4dd64522.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-0391f0ff1f864af9a9d3f7819f72f6af: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:45996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-560ba590901b4319b1c37fb8005c835e.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-0391f0ff1f864af9a9d3f7819f72f6af.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-12279ef354454da1b77a632199551e93: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-12279ef354454da1b77a632199551e93.
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [logger.py:43] Received request chatcmpl-0d18c048730d44d8af6b1cd882de57c4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:38 [async_llm.py:270] Added request chatcmpl-0d18c048730d44d8af6b1cd882de57c4.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-af67827f09004e7a9bddee5fde91efd2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-af67827f09004e7a9bddee5fde91efd2.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-331e571cb62348ecbd5b60395a8a4bc0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46040 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-331e571cb62348ecbd5b60395a8a4bc0.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-0d08546b083f40d5ad45e6c31ffe58fe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-0d08546b083f40d5ad45e6c31ffe58fe.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-567924e051d9453c9902faee7682d41b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-567924e051d9453c9902faee7682d41b.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-d1f2459fb206405aaa4040401b168bc1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-d1f2459fb206405aaa4040401b168bc1.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-e62d1a35e8604d99867e9eed396254ad: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-e62d1a35e8604d99867e9eed396254ad.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-8586408a5b514907bcf1145ff1eb13ee: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-8586408a5b514907bcf1145ff1eb13ee.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-83818b647c0645b280e5ba1c7159063a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-83818b647c0645b280e5ba1c7159063a.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-7c86b5d8d9234e00a29e22b9cae97ede: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-7c86b5d8d9234e00a29e22b9cae97ede.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-3497caaaf1da400ca3b8d70d8dcc4dfe: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46108 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-3497caaaf1da400ca3b8d70d8dcc4dfe.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-1f1dc50797ed49b5b58bb807b46b9bac: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-1f1dc50797ed49b5b58bb807b46b9bac.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-4a374a5915ea4c3e95ae2affa5f633a0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-4a374a5915ea4c3e95ae2affa5f633a0.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-ccfd95c53a7d4a8e9e92618b7134699e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-ccfd95c53a7d4a8e9e92618b7134699e.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-08b6bc5be0564759b1af5ddb153c4366: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-08b6bc5be0564759b1af5ddb153c4366.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-d411909a092e4078bb0d882645d58b67: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-d411909a092e4078bb0d882645d58b67.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-a9f092bfee734a5d8dfcdf0527467809: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-a9f092bfee734a5d8dfcdf0527467809.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-294ff197ceab4738b449a01a324fac9c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-294ff197ceab4738b449a01a324fac9c.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-20d41d634ff24fc999674f1d67df0fd7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-20d41d634ff24fc999674f1d67df0fd7.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-d1e2189ea74d4271acb7fb715f278c10: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-d1e2189ea74d4271acb7fb715f278c10.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-f07e859ab3a3475e86d4a65bc3e1ae64: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-f07e859ab3a3475e86d4a65bc3e1ae64.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-25cc64f3748246d6bb4e83704fcfb407: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-25cc64f3748246d6bb4e83704fcfb407.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-81be09b94c9f4fd4adc42c219b1a2ee6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-81be09b94c9f4fd4adc42c219b1a2ee6.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-f1baf0a10cdd4f4c9f822f5bdb47eb08: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-f1baf0a10cdd4f4c9f822f5bdb47eb08.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-ac1ebde6fda54cdb93509450e1f0b7b7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-ac1ebde6fda54cdb93509450e1f0b7b7.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-130490a3d2e942088a8ff5b200e61f0a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-130490a3d2e942088a8ff5b200e61f0a.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-1fc377b70be641f39787d82e118cef1c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-1fc377b70be641f39787d82e118cef1c.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-7ebc6d6c2cc149d1a98daa66fdc1890f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-7ebc6d6c2cc149d1a98daa66fdc1890f.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-85fddca04ecf4bcd946eb9be308bc1cf: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46260 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-85fddca04ecf4bcd946eb9be308bc1cf.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-389732bec39b4bdcb43a441a6be6aba1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-389732bec39b4bdcb43a441a6be6aba1.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-5d81641f68084f40b4a78ce824634b9b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-5d81641f68084f40b4a78ce824634b9b.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-6180159844aa4d10a46b5667fd371972: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-6180159844aa4d10a46b5667fd371972.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-2716a11e33c147398647f82b51e195b4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46280 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-2716a11e33c147398647f82b51e195b4.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-868489cf40114978b0ca3ee41fc263d5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-868489cf40114978b0ca3ee41fc263d5.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-2b26cfdfe9644f6d82cda1b2a85be869: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-2b26cfdfe9644f6d82cda1b2a85be869.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-8af9d40ceb674de29ce6e206d0a4d9e2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-8af9d40ceb674de29ce6e206d0a4d9e2.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-68b1f3d1979745e7baabbb5e726f43c0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-3b9c101aa58f4bf6a2d61dea8abc86ea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-68b1f3d1979745e7baabbb5e726f43c0.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-3b9c101aa58f4bf6a2d61dea8abc86ea.
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [logger.py:43] Received request chatcmpl-b18624d7a1d64431b73c66a6c400194f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:39 [async_llm.py:270] Added request chatcmpl-b18624d7a1d64431b73c66a6c400194f.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-256962ec295d48b49dd8c8730e23ffd8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-256962ec295d48b49dd8c8730e23ffd8.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-bc0b6cdf7e6544999e385a8165709b05: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-bc0b6cdf7e6544999e385a8165709b05.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-beb65e29c7ba4ba38a71c48d72a4b26b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-beb65e29c7ba4ba38a71c48d72a4b26b.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-25aaaf352ac840b68c2358d84b32894a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-25aaaf352ac840b68c2358d84b32894a.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-f65642e1c2f642e5b57751fa73037c3d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-f65642e1c2f642e5b57751fa73037c3d.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-71fe50ea4a384ba78831f2a19d75ebe9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-71fe50ea4a384ba78831f2a19d75ebe9.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-4e11e71a8225463f960b54ab240ff723: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-4e11e71a8225463f960b54ab240ff723.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-9a44ec6dad524694a8e1a1adc972a7ba: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-9a44ec6dad524694a8e1a1adc972a7ba.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-ef61daa4187b455682cdb624b53693a4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-ef61daa4187b455682cdb624b53693a4.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-fce07e52ad74469d9cdc72f567db2a32: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-fce07e52ad74469d9cdc72f567db2a32.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-0f2b71743734416f88ddd70857b0ca9c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-0f2b71743734416f88ddd70857b0ca9c.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-8fb352e260784562b7448e1722c08d28: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-8fb352e260784562b7448e1722c08d28.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-b93e8c37fc444a27bfc572cd9f8ada4d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-b93e8c37fc444a27bfc572cd9f8ada4d.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-4a0c92800f4642479afa4652b26ae3a7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-4a0c92800f4642479afa4652b26ae3a7.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-9cd0478f1d0141458802bb8d6728a0d7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-9cd0478f1d0141458802bb8d6728a0d7.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-3fab18e60ad84793b8a37835a0f97ec2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-3fab18e60ad84793b8a37835a0f97ec2.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-4a2f1222160844d98a4f921653d802fe: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-4a2f1222160844d98a4f921653d802fe.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-6a58cf1755484a16b497383bf1e22444: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-6a58cf1755484a16b497383bf1e22444.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-948bcf4d16534cf2a84e189b4a53094a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-948bcf4d16534cf2a84e189b4a53094a.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-73c9a57a7a6b450cb6d7566740fe2aad: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-73c9a57a7a6b450cb6d7566740fe2aad.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-53753f8d053b47f2819c333c262fa91e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-53753f8d053b47f2819c333c262fa91e.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-1b0e11cde79b491ab70cb67ab441cee6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46534 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-1b0e11cde79b491ab70cb67ab441cee6.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-b3b3f74ea191491695ec0d9eb95f7357: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-b3b3f74ea191491695ec0d9eb95f7357.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-00cb883bb7344dc3ae0e0404f89ad7dc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-00cb883bb7344dc3ae0e0404f89ad7dc.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-73bb97ca76a240d1a18988e1a552287b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-73bb97ca76a240d1a18988e1a552287b.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-fa140fe053b147a0bee138665fe59010: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-fa140fe053b147a0bee138665fe59010.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-2fee6083721c4823a9ac1c870b151c19: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-ef2ba3af88e64d8cb61c92ca039dfc66: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-2fee6083721c4823a9ac1c870b151c19.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-1a53e4bc30fc4582830c48c2b30bc8a5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-882fd7ea91cb4546a70d466bffa0e17b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-ef2ba3af88e64d8cb61c92ca039dfc66.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-1a53e4bc30fc4582830c48c2b30bc8a5.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-0ed679c6fa444a4a9e122be457cbde8c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-882fd7ea91cb4546a70d466bffa0e17b.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-0ed679c6fa444a4a9e122be457cbde8c.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-d56682d491134b5781925c1a6dd842a2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-d56682d491134b5781925c1a6dd842a2.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-6f043af7c9af4517a2a4084b9314c608: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-6f043af7c9af4517a2a4084b9314c608.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-9aaf597ef74f40ec9d8fcc489a7ef8c4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-9aaf597ef74f40ec9d8fcc489a7ef8c4.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-02ffe634be2143269abeb8d243bf0275: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-02ffe634be2143269abeb8d243bf0275.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-ec9b927ae24047d4aa4ab7a7eb27f719: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-ec9b927ae24047d4aa4ab7a7eb27f719.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-8080877f1dad47208b8b1348c33e0b21: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-8080877f1dad47208b8b1348c33e0b21.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-e261f2d6832f422eba80be3e52677013: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-e261f2d6832f422eba80be3e52677013.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-577349c2073c4b4fb1776d47b8dd55ba: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-577349c2073c4b4fb1776d47b8dd55ba.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-b10fa08321a840da93ca695e9802f78d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-b10fa08321a840da93ca695e9802f78d.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-6c48d2310e0e4bfca566748383b46adf: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-6c48d2310e0e4bfca566748383b46adf.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-8888b5908baf4fb7860e3b1c02789ee5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-8888b5908baf4fb7860e3b1c02789ee5.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-757aee8ee06e4f77aba7ca46a7d20951: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-757aee8ee06e4f77aba7ca46a7d20951.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-3c9051d505b8479c8198eaba3450714f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-3c9051d505b8479c8198eaba3450714f.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-cb4d1d16f7bf40e5a23a58ab47d0f5dd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-cb4d1d16f7bf40e5a23a58ab47d0f5dd.
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [logger.py:43] Received request chatcmpl-cb6a798cf3a44c1f815c522858d994f2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:40 [async_llm.py:270] Added request chatcmpl-cb6a798cf3a44c1f815c522858d994f2.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-8f79b8a7210e48adafea5c4a2a28f176: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-8f79b8a7210e48adafea5c4a2a28f176.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-858ae28c246d446fb2972b039ac9dc72: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-858ae28c246d446fb2972b039ac9dc72.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-e29a748413ab4902a6a586e88392944d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-e29a748413ab4902a6a586e88392944d.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-930cc0301ea34f939b9da8c4a3b1babd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-930cc0301ea34f939b9da8c4a3b1babd.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-27f390b90cb54a25a363e3b41866c878: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-27f390b90cb54a25a363e3b41866c878.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-3f23974784a94be69e3bcfad9f152182: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-3f23974784a94be69e3bcfad9f152182.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-a83c00b4c66f48fe9d6c1cdfe5da68cd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-a83c00b4c66f48fe9d6c1cdfe5da68cd.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-5a52ee0ff5dd4c56827ad0003e91e721: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-5a52ee0ff5dd4c56827ad0003e91e721.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-c68279c618414a0eab6de95a0c464208: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-c68279c618414a0eab6de95a0c464208.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-5b435fcf0917445fb7fb319264b17ba7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46830 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-5b435fcf0917445fb7fb319264b17ba7.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-11774cce438640149bfd0ead39646587: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-11774cce438640149bfd0ead39646587.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-9f8539360ca1481190ab2e6cff28c0d5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-9f8539360ca1481190ab2e6cff28c0d5.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-7cfa2e4f835c4818a1c322159c46c62e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-7cfa2e4f835c4818a1c322159c46c62e.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-ac3113a203294a6ca6329b35ab1ee730: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-ac3113a203294a6ca6329b35ab1ee730.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-c52c4943bca44030af08332786afa379: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-c52c4943bca44030af08332786afa379.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-8f8515d624c740b9b2439aa1ff8f70d9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46888 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-8f8515d624c740b9b2439aa1ff8f70d9.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-40d2b488774a40c28d4969cb4c8ff6ab: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-40d2b488774a40c28d4969cb4c8ff6ab.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-f758db05c14e43f381654752e700f3b6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-f758db05c14e43f381654752e700f3b6.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-1eed389a92be46e39ee5512be44caa22: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46900 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-1eed389a92be46e39ee5512be44caa22.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-34d6fbf85cd2472a958358fcb77087f3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-34d6fbf85cd2472a958358fcb77087f3.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-5166b30a1c454df69a391e1f01262e3f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-5166b30a1c454df69a391e1f01262e3f.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-8437d5caeb1c4343a904c704de135389: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-8437d5caeb1c4343a904c704de135389.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-25d7018ee1134a7da6ee4075ee1ad2dc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-25d7018ee1134a7da6ee4075ee1ad2dc.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-80a535efb45944ac8ab544553af8dde8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-80a535efb45944ac8ab544553af8dde8.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-254d63891c754431ac4538e2cd7d22c0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-254d63891c754431ac4538e2cd7d22c0.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-50eef56ca9af418d9e1eff2bf95ad27f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-50eef56ca9af418d9e1eff2bf95ad27f.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-37359b9713a44fae902d1d05c466e274: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-37359b9713a44fae902d1d05c466e274.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-d51c63199cdf4f05a8dd976ebe6d9075: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-d51c63199cdf4f05a8dd976ebe6d9075.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-39016aef61ce4e1490bd7996c2702703: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:46996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-39016aef61ce4e1490bd7996c2702703.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-67fb2239e4714d6da5def5918dae8c21: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-e927d7b43a3f439e82b1a0400139cbe7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-67fb2239e4714d6da5def5918dae8c21.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-e927d7b43a3f439e82b1a0400139cbe7.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-519a194e5a02465eb95eb0a01e17b41c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-519a194e5a02465eb95eb0a01e17b41c.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-5937940f72ac432fa78fa84e8870941d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-5937940f72ac432fa78fa84e8870941d.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-68d0a1f0ea0a4baeb3310738ae9bbda1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-68d0a1f0ea0a4baeb3310738ae9bbda1.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-f5fbfbfd79b64041a410f6ead5b43cb5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-f5fbfbfd79b64041a410f6ead5b43cb5.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-8ac2e717a7934c3391604ffae3bcb14d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-8ac2e717a7934c3391604ffae3bcb14d.
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [logger.py:43] Received request chatcmpl-bfea479dec2d49bf9808741c1ed581ca: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:41 [async_llm.py:270] Added request chatcmpl-bfea479dec2d49bf9808741c1ed581ca.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-85f961304f6d4e3090ac26ab1fbe83e9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-85f961304f6d4e3090ac26ab1fbe83e9.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-c8ea839701e74b429d88a32e254af4e2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-c8ea839701e74b429d88a32e254af4e2.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-19119fcad58141848bbb174332e1eaf0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-19119fcad58141848bbb174332e1eaf0.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-3be2a31805ff422aa60d236a558d2931: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-3be2a31805ff422aa60d236a558d2931.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-740586f73ff64d50a82c7a997cb6e223: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-740586f73ff64d50a82c7a997cb6e223.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-3cb2b8cadcc64f09a1487577e0541c2b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-3cb2b8cadcc64f09a1487577e0541c2b.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-41bcea10b5094278b588053a7d17fc7d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-41bcea10b5094278b588053a7d17fc7d.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-82527efaab6b4196a79cb15d975483be: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-82527efaab6b4196a79cb15d975483be.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-6c62316088aa4a6fa15371f993e27a87: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-6c62316088aa4a6fa15371f993e27a87.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-4fa0a9c9b3cc4e16933eca9a5c945757: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-4fa0a9c9b3cc4e16933eca9a5c945757.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-1c2e4d311ee14b9ea5ea2846367771ef: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-1c2e4d311ee14b9ea5ea2846367771ef.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-f7fff9c195294e9a85ae572a850ebaeb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-f7fff9c195294e9a85ae572a850ebaeb.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-37adc824c7a64d8dad75a84747bd9676: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-37adc824c7a64d8dad75a84747bd9676.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-9831637821a44f6e814eed0ed98be56b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-9831637821a44f6e814eed0ed98be56b.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-2f33a6a91da74a8aa25b7e5f0e1c487f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-2f33a6a91da74a8aa25b7e5f0e1c487f.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-340555b5745a462dbba56cdbeb17bff9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-340555b5745a462dbba56cdbeb17bff9.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-17a1b180cd734c4ba2daca8e6e24f160: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-17a1b180cd734c4ba2daca8e6e24f160.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-914c11c327a840e18de0d4bdeab0235b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-914c11c327a840e18de0d4bdeab0235b.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-db550957e31f4b9d879ecd06acda4516: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-db550957e31f4b9d879ecd06acda4516.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-9289dca4e01b496c916f2ee9206cb59a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-9289dca4e01b496c916f2ee9206cb59a.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-2f8917d4635b4e3782fd05d4bfa7d06a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-2f8917d4635b4e3782fd05d4bfa7d06a.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-361135118ef84a289867d73b745bcdb8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-361135118ef84a289867d73b745bcdb8.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-ab50313ae5674f9783135db3dad99e5c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-ab50313ae5674f9783135db3dad99e5c.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-0c0af59d29324ef58703c8a483b5db9e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-0c0af59d29324ef58703c8a483b5db9e.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-51271604fa1349c7b2667b3a0b6c3577: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:47248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-51271604fa1349c7b2667b3a0b6c3577.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-fb5e30d584984a2ead1e8ed36cb757ab: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-3eb5d2abaf354a34a193da3acb5172d9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-fb5e30d584984a2ead1e8ed36cb757ab.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-3eb5d2abaf354a34a193da3acb5172d9.
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [logger.py:43] Received request chatcmpl-1d9e1fe83bc847d292b1addd0de601b1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:42 [async_llm.py:270] Added request chatcmpl-1d9e1fe83bc847d292b1addd0de601b1.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-5b12a7bff188418db767068239a915f3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-5b12a7bff188418db767068239a915f3.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-bbceaec8013047d7bedba227893c26c5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-bbceaec8013047d7bedba227893c26c5.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-927de5a17e1144958abe06c42adfb514: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-dd076e7b931b401e8d48347976d33034: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-b3dcb7a04135491f92c97204a237c793: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-927de5a17e1144958abe06c42adfb514.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-d9c818ee4c694ae492898741903fb87e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-dd076e7b931b401e8d48347976d33034.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-b3dcb7a04135491f92c97204a237c793.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-d9c818ee4c694ae492898741903fb87e.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-294b21ab9a004beba7545f29a9f6c183: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-171460d8197746ff823431c122a7a753: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-294b21ab9a004beba7545f29a9f6c183.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-171460d8197746ff823431c122a7a753.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-cdb1972b34c94625baf7c4670fe08b46: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-cdb1972b34c94625baf7c4670fe08b46.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-ab96a2c3a8d141d2a562e81d881559ce: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-ab96a2c3a8d141d2a562e81d881559ce.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-8813ed9378a94ceaafff0bbc5dc74604: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-1c01714a8e89473495311bdf081af4b6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-8813ed9378a94ceaafff0bbc5dc74604.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-1c01714a8e89473495311bdf081af4b6.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-3dd196c58b3e4522b52f966d435684ba: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-3dd196c58b3e4522b52f966d435684ba.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-d7018c70b56547e9b685744ee5efc3b7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-d7018c70b56547e9b685744ee5efc3b7.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-f79f72883363425cbb69ea4e7013ea96: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-f79f72883363425cbb69ea4e7013ea96.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-8eacea125bdc4179b1defa0d4fa26143: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-8eacea125bdc4179b1defa0d4fa26143.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-483648219fa444b086708ca44a66fb42: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-483648219fa444b086708ca44a66fb42.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-b351d6d64d6f403ab5ff0713e87d4586: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-b351d6d64d6f403ab5ff0713e87d4586.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-7b4419dddbd34534b54d48fd5fa615c4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-7b4419dddbd34534b54d48fd5fa615c4.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-a62b23ec86cf45cb80be6b9514918a68: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59900 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-a62b23ec86cf45cb80be6b9514918a68.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-5e04bfe2d37549c8ba66fcd0b951a241: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-5e04bfe2d37549c8ba66fcd0b951a241.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-10ca1f911b4d427d8f0733a26500b080: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-10ca1f911b4d427d8f0733a26500b080.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-21f9bc1acb544c91a2458e7eb9a5937f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-21f9bc1acb544c91a2458e7eb9a5937f.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-26212f458f56419587a12628e1bc11fb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-5502dafe271c4e50965a8189085afecc: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-26212f458f56419587a12628e1bc11fb.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-5502dafe271c4e50965a8189085afecc.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-a2442a62d6584b51a9b8970118c33403: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-a2442a62d6584b51a9b8970118c33403.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-41f17d4d63eb4845892c7c70e25fb02f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-41f17d4d63eb4845892c7c70e25fb02f.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-c12acc1108834b6ea68d0db7c36e38d9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-c12acc1108834b6ea68d0db7c36e38d9.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-c9171764c138403d905fa0c13dfbe8a5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-8fede2860fb740bcab35d5d701136277: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59980 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-c9171764c138403d905fa0c13dfbe8a5.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-8fede2860fb740bcab35d5d701136277.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-a919cc57f51f4184aaa207d97cff2148: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-a919cc57f51f4184aaa207d97cff2148.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-e6638fffcefb4300951192ac3e21dd9e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-e6638fffcefb4300951192ac3e21dd9e.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-1b88f76c78a44afc8985f62027201b95: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-1b88f76c78a44afc8985f62027201b95.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-9ee0370315d94f4299385cff5f3d85d7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60014 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-9ee0370315d94f4299385cff5f3d85d7.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-96c3370bf34748f89cf3a80927a15741: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-96c3370bf34748f89cf3a80927a15741.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-b817bdd4906c4fe882318949c81a39dc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-b817bdd4906c4fe882318949c81a39dc.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-64a160be7426428a8c58dc4e9a6ed9a3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-64a160be7426428a8c58dc4e9a6ed9a3.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-f2c5efca34864cf4a4a42c7eb1590356: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-f2c5efca34864cf4a4a42c7eb1590356.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-d6d32a8e28e14fddb7745285f6678e22: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-d6d32a8e28e14fddb7745285f6678e22.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-bca6944ba8f84300b215446c8a068561: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-bca6944ba8f84300b215446c8a068561.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-0fcd2b03c0e842c6b7736b658fe94fe7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-0fcd2b03c0e842c6b7736b658fe94fe7.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-ce93ca8798704cb9a993bba011891ad4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-ce93ca8798704cb9a993bba011891ad4.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-cf8368d57f674f1ca0e201a7fb1788ec: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-cf8368d57f674f1ca0e201a7fb1788ec.
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [logger.py:43] Received request chatcmpl-708277b28f6044f791c430d5edade420: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:43 [async_llm.py:270] Added request chatcmpl-708277b28f6044f791c430d5edade420.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-d4a9c0700e664cce80762d0e37bf426c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-ac34bb0073a44748b5693f7a0ab32288: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-54e1036a5d0f4cd7a1bae60033886820: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-d4a9c0700e664cce80762d0e37bf426c.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-5d5403f371e3489baa844ecf55e02364: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-ac34bb0073a44748b5693f7a0ab32288.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-61bf1736f1f3448aa38dea5f368d1852: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-3ce3a90357004ab7a6b1fe904ebd6de5: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-54e1036a5d0f4cd7a1bae60033886820.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-5d5403f371e3489baa844ecf55e02364.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-61bf1736f1f3448aa38dea5f368d1852.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60148 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-3ce3a90357004ab7a6b1fe904ebd6de5.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-d3c2eb7ef286410f88d43696a7e74764: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-d3c2eb7ef286410f88d43696a7e74764.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-de845cafd1474ac9a0a3c4ed0075adc3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-de845cafd1474ac9a0a3c4ed0075adc3.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-a6f510706a8749dab33ea43cd640a36e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-a6f510706a8749dab33ea43cd640a36e.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-fefbc8b27d4c46e8bb6e53f3cb2b9df0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-fefbc8b27d4c46e8bb6e53f3cb2b9df0.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-113b1f5d42ee4fcbb3dcb95b0f1da457: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-113b1f5d42ee4fcbb3dcb95b0f1da457.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-d4f979773a774108b477b9a150e3da59: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-d4f979773a774108b477b9a150e3da59.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-ad789dd9d0c548a9b5b1183cb48ca252: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-ad789dd9d0c548a9b5b1183cb48ca252.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-92a92bcedba04317aca218810dcff239: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60224 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-92a92bcedba04317aca218810dcff239.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-3c512aec9dea40d0bc22d0540b2e1870: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-3c512aec9dea40d0bc22d0540b2e1870.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-dd3cf2d9e171416092bcfad01b0c680a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-dd3cf2d9e171416092bcfad01b0c680a.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-b0879cfabbcd4e05ab53c94e099c7877: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-b0879cfabbcd4e05ab53c94e099c7877.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-fb41ff276f3b4b1ab6c3de64548bbeab: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-b7eabc7966e94e819c8329118295add0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-fb41ff276f3b4b1ab6c3de64548bbeab.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-b7eabc7966e94e819c8329118295add0.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-7232e84c704e4436a21ae81d2061ca6f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-7232e84c704e4436a21ae81d2061ca6f.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-fb2e40f78e534ff4bb2fb6e5f88909ea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-fb2e40f78e534ff4bb2fb6e5f88909ea.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-cbc9f22be58a4adcab30d04ca5c444bb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-cbc9f22be58a4adcab30d04ca5c444bb.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-fa434ef407d64a8bb6691b9e9171182b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-fa434ef407d64a8bb6691b9e9171182b.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-6a97a3d06c6e4e7891f5735ad932d9f7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-6a97a3d06c6e4e7891f5735ad932d9f7.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-0ec50b1607df4328b6c439511690d013: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-0ec50b1607df4328b6c439511690d013.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-91a98321bed146688d692e9d89a01ba7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-91a98321bed146688d692e9d89a01ba7.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-9f16fb822e8c46a696f53b5805223404: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-9f16fb822e8c46a696f53b5805223404.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-3ad02ce0c05e4ac68af545e14aac3980: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-3ad02ce0c05e4ac68af545e14aac3980.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-3f78e2b252e643038f44de66b74804e4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-d6dd52c8362246d5b5151dbceb9f947d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-3f78e2b252e643038f44de66b74804e4.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-d6dd52c8362246d5b5151dbceb9f947d.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-007c1885f32f4d4b84e5979528aa9bab: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-007c1885f32f4d4b84e5979528aa9bab.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-8065794165ce40fb8f98f3d37a6e3467: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-8065794165ce40fb8f98f3d37a6e3467.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-e3263578e9a64bed85e017d5b354990b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-e3263578e9a64bed85e017d5b354990b.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-38dac2514d7441838cd8b2916dba78e8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-38dac2514d7441838cd8b2916dba78e8.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-671646eb6c20400b8cd39f88e8974eae: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-1c51898b5a264ddb99622034009204b2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-671646eb6c20400b8cd39f88e8974eae.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-207b6bb8e1da43ae98da66e114a72b15: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60418 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-1c51898b5a264ddb99622034009204b2.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-207b6bb8e1da43ae98da66e114a72b15.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-256f0e709fd94109a70a4fc1087d1a9e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-256f0e709fd94109a70a4fc1087d1a9e.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-44adf93f6e53418797b68c2db480e346: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-44adf93f6e53418797b68c2db480e346.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-f6cef1f9e0514cd682e07c42f91344c4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-f6cef1f9e0514cd682e07c42f91344c4.
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [logger.py:43] Received request chatcmpl-b0d7abd29ca346fc9243c750bd971028: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:44 [async_llm.py:270] Added request chatcmpl-b0d7abd29ca346fc9243c750bd971028.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-0714fe1065654ee98ba89c3a64e6c556: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-0714fe1065654ee98ba89c3a64e6c556.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-59a51a0e406b425a873f9e69c102f15e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-59a51a0e406b425a873f9e69c102f15e.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-5487a5a788e54e0f875e9e2c3afe9431: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-5487a5a788e54e0f875e9e2c3afe9431.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-e1834f60a4d44874a94539df3915d0ce: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-e1834f60a4d44874a94539df3915d0ce.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-b65b62d2791e48caa3c39c6de3d7abe0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-b65b62d2791e48caa3c39c6de3d7abe0.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-a8eb3c7777c64f42b219e6473a803114: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-a8eb3c7777c64f42b219e6473a803114.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-f5e6228f3c7b4b3689be22a08335966f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60536 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-f5e6228f3c7b4b3689be22a08335966f.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-b713ab55b37f4e57bb2f22d56bb3e5df: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-b713ab55b37f4e57bb2f22d56bb3e5df.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-a0f70b806e9c495eaee28648a48196a1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-a0f70b806e9c495eaee28648a48196a1.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-dcae7430e25041c1b3294463404c0d79: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-dcae7430e25041c1b3294463404c0d79.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-2801a11d36e54f8a91a36e7dcc8d1c32: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-2801a11d36e54f8a91a36e7dcc8d1c32.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-fb2e66e45db3422a9560119a0462bb03: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-fb2e66e45db3422a9560119a0462bb03.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-d1df95f86cdc43ff94665a98abaf3234: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-d1df95f86cdc43ff94665a98abaf3234.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-648a9488d5f844369920fd60499dfaf0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-648a9488d5f844369920fd60499dfaf0.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-dc11a9e40ac9411fac81ede6ea2b4475: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-dc11a9e40ac9411fac81ede6ea2b4475.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-84a6a8c142ad46b3b744fba135f12ce3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-84a6a8c142ad46b3b744fba135f12ce3.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-d826ca0e1a39428fb940a28984a63192: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60638 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-d826ca0e1a39428fb940a28984a63192.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-b1255d00243244519ac795f4d4cf193f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-b1255d00243244519ac795f4d4cf193f.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-3266ff46973a4522ad3afc9dcc55eceb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-3266ff46973a4522ad3afc9dcc55eceb.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-197854b4b3aa400cb038b95081c5bbbb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-197854b4b3aa400cb038b95081c5bbbb.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-6feb6b43de91473f82185fb160042fc3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-6feb6b43de91473f82185fb160042fc3.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-7694f1f8be334066b062a25b01f1260c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-7694f1f8be334066b062a25b01f1260c.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-2eee9a755aea4d608be2925dc75e4a04: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-2eee9a755aea4d608be2925dc75e4a04.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-3661fbdd164f4035b512a67d0868313a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-3661fbdd164f4035b512a67d0868313a.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-e8b34d4ee0c941f89977df1ba8448c54: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-e8b34d4ee0c941f89977df1ba8448c54.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-2e75042af3fd44fbb97390d26bd40b58: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-2e75042af3fd44fbb97390d26bd40b58.
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [logger.py:43] Received request chatcmpl-0290689dd9894923809b0813f8f213f5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:45 [async_llm.py:270] Added request chatcmpl-0290689dd9894923809b0813f8f213f5.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-b8b31a1a836f407692f292083e267c9d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60712 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-b8b31a1a836f407692f292083e267c9d.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-c710e3a51aa440708eff26dea35d0a0c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-c710e3a51aa440708eff26dea35d0a0c.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-a600dba3bba94ecdab17767235e9687a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-141f3d02f348498f91f5af63cbd30323: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-a600dba3bba94ecdab17767235e9687a.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-8a83fa9c5765444ba222144672e0189d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-337c8c8bb4694855b18d8f26a35eeaa3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-141f3d02f348498f91f5af63cbd30323.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-8a83fa9c5765444ba222144672e0189d.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-337c8c8bb4694855b18d8f26a35eeaa3.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-fbbd8d7abc6e43e8abfeb7cded841624: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-fbbd8d7abc6e43e8abfeb7cded841624.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-af2d536e3044474bb341b89f7e275039: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-af2d536e3044474bb341b89f7e275039.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-659dc325187f4911bef2986bdc2db4ea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-659dc325187f4911bef2986bdc2db4ea.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-b1a011d35e574a24b4fd2b5c4a52e6a8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-b1a011d35e574a24b4fd2b5c4a52e6a8.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-9eaeca437e2144e2a05e3034fd97c7dd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-9eaeca437e2144e2a05e3034fd97c7dd.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-3f5905c4b82c4876be183bdd4f0a13c6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-3f5905c4b82c4876be183bdd4f0a13c6.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-ed470f2c02ee4a4bbd8e6689197e078d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-ed470f2c02ee4a4bbd8e6689197e078d.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-d8bcef5a41b04ce189e69c939e3dc631: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-d8bcef5a41b04ce189e69c939e3dc631.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-75518e5c80134f2681c3957c0f01305f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60830 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-75518e5c80134f2681c3957c0f01305f.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-73ff2075f7d04c5695906b39f0c4e2aa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-1f58162ee37b4d318bdb41d2a16851d4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-73ff2075f7d04c5695906b39f0c4e2aa.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-1f58162ee37b4d318bdb41d2a16851d4.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-f2d27dbc76744844ad4007fcb9fba917: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-f2d27dbc76744844ad4007fcb9fba917.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-ae9ef94ac4ca470690f52d7419b62125: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-ae9ef94ac4ca470690f52d7419b62125.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-65b2645f093f4f70b19e628bec6bd33c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-65b2645f093f4f70b19e628bec6bd33c.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-347207e1e927481895526af9e491a15f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-347207e1e927481895526af9e491a15f.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-1defc9abd8464448a9c9c66c5769ea00: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60900 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-1defc9abd8464448a9c9c66c5769ea00.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-549f2f6fa63744f7aed9ce8426ca5118: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-549f2f6fa63744f7aed9ce8426ca5118.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-d7645199983c4865b4986928b29b6435: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-d7645199983c4865b4986928b29b6435.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-98cb839bb4cc4ee49cfbf9cca70b3f90: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-a71a129a10fc430ab804c9054746b2fa: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-98cb839bb4cc4ee49cfbf9cca70b3f90.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-1e888aaa03a440e985b46b89d6adb14e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-a71a129a10fc430ab804c9054746b2fa.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-4623f4940829454993c419c5959142e5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-1e888aaa03a440e985b46b89d6adb14e.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-4623f4940829454993c419c5959142e5.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-acb0ffa5ea8d402a9c595ea438409b99: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-acb0ffa5ea8d402a9c595ea438409b99.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-446e2a93b2a74ef0a7c4dd9755061346: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-446e2a93b2a74ef0a7c4dd9755061346.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-99bccdbd32fa497382204e97c5cf5748: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-99bccdbd32fa497382204e97c5cf5748.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-d8b7986f145546feba737769272f6b9b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-d8b7986f145546feba737769272f6b9b.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-eff772f4f45043a0866afc86019f8578: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-1b466745eeea4b018f7cdc57ee2ef5ab: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-eff772f4f45043a0866afc86019f8578.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-1b466745eeea4b018f7cdc57ee2ef5ab.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-3cc111bfd3a9451895df41f6180b51cc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-3cc111bfd3a9451895df41f6180b51cc.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-e369115a7c4f46c9b6b56d351f254a8b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-e369115a7c4f46c9b6b56d351f254a8b.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-7efaaeb4263c4bc8a21fc463bb7e62a0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-7efaaeb4263c4bc8a21fc463bb7e62a0.
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [logger.py:43] Received request chatcmpl-98eff48e792b41deb48d980a539248da: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:46 [async_llm.py:270] Added request chatcmpl-98eff48e792b41deb48d980a539248da.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-2e6a986273c348738c20f59d288b436d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-2e6a986273c348738c20f59d288b436d.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-fb25c6ed194344f18db9406126626e2c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-fb25c6ed194344f18db9406126626e2c.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-4bc62d2320464701a90416a72e03bbd2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-4bc62d2320464701a90416a72e03bbd2.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-5d20514dd7244fb386f5a6ba6e4e14b7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-d1144a3fed3b494ab1c6b98af258ad2f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-62b2ebfecac04172b1af1f0b9912aa46: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-5d20514dd7244fb386f5a6ba6e4e14b7.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-d1144a3fed3b494ab1c6b98af258ad2f.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-62b2ebfecac04172b1af1f0b9912aa46.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-f44c32cff6b4478a8d65e890baaec80e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-f44c32cff6b4478a8d65e890baaec80e.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-71cfed832ec44badb3e18eac775d1b3f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-71cfed832ec44badb3e18eac775d1b3f.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-7001e6b688ca4149ab21d354b76ba165: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-7001e6b688ca4149ab21d354b76ba165.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-9f1d4d1e62b44003854915ce88156a05: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-9f1d4d1e62b44003854915ce88156a05.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-d3f965d62b4a484988ab97dbbe95f0b2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-d3f965d62b4a484988ab97dbbe95f0b2.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-e4cd2b43fab747979befe5d8efec5085: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-e4cd2b43fab747979befe5d8efec5085.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-c68a8f3cfc4c438db4e77a4931ce76b5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-c68a8f3cfc4c438db4e77a4931ce76b5.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-36d0f6862e714fe199d8b12e51b0a150: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-36d0f6862e714fe199d8b12e51b0a150.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-61fe46715df344c888c2f29a6b3eb94b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-61fe46715df344c888c2f29a6b3eb94b.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-a94fb03bfb374bc791933134e32c95b0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-a94fb03bfb374bc791933134e32c95b0.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-386b31708743488e84122fc527dfc3ff: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-386b31708743488e84122fc527dfc3ff.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-3a736241532048bf8bd790805fe8ec91: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-3a736241532048bf8bd790805fe8ec91.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-583cb4e9f4e0493c885ea529de48a18d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-583cb4e9f4e0493c885ea529de48a18d.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-1e35a5b057e74b39be31ddc727b0abf8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-1e35a5b057e74b39be31ddc727b0abf8.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-1a91397e3478449e98230b8bd1c53bc0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-1a91397e3478449e98230b8bd1c53bc0.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-22641973241d4986a94327c8f362b789: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-8df4d2daff014c7ea43e1e3d2d57c9a5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-22641973241d4986a94327c8f362b789.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-8df4d2daff014c7ea43e1e3d2d57c9a5.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-2af8ad9a5c1a408988d79047c5898887: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-2af8ad9a5c1a408988d79047c5898887.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-937c3202338c483793d606d8ff57ad41: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-937c3202338c483793d606d8ff57ad41.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-41a2c24b85194914a50c2c4fb0472a69: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-41a2c24b85194914a50c2c4fb0472a69.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-56be63476d404369a1946f785c99d48a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-56be63476d404369a1946f785c99d48a.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-750fbb5cd59b4e44b5a020e8a394f524: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33074 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-750fbb5cd59b4e44b5a020e8a394f524.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-6818bfb91f1f4808b4ab8d45f1c4c33e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-6818bfb91f1f4808b4ab8d45f1c4c33e.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-4fea6ad261554dcebe8c24552499485a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-4fea6ad261554dcebe8c24552499485a.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-c03fc63ccea447c09ac0b8cfec05eb3b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-c03fc63ccea447c09ac0b8cfec05eb3b.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-0ba93dc5e17149989e8bfc40057a618f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-0ba93dc5e17149989e8bfc40057a618f.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-415c13aceec04d1dbe106ef89b52eb93: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-415c13aceec04d1dbe106ef89b52eb93.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-2a720c79205c43afb47bda689e48dc13: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-2a720c79205c43afb47bda689e48dc13.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-a1761e1d464a4ae190555961e3304587: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-a1761e1d464a4ae190555961e3304587.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-414710ed331b4ac99051f287025dfcd9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-414710ed331b4ac99051f287025dfcd9.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-9b8558d915864b6f89ac0a0eddd74156: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-9b8558d915864b6f89ac0a0eddd74156.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [loggers.py:118] Engine 000: Avg prompt throughput: 1327.5 tokens/s, Avg generation throughput: 2825.2 tokens/s, Running: 256 reqs, Waiting: 96 reqs, GPU KV cache usage: 66.6%, Prefix cache hit rate: 84.6%
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-baa667c2f65244488d7b81c39af35811: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-baa667c2f65244488d7b81c39af35811.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-da9e02c42c274318a5f5d361ebb40e53: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-da9e02c42c274318a5f5d361ebb40e53.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-0e17a564415942538f13362e4c7bdeef: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33148 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-0e17a564415942538f13362e4c7bdeef.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-53fc1d16b8a44a1b939f5deced1dd4c0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-53fc1d16b8a44a1b939f5deced1dd4c0.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-0ac72d3c8e2445588cd5e96d9bb6cf11: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-0ac72d3c8e2445588cd5e96d9bb6cf11.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-326a6e21146845c987c70ffe4dfd7829: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-326a6e21146845c987c70ffe4dfd7829.
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [logger.py:43] Received request chatcmpl-ec716e0d65dd45d2a63f3624ff77bf38: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:47 [async_llm.py:270] Added request chatcmpl-ec716e0d65dd45d2a63f3624ff77bf38.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-22c4f4ff20d8430686e5c3fa7ac43c2b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-22c4f4ff20d8430686e5c3fa7ac43c2b.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-b3aca09058894d8e89c3acb7ba903446: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-b3aca09058894d8e89c3acb7ba903446.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-6fd45adc440e412a942d2ea8175c4a01: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-6fd45adc440e412a942d2ea8175c4a01.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-76151fe54ae84d08ae71992884b77c07: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33230 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-76151fe54ae84d08ae71992884b77c07.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-e0f50fd12fd545f4a2fe114e24707f99: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-e0f50fd12fd545f4a2fe114e24707f99.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-881e3eae5029440883326cdce19a06f2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-881e3eae5029440883326cdce19a06f2.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-674bbe6f695141f585e479bdea822914: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-674bbe6f695141f585e479bdea822914.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-20a4ebef3a50414186e0828240917920: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-20a4ebef3a50414186e0828240917920.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-8bf15ca423da48d6aa1ca255bd4971f4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-8bf15ca423da48d6aa1ca255bd4971f4.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-d9f84e2fe99649a794fb03137f2e89c4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-d9f84e2fe99649a794fb03137f2e89c4.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-9c90a5fd9fde43e88a4ad9c0071b35f2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-9c90a5fd9fde43e88a4ad9c0071b35f2.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-9ffdc68e4ab0448ba3c8644bb1193de1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-9ffdc68e4ab0448ba3c8644bb1193de1.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-eb968a3382964f8a8a29f1ccceb85e5b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-eb968a3382964f8a8a29f1ccceb85e5b.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-ac81b6a316ff45dda67db943695a97c7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-ac81b6a316ff45dda67db943695a97c7.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-7d99b6a76de74c23bfca837bac8dd2ea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-7d99b6a76de74c23bfca837bac8dd2ea.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-296651f3f2e94330be21f5abe40016fb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-296651f3f2e94330be21f5abe40016fb.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-3652d89b50fc4d0ea79ad26a469566d6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-3652d89b50fc4d0ea79ad26a469566d6.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-f80aca40f0ad41c19579fa3c34a2ff68: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-f80aca40f0ad41c19579fa3c34a2ff68.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-af688972e3394bf0b11169bb162c265b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-af688972e3394bf0b11169bb162c265b.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-e9b7265f8fdd4bc8b1c8c1403a9e0c97: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-e9b7265f8fdd4bc8b1c8c1403a9e0c97.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-6d2eb0bc279c414bb4799e3028eea6bf: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-6d2eb0bc279c414bb4799e3028eea6bf.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-4476293897fd47fa9e1d58a4b01378d8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-4476293897fd47fa9e1d58a4b01378d8.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-00113e1bed82429392b2f1ce35837748: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-00113e1bed82429392b2f1ce35837748.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-33432ad460974874b187793b772e776a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-33432ad460974874b187793b772e776a.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-11d8b04ce022451cbb6f8e0c4d23917f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-11d8b04ce022451cbb6f8e0c4d23917f.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-11fc00fb669946339fba38f03a6535f4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-11fc00fb669946339fba38f03a6535f4.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-fb415511902b4f928e51e369714ef39f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-fb415511902b4f928e51e369714ef39f.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-730f259e89a24f6fae724d8fceaac7f0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-659870aae4a74380938f80a69f266901: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-730f259e89a24f6fae724d8fceaac7f0.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-659870aae4a74380938f80a69f266901.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-19bbfc127918488bb089135eb2e78f1c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-19bbfc127918488bb089135eb2e78f1c.
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [logger.py:43] Received request chatcmpl-cacd06683f334754bd5982ae70fbe206: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:48 [async_llm.py:270] Added request chatcmpl-cacd06683f334754bd5982ae70fbe206.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-11aaef3b761d4bbeb578a71b27608ca0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-14e795190f354f9284df7b271cb30fd4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-11aaef3b761d4bbeb578a71b27608ca0.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-14e795190f354f9284df7b271cb30fd4.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-a3f8a29653344f7b9ac39b8932e481ba: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-3223d014a5744184bf182485af01e485: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33524 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-a3f8a29653344f7b9ac39b8932e481ba.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33532 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-3223d014a5744184bf182485af01e485.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-9b29ca9064144c4da3a3e3179ac2b875: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-ae4fca02d02b441db434b2fcef899758: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-9b29ca9064144c4da3a3e3179ac2b875.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-ae4fca02d02b441db434b2fcef899758.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-163512aa4274466cbeea5e3c6a169f1e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-f33e8bc9225e444faf0da9423aa154f9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-163512aa4274466cbeea5e3c6a169f1e.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-f33e8bc9225e444faf0da9423aa154f9.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-17ad62c4177d4dbfab2ee625a5fa67a9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-17ad62c4177d4dbfab2ee625a5fa67a9.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-ea4af7805fcd4262812b831c3dd6f180: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-ea4af7805fcd4262812b831c3dd6f180.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-a7ac48150c714e3c89ed5fdeb4bd8be7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33586 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-a7ac48150c714e3c89ed5fdeb4bd8be7.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-f274cba3d5694d6aad126b0d87ea3ff0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-f274cba3d5694d6aad126b0d87ea3ff0.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-16e1ac831c904f9eb41cb10df2be6ee7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-16e1ac831c904f9eb41cb10df2be6ee7.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-93a9ae410eeb4b70bd7cf8e1be1a48b6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-93a9ae410eeb4b70bd7cf8e1be1a48b6.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-a99ecbd6d6624b5c9e709c71082645f0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-a99ecbd6d6624b5c9e709c71082645f0.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-a3434e2fa8504f6894277607e5b8ff0f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-a3434e2fa8504f6894277607e5b8ff0f.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-83742eec7df1486190bb1f51c9c9fdc3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-83742eec7df1486190bb1f51c9c9fdc3.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-5de3e4f57a66498ab3034f2aa995b60d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-5de3e4f57a66498ab3034f2aa995b60d.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-8ab7cb24ab504a63b70a8b28da48ccd0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-8ab7cb24ab504a63b70a8b28da48ccd0.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-30213195828e4c299075525e8ba58df9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-30213195828e4c299075525e8ba58df9.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-e7a3cc8878bd436cbb3049815e9f01d9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-e7a3cc8878bd436cbb3049815e9f01d9.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-7b10577efeaf41009fcc82707e15c03d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-7b10577efeaf41009fcc82707e15c03d.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-62797d9a2eab41f5b9aa97098a821b6d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-62797d9a2eab41f5b9aa97098a821b6d.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-1de03a654b924f658e2d2c2c5aaa3fd3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-c60b5f704bb9405da4ccc674a368bd0b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-1de03a654b924f658e2d2c2c5aaa3fd3.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-c60b5f704bb9405da4ccc674a368bd0b.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-80429f4a703e40f28678aa4c5ad5c04c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-80429f4a703e40f28678aa4c5ad5c04c.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-67feac6db6b44596a3ac2c8e57e2b07e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-67feac6db6b44596a3ac2c8e57e2b07e.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-eb3940fd9fea4497b0d57335567d0269: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-eb3940fd9fea4497b0d57335567d0269.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-b95654c86e0e4afcae77a53043cd6157: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-b95654c86e0e4afcae77a53043cd6157.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-fbffcb1c34384f1e8470308d74b95d5c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-fbffcb1c34384f1e8470308d74b95d5c.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-e660f01b7e2d44c0a392fd9451fc40e0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-e660f01b7e2d44c0a392fd9451fc40e0.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-ddbe4ff164ad4f2296a9f2e681fbf686: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-ddbe4ff164ad4f2296a9f2e681fbf686.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-1b7bf8787621448f8f946fd0579f4f03: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-1b7bf8787621448f8f946fd0579f4f03.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-13d3f4010de24ce395c2d776af72a228: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-f1f2e0f2daab46b093a75b0a49fe1059: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-13d3f4010de24ce395c2d776af72a228.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-f1f2e0f2daab46b093a75b0a49fe1059.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-52619c4decd94c069f405c99ae6072e4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-52619c4decd94c069f405c99ae6072e4.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-6bf011da98ec4c14b1c7bed7c328f727: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-6bf011da98ec4c14b1c7bed7c328f727.
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [logger.py:43] Received request chatcmpl-6bb2ccfb80604dbda6978a8680722d81: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:49 [async_llm.py:270] Added request chatcmpl-6bb2ccfb80604dbda6978a8680722d81.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-f01175dd3a774cba8fc426c0b753e35b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-f01175dd3a774cba8fc426c0b753e35b.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-cacfdf0c6f4e4120b02dd964d11fb829: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-cacfdf0c6f4e4120b02dd964d11fb829.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-4b93a571472842d0aeb7ac32939176b8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-4b93a571472842d0aeb7ac32939176b8.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-ce527e7524ee406f82bbd76375743f34: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-ce527e7524ee406f82bbd76375743f34.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-dfaaa4bf696c47d89ce15ce99a92423a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-dfaaa4bf696c47d89ce15ce99a92423a.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-e0ef28fa95044bbcb39397d2d4877e0a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-e0ef28fa95044bbcb39397d2d4877e0a.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-b556dfd43ae74dd38a42d28687de3223: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-b556dfd43ae74dd38a42d28687de3223.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-6aadde169c824d989730262e9cbc31a4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-6aadde169c824d989730262e9cbc31a4.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-6624bf76960e49a0838213c441c722b8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-6624bf76960e49a0838213c441c722b8.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-5cc6baba88c14569aa819375bd3f689f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-5cc6baba88c14569aa819375bd3f689f.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-29e9d2824a714f8bb4ee0f95b90337f9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-29e9d2824a714f8bb4ee0f95b90337f9.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-1b219416b3814c95b9f0d21e5ed9e5e6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-1b219416b3814c95b9f0d21e5ed9e5e6.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-47d445d0ebcc4d2193a1d35d76458166: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-47d445d0ebcc4d2193a1d35d76458166.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-8452813c143b4daca8a355237f7defe0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-8452813c143b4daca8a355237f7defe0.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-fe903a6e1490407fbc5e01914cb3340a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-fe903a6e1490407fbc5e01914cb3340a.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-bc194b9fa6ed413e9153fc30d039ef48: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-bc194b9fa6ed413e9153fc30d039ef48.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-00692755dfa743a6a9f3e9718a46294e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-00692755dfa743a6a9f3e9718a46294e.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-428ce6e8904c4b60ac4230b6a26a77ac: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-428ce6e8904c4b60ac4230b6a26a77ac.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-d46d10987ead44debcff919ded48540f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-d46d10987ead44debcff919ded48540f.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-c2b15b9e5c314ef0b417c59444971e2b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-c2b15b9e5c314ef0b417c59444971e2b.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-27cc9f1e41dc4b16b24010b265596a26: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-27cc9f1e41dc4b16b24010b265596a26.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-72088e82ef484461a1d3629a1fdefda1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-72088e82ef484461a1d3629a1fdefda1.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-b5e2e2237265414a908a639d6d9b8a0e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34074 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-b5e2e2237265414a908a639d6d9b8a0e.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-6b46478ba3534b5ca166e76c29fe7c50: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-6b46478ba3534b5ca166e76c29fe7c50.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-9eec9dbb25c9479c838badd60614aa22: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-9eec9dbb25c9479c838badd60614aa22.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-59178483137347efa563286feae058fa: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-59178483137347efa563286feae058fa.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-2b0a5ba3cf574232b7a05e1823ac6f63: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-2b0a5ba3cf574232b7a05e1823ac6f63.
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [logger.py:43] Received request chatcmpl-0e91933879bf4834b47717b4ccb67e84: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:50 [async_llm.py:270] Added request chatcmpl-0e91933879bf4834b47717b4ccb67e84.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-9de209feee554a7fab2c9f731f604803: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-9de209feee554a7fab2c9f731f604803.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-b13f7b9efb604b309126092dec22c421: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-b13f7b9efb604b309126092dec22c421.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-d68d48f05e7b4df69780c7aa22d21ba9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-d68d48f05e7b4df69780c7aa22d21ba9.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-d68adaabc98141fd93a5d1a1bd9752ea: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-d68adaabc98141fd93a5d1a1bd9752ea.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-69cb647f7bc6472594a9dc9354163e48: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-69cb647f7bc6472594a9dc9354163e48.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-08ce8110f46343569f9420fd0fcba0fa: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-08ce8110f46343569f9420fd0fcba0fa.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-efeb1a5a234d4bbe96b6de7041bc1962: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-efeb1a5a234d4bbe96b6de7041bc1962.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-47427f3ea2924398955a6b932cfb84b5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-47427f3ea2924398955a6b932cfb84b5.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-790816fcd38448ef9e6b50ba5040c7ed: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-790816fcd38448ef9e6b50ba5040c7ed.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-e583b436c2a64775987dc1a439efd9b2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-e583b436c2a64775987dc1a439efd9b2.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-3443f9b0f124442399faa35e2d724616: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-3443f9b0f124442399faa35e2d724616.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-6444a060829947cbbdfdf88ad44872b0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-6444a060829947cbbdfdf88ad44872b0.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-cd9f63d1cb254d95b21a00b427b1bb88: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-cd9f63d1cb254d95b21a00b427b1bb88.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-a6df7590eb0d4bb4a138e5f4c0bf9c9a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-a6df7590eb0d4bb4a138e5f4c0bf9c9a.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-64956fde485f4402892cc94dd933f693: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-64956fde485f4402892cc94dd933f693.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-d7e136b1a1a345ebbffed7da49072cdb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-d7e136b1a1a345ebbffed7da49072cdb.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-21f2e67a66084dd49fd762e3ad7023d3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-21f2e67a66084dd49fd762e3ad7023d3.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-33aeb53f359e469f894d87173b0e6c7f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-33aeb53f359e469f894d87173b0e6c7f.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-13b865dcf28244f69a7ff8eefc2b3f11: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-13b865dcf28244f69a7ff8eefc2b3f11.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-9bf5ffa154564988b5f5f5f97ec64b1a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-9bf5ffa154564988b5f5f5f97ec64b1a.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-9f7e67dc700a41a79227a3b9cc8abec9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-284dc8647af44d6fb1a62963c9559143: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-9f7e67dc700a41a79227a3b9cc8abec9.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-284dc8647af44d6fb1a62963c9559143.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-48912959d90c44d297824817b3c341bb: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-f1ec723fc7b8440fb8a0a6a516b44f66: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-48912959d90c44d297824817b3c341bb.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-f1ec723fc7b8440fb8a0a6a516b44f66.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-b6cc805fb5f14f0f9174676a66f779e0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-9b47cfefb8e14d4d8b5188632f033f20: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-b6cc805fb5f14f0f9174676a66f779e0.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-9b47cfefb8e14d4d8b5188632f033f20.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-f2fd11b49109423e991a13693cda42c2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-f2fd11b49109423e991a13693cda42c2.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-0fa764bd0bcb404981338ed67c379c63: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-0fa764bd0bcb404981338ed67c379c63.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-8d795500fc704567a8f1551a08f21fb6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-8d795500fc704567a8f1551a08f21fb6.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-e7514f01e13f4d1b8dc8afaeec01b63f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-e7514f01e13f4d1b8dc8afaeec01b63f.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-708794a8fa8742a9babc239a31a209ca: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-708794a8fa8742a9babc239a31a209ca.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-bc383c6608204764afde85ea31fc7b1f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [async_llm.py:270] Added request chatcmpl-bc383c6608204764afde85ea31fc7b1f.
[36mllm_server_1  |[0m INFO 07-20 23:04:51 [logger.py:43] Received request chatcmpl-9520e3360b0542229c89ad22e0d3c6a7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-9520e3360b0542229c89ad22e0d3c6a7.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-9365c059272e4cdaa941e07bd14a2745: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-9365c059272e4cdaa941e07bd14a2745.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-360e321de7fb4d56a28e25341a02f0af: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-360e321de7fb4d56a28e25341a02f0af.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-57e2eadf7e1f4704a741986a7691faac: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-57e2eadf7e1f4704a741986a7691faac.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-9771c5eee80241409659930f78c5fcd9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-9771c5eee80241409659930f78c5fcd9.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-8f86b49d99d740109b3c5badd9cf7eb4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-8f86b49d99d740109b3c5badd9cf7eb4.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-c52d90576f8e40ae9ed3a19c708cc46a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-c52d90576f8e40ae9ed3a19c708cc46a.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-9c4ba5136f584bf5bee50b9a4fb1c112: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-9c4ba5136f584bf5bee50b9a4fb1c112.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-18741b7cc4434aed880cce433f4ce7e8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-18741b7cc4434aed880cce433f4ce7e8.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-d8f316df5d144ead84ed40b4f9fb0ff7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-d8f316df5d144ead84ed40b4f9fb0ff7.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-050027b0bb6c4a30af5ddc348c8586e2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-050027b0bb6c4a30af5ddc348c8586e2.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-3eb48553c3c34863b31d9616138a5bcc: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-3eb48553c3c34863b31d9616138a5bcc.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-613eaa170f104045aadc73e61b04bd03: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-613eaa170f104045aadc73e61b04bd03.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-3ec4259bdb7b4af2a6adbfa3a6f76f39: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-3ec4259bdb7b4af2a6adbfa3a6f76f39.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-0e75a2f87ea3405db00b9728693cffa4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-0e75a2f87ea3405db00b9728693cffa4.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-5c0d26f841f448cab7df65994389736b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-26ca0a0bd1c6478eb7cabe7cb91f2a33: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-5c0d26f841f448cab7df65994389736b.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34536 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-26ca0a0bd1c6478eb7cabe7cb91f2a33.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-952b8459548b459ab0acc591379a6128: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-952b8459548b459ab0acc591379a6128.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-9f85381e4ceb4e939908ff634699bafe: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-8e88bf079909460aae9b6e17bf04ed8a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-9f85381e4ceb4e939908ff634699bafe.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-8e88bf079909460aae9b6e17bf04ed8a.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-a2823203dd724782ad8b6ab43934d80c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-a2823203dd724782ad8b6ab43934d80c.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-2a279cf749bc4a3a93cc75c6e42eb350: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-2a279cf749bc4a3a93cc75c6e42eb350.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-3ca1cd54cf704dec9fe21db80b21d364: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-3ca1cd54cf704dec9fe21db80b21d364.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-6323db0ddd81401aa0113764fcdd5f5a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-6323db0ddd81401aa0113764fcdd5f5a.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-0daf7376284b411580d5922ea02fbe9e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-0daf7376284b411580d5922ea02fbe9e.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-62dab2158a9740ab80a38128b9baca09: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-62dab2158a9740ab80a38128b9baca09.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-5688a5294b744c02b2808da3f4f126b2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-5688a5294b744c02b2808da3f4f126b2.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-bc2581e2c45e45d18324cfd3d96bdd99: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-bc2581e2c45e45d18324cfd3d96bdd99.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-d72ce3c217604742a18de6dea67ecee8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-d72ce3c217604742a18de6dea67ecee8.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-db723871e47a4c04bb239433e597ddb4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-db723871e47a4c04bb239433e597ddb4.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-49c9ee74ba96478c86912d2fba861e6e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-49c9ee74ba96478c86912d2fba861e6e.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-3d0118314f73449fba3f40fbd4edeee0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-3d0118314f73449fba3f40fbd4edeee0.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-66312698588a4755a4e146e5a9f449a7: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-66312698588a4755a4e146e5a9f449a7.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-daeba893e1f9470ca44e90b88de62d9f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-be01c3c6908b4545abd4d16afe2d2816: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-daeba893e1f9470ca44e90b88de62d9f.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-be01c3c6908b4545abd4d16afe2d2816.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-78e52b96602b48dc91453fbe563a35ce: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-78e52b96602b48dc91453fbe563a35ce.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-75e342d444a14ce18979ba98fc33e301: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-75e342d444a14ce18979ba98fc33e301.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-7470c2e7be8c46dbb4fcf34a0d39a20d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-7470c2e7be8c46dbb4fcf34a0d39a20d.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-6c71766356a8421fb002ed876337f17a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-6c71766356a8421fb002ed876337f17a.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-1f6b6d822ad44c1ebd66a18d2a82a8c2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-1f6b6d822ad44c1ebd66a18d2a82a8c2.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-1764b2d72ec842719c965e83cbb73be1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-1764b2d72ec842719c965e83cbb73be1.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-2fa3631a8951497ca0723ffe7436607f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-2fa3631a8951497ca0723ffe7436607f.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-cbfea6d3653c4b3aaae940e9c65d82b4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-cbfea6d3653c4b3aaae940e9c65d82b4.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-aa6bdf3d536a40a09bf3995876765be6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:34786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-aa6bdf3d536a40a09bf3995876765be6.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-96604ef9ed0349f69b8a388b61187054: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-96604ef9ed0349f69b8a388b61187054.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-384a8139b18c4caea5da0e97d598d1d2: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-384a8139b18c4caea5da0e97d598d1d2.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-aa531f568304449a934d9962df98cb09: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-aa531f568304449a934d9962df98cb09.
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [logger.py:43] Received request chatcmpl-eb5cf0ae38484463be95cfd1f5172ce3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:52 [async_llm.py:270] Added request chatcmpl-eb5cf0ae38484463be95cfd1f5172ce3.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-b98fb3d97e6e4678b8fa9423b3429fdb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-66e7f6d64b934214894a0304a92f2c74: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-b98fb3d97e6e4678b8fa9423b3429fdb.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-66e7f6d64b934214894a0304a92f2c74.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-34ad8058b1374d5fb9ee7efaf3012738: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-34ad8058b1374d5fb9ee7efaf3012738.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-60c2034b50294a27a25026d63dff38cb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-60c2034b50294a27a25026d63dff38cb.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-e2d4b269443146989d5f3819a05a48b5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-e2d4b269443146989d5f3819a05a48b5.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-992b8aba0fb64b839cad039bd2b192e4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-9d724c6d1cef4ed389971d28ceb38992: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-992b8aba0fb64b839cad039bd2b192e4.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-9d724c6d1cef4ed389971d28ceb38992.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-0bc1cfdd67144c6b82e2f8aa163af76b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-0bc1cfdd67144c6b82e2f8aa163af76b.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-612848bd51514ab6b8307a89716c6f72: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-612848bd51514ab6b8307a89716c6f72.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-c6db654e3d9c4668bc7648e5a08d05e6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-c6db654e3d9c4668bc7648e5a08d05e6.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-255e06e747b24f5ba34d981ed9d5aff6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-255e06e747b24f5ba34d981ed9d5aff6.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-936093ceb13b46d49fa0c8431a0a544b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-936093ceb13b46d49fa0c8431a0a544b.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-a554d4147e534160b679159540163e98: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-a554d4147e534160b679159540163e98.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-cb47545f16544b96a7c9f7d101be2830: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-cb47545f16544b96a7c9f7d101be2830.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-7e8651177f274adbaa236ed4ad640894: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-7e8651177f274adbaa236ed4ad640894.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-7a7a3d553b474077acc7a43dbdc2d90c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-7a7a3d553b474077acc7a43dbdc2d90c.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-45901d5fb25d4e088b3f14d36f23aec7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-45901d5fb25d4e088b3f14d36f23aec7.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-654df48eb5884b009dd9f00f50af4a62: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-654df48eb5884b009dd9f00f50af4a62.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-6ad1ef0ba78d4dfab1d6c5f919397c0a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-6ad1ef0ba78d4dfab1d6c5f919397c0a.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-b6f89700a341435db5b7387ca57a6eff: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-b6f89700a341435db5b7387ca57a6eff.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-755f7a2af697485b83b645f237293af1: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-755f7a2af697485b83b645f237293af1.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-704380a774104e07ba761613fc4a8282: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-704380a774104e07ba761613fc4a8282.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-5d2b25d6d2834930a9cfa8819e53e9d1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-5d2b25d6d2834930a9cfa8819e53e9d1.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-c30dcc0600cb4185abdefeab6a140f5e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-c30dcc0600cb4185abdefeab6a140f5e.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-ea4acbc3972c4faaa92115aec684cf62: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-ea4acbc3972c4faaa92115aec684cf62.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-2e3da568d0b64663b3c1bd0f15332f51: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-2e3da568d0b64663b3c1bd0f15332f51.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-13c4f0a51780474ebe17f838d62f270b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-13c4f0a51780474ebe17f838d62f270b.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-50cee2c9c5044e6dadf6c39044c9bbba: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-50cee2c9c5044e6dadf6c39044c9bbba.
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [logger.py:43] Received request chatcmpl-c42e27c6827e43f0a17ede1d37716f50: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:53 [async_llm.py:270] Added request chatcmpl-c42e27c6827e43f0a17ede1d37716f50.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-72cb20ace6b94e718cfa4770703f034b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-72cb20ace6b94e718cfa4770703f034b.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-e4ae27059e234781a5fd51483ca9bda0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-e4ae27059e234781a5fd51483ca9bda0.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-2f38d13130464a80be9d1d5eebab3c63: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-2f38d13130464a80be9d1d5eebab3c63.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-beb399304baa4f289cf9dc3f96895964: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-beb399304baa4f289cf9dc3f96895964.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-7fc9ef0b71d64e8c9766ac0d9135b2cf: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-7fc9ef0b71d64e8c9766ac0d9135b2cf.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-a0ef1575dfb64781a35159112e2f0801: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-a0ef1575dfb64781a35159112e2f0801.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-2888ac702acd477e9241b4c29163f846: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-2888ac702acd477e9241b4c29163f846.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-ad4f32c67a954665aaa2d7eb2054223d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-ad4f32c67a954665aaa2d7eb2054223d.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-a75173ee68774e14ab356f0b25577680: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-a75173ee68774e14ab356f0b25577680.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-9eb22146ff764df3801616f2a2431973: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-9eb22146ff764df3801616f2a2431973.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-5d9675498ffb4d2880caa8af07855e2d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-5d9675498ffb4d2880caa8af07855e2d.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-7a2d26f5fe494771821b54884d7c34e9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-7a2d26f5fe494771821b54884d7c34e9.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-b1686c96e64845a3afe4308dee6446dd: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-b1686c96e64845a3afe4308dee6446dd.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-4e0dd6f9572044189c074954a9ddd994: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-4e0dd6f9572044189c074954a9ddd994.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-c3f74c41fa0c4209b2387a3d4cf34599: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-c3f74c41fa0c4209b2387a3d4cf34599.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-d97dba874cc8409797472ef50f0f8ae3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-d97dba874cc8409797472ef50f0f8ae3.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-389e74c01fed4b55aba79f6e0b62dd80: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-b545f93575254718b98a7f30d2120690: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-fc1bab8aabe34d8bbafd1c6ddae4ba7b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-389e74c01fed4b55aba79f6e0b62dd80.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-b545f93575254718b98a7f30d2120690.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-b9618c96586d4eeda3f428a9ef8075d9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-fc1bab8aabe34d8bbafd1c6ddae4ba7b.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-f1746100478242e3ad35367a5e40c576: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-b9618c96586d4eeda3f428a9ef8075d9.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58552 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-f1746100478242e3ad35367a5e40c576.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-2f0765f51ded44ba89cebd5f11dbd9be: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-2f0765f51ded44ba89cebd5f11dbd9be.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-cba7040ba7834e36a430940d56db9cb7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-cba7040ba7834e36a430940d56db9cb7.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-73e19f793df0414db2a50ecb2bab21bb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-73e19f793df0414db2a50ecb2bab21bb.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-df04d3b3e6be45a58f7951fd79fd5a4a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-df04d3b3e6be45a58f7951fd79fd5a4a.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-33ceb992cfd340ed85bbbd63f9ce52b6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-33ceb992cfd340ed85bbbd63f9ce52b6.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-dd1ecb6dbf764e6f9900cbd45d61beb0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-dd1ecb6dbf764e6f9900cbd45d61beb0.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-a6a1268325ae4d07835358f6cf0c91e4: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-a6a1268325ae4d07835358f6cf0c91e4.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-ba2ad94238284fa48dee9bb1ee3552d3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58616 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-ba2ad94238284fa48dee9bb1ee3552d3.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-35d7a62f6502413a9376b404687de339: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58632 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-35d7a62f6502413a9376b404687de339.
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [logger.py:43] Received request chatcmpl-8a8a06dbcb9b4211af241951106cea28: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:54 [async_llm.py:270] Added request chatcmpl-8a8a06dbcb9b4211af241951106cea28.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-7b4c00f707154ebcbf96ddddbd4b0915: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-7b4c00f707154ebcbf96ddddbd4b0915.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-dd20cefc17d846ffae773b22b89c7c13: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-dd20cefc17d846ffae773b22b89c7c13.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-afa6c7675e2a45cda5053f6b8daf9bec: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-afa6c7675e2a45cda5053f6b8daf9bec.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-82e611e92c264556af6a84097942f184: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-82e611e92c264556af6a84097942f184.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-77ca5b52c7664c19a22c78e18f7c2493: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-77ca5b52c7664c19a22c78e18f7c2493.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-45d56965e5d1449b89facd4855067fb5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-45d56965e5d1449b89facd4855067fb5.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-11acf3f9eb0b45cebd4eeb55616d801f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-11acf3f9eb0b45cebd4eeb55616d801f.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-8a8e9e346e1a42bf83b9679466394119: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-8378058b43354c3987312d01c4de6ef1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-8a8e9e346e1a42bf83b9679466394119.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-8378058b43354c3987312d01c4de6ef1.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-e7bffdd36f094f22bb405d37a24dbd0a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-e7bffdd36f094f22bb405d37a24dbd0a.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-b7598cbd4dcb4e7fbaa553f441152c5f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-b7598cbd4dcb4e7fbaa553f441152c5f.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-84d701b703f34e43be46422ed71129bd: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-84d701b703f34e43be46422ed71129bd.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-9adea2b8b6a54175b012527b52ca50ce: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-9adea2b8b6a54175b012527b52ca50ce.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-3452bf5d0ac14139aee19644701570be: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-3452bf5d0ac14139aee19644701570be.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-89c177e555164bcfbd9f86c4706f1636: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-89c177e555164bcfbd9f86c4706f1636.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-ee02ae08347d4b25925e85ce399b4b43: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-ee02ae08347d4b25925e85ce399b4b43.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-c07ce420448a453c878d270c7d1b4c87: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-c07ce420448a453c878d270c7d1b4c87.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-4a2970d9866a4e659f279f9d8285d6a3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-4a2970d9866a4e659f279f9d8285d6a3.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-ac3319040ee3466fa7528338cb02b526: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-ac3319040ee3466fa7528338cb02b526.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-2ac0d2348d3e43fe9ddd51c6b376c44c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-2ac0d2348d3e43fe9ddd51c6b376c44c.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-367fdfd2e33449cbb5ef76263a113cea: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-367fdfd2e33449cbb5ef76263a113cea.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-8e74627fe0ea49ae88e0b1f82b243485: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-8e74627fe0ea49ae88e0b1f82b243485.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-4e7ecd4feb4f429cb8920a18c7377779: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-4e7ecd4feb4f429cb8920a18c7377779.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-b553c41a7a6d48328885ffa1bc0f2688: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-b553c41a7a6d48328885ffa1bc0f2688.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-f01375bfe19e46f394f94b07ab164430: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-f01375bfe19e46f394f94b07ab164430.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-2b99075eead8462aa03d794748302540: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-2b99075eead8462aa03d794748302540.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-1527f3d942d04675afa0f6ad5235383f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-1527f3d942d04675afa0f6ad5235383f.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-153f51aa30cb44c8b1fc9bc1196869ee: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-153f51aa30cb44c8b1fc9bc1196869ee.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-430f9b6740684d729019b9331a649013: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-430f9b6740684d729019b9331a649013.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-5071042c0fc2463080de885c003b9b74: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-5071042c0fc2463080de885c003b9b74.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-d51e21e4780c470995ec2982da1518e7: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-d51e21e4780c470995ec2982da1518e7.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-c4a00ac66c224ed6a8869fc59980d431: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-c4a00ac66c224ed6a8869fc59980d431.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-1f61ca3c09bb42b18928e3f98c10a214: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-1f61ca3c09bb42b18928e3f98c10a214.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-770ca3d0560248eda5fcc590802c6005: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-770ca3d0560248eda5fcc590802c6005.
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [logger.py:43] Received request chatcmpl-527b09aadd5a4fe88ddd66782ffb1b85: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:55 [async_llm.py:270] Added request chatcmpl-527b09aadd5a4fe88ddd66782ffb1b85.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-64afe5f1fcdb40d49f5cbc869abb843c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-64afe5f1fcdb40d49f5cbc869abb843c.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-c934d5b8655a4c39827bab0d1e7e85f5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-c934d5b8655a4c39827bab0d1e7e85f5.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-43e0fa4789e44d7391824bcb6b70213b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-43e0fa4789e44d7391824bcb6b70213b.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-c11819ebf9b54ded9470c7c1f91bab89: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-c11819ebf9b54ded9470c7c1f91bab89.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-08f4cb67a66448158f5edb36cd2bb9ad: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-08f4cb67a66448158f5edb36cd2bb9ad.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-dddd84d4d7c14b55a00cebc259400305: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-dddd84d4d7c14b55a00cebc259400305.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-0c7636eb46e04d4184c4ce644c93abf7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-0c7636eb46e04d4184c4ce644c93abf7.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-45810973e4b34c90ad545decf46979ab: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-45810973e4b34c90ad545decf46979ab.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-55b8a0cff67141829702b281c4919118: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-55b8a0cff67141829702b281c4919118.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-50a8de5b47e147f881c7e09cf187f100: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-50a8de5b47e147f881c7e09cf187f100.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-ed84361022064bc4944081bf1b6ea608: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-ed84361022064bc4944081bf1b6ea608.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-c2bf1c661d4949f99628885ee3dbe143: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-c2bf1c661d4949f99628885ee3dbe143.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-d304ac483d7541eea85962f8a19c8b21: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-d304ac483d7541eea85962f8a19c8b21.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-db0b8404be4148cc89774f99591b83c2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-db0b8404be4148cc89774f99591b83c2.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-42278b2bb4f44158929a2b1c974260d6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-42278b2bb4f44158929a2b1c974260d6.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-7ba42be9588a4eb2bdd3cbb385778902: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-7ba42be9588a4eb2bdd3cbb385778902.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-e9095d8b1e564c849cd46859a1862274: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-e9095d8b1e564c849cd46859a1862274.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-657d3a14f6fe4358bad3608fea35099a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-657d3a14f6fe4358bad3608fea35099a.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-0a516b796a2040ebb413ba017174061b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-0a516b796a2040ebb413ba017174061b.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-9d1db2eaaa0b4d8a9cd62458524e3310: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-9d1db2eaaa0b4d8a9cd62458524e3310.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-1ab0b493ec0449818368f5e319d08f9a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-1ab0b493ec0449818368f5e319d08f9a.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-89d38ba74c534a85908e45260a2c8306: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-89d38ba74c534a85908e45260a2c8306.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-b2600b39ec7e449ead35f81939196525: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-b2600b39ec7e449ead35f81939196525.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-0fed7eb425a44409b4a8df622fd9d502: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-0fed7eb425a44409b4a8df622fd9d502.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-bd4b2724155b4c36b6ff97493a040251: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-bd4b2724155b4c36b6ff97493a040251.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-3b77b2c0d01a43c392dc9bb1558e5249: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59198 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-3b77b2c0d01a43c392dc9bb1558e5249.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-9acb74036c414a8daaa50e083f40ee42: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-9acb74036c414a8daaa50e083f40ee42.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-79099a43cc724a15810686b838eb9ec9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-79099a43cc724a15810686b838eb9ec9.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-8ad25d5f022e4f00a0926d61d39b66e9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-8ad25d5f022e4f00a0926d61d39b66e9.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-c362b50f23c744a8952a471d6b65d7b9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-c362b50f23c744a8952a471d6b65d7b9.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-afc9e1605d2348deb4bed304d7e51ee5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-afc9e1605d2348deb4bed304d7e51ee5.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-82ea4b62ff9d4228883ed5086ee4831a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-82ea4b62ff9d4228883ed5086ee4831a.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-775fce3b16e94ca486e57e00a4b52a18: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-775fce3b16e94ca486e57e00a4b52a18.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-5a4f7f034c7942c3bd5e188cb1a030de: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59242 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-5a4f7f034c7942c3bd5e188cb1a030de.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-371464885a6d469e8eefb1cd62b9c27c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-371464885a6d469e8eefb1cd62b9c27c.
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [logger.py:43] Received request chatcmpl-02fa34e6e9094a9a8180f62c46534ab2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:56 [async_llm.py:270] Added request chatcmpl-02fa34e6e9094a9a8180f62c46534ab2.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-29fa33853b6e4d50a23081e3a9c0c724: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-29fa33853b6e4d50a23081e3a9c0c724.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-0e916ba525d84a06abd25b2f50ec2124: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-0e916ba525d84a06abd25b2f50ec2124.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-98dcb5a9f3cc42c1b0a8526cfdeee73e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-98dcb5a9f3cc42c1b0a8526cfdeee73e.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-2da4ae1fa1754895a8d7c6d8e236f642: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-2da4ae1fa1754895a8d7c6d8e236f642.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-36f94f3e6cb544ac9a0b992842b36b33: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-36f94f3e6cb544ac9a0b992842b36b33.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-c5bdc89bd35c4b7d9c08768f15613742: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-c5bdc89bd35c4b7d9c08768f15613742.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-663ae6e08a5245bfbe71651cedc1b60c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-663ae6e08a5245bfbe71651cedc1b60c.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-9bc013c0501e4645b15680c9d0514e70: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-0d08fbea519d41a0a612be91b1986202: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-9bc013c0501e4645b15680c9d0514e70.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-0d08fbea519d41a0a612be91b1986202.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-17a1454909be47fd85e9013932299846: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-17a1454909be47fd85e9013932299846.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-988a798429a94b499ded8ce4499dd480: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-988a798429a94b499ded8ce4499dd480.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-aa00ddb8cfd54b8b885c7c5f4d8387d9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-aa00ddb8cfd54b8b885c7c5f4d8387d9.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-1e20f72654f64214b3198e26848956e3: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-1e20f72654f64214b3198e26848956e3.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-e9433767e95c44198b7480974c7c172b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-e9433767e95c44198b7480974c7c172b.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-2eb9c2b105bd4fcc936363cf2d8a277f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-2eb9c2b105bd4fcc936363cf2d8a277f.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-80c6e90bf31e49669709ecbeefb3b13a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-80c6e90bf31e49669709ecbeefb3b13a.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-1eaece6209194f24bfcd38ac2fdf6c70: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-1eaece6209194f24bfcd38ac2fdf6c70.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-e2db1ed2e2c74d5d876d0079b2287eac: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-e2db1ed2e2c74d5d876d0079b2287eac.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-eae76e73de114a508fc1aee47cc982e9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-eae76e73de114a508fc1aee47cc982e9.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-b2bdc31b25694f08969fc7490f5550c6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-b2bdc31b25694f08969fc7490f5550c6.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-3ee6de6556e64b439f04c8578c315a01: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-3ee6de6556e64b439f04c8578c315a01.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-000d07c9e37a4a24b886bcdb9dd618ca: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-000d07c9e37a4a24b886bcdb9dd618ca.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-d9d9805b698e482a96261979356edfc9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59484 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-d9d9805b698e482a96261979356edfc9.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-a937970f37d246b08b70e1ed2f34831b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-a937970f37d246b08b70e1ed2f34831b.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-8df92e2b5053401ea94721db66ccb2d7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-8df92e2b5053401ea94721db66ccb2d7.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-df54d347af0b42a8b125672316a622c2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-df54d347af0b42a8b125672316a622c2.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-a77b27ee17eb47b09633c09f209d7ebe: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-a77b27ee17eb47b09633c09f209d7ebe.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-02dbf89d9d094f2fa046e2ac8e407a84: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-02dbf89d9d094f2fa046e2ac8e407a84.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-fdd4e56ea432421dbd61f3e828b40d6a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59524 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-fdd4e56ea432421dbd61f3e828b40d6a.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-c5b1dd8d759d41938831347f09f8d7f1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59536 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-c5b1dd8d759d41938831347f09f8d7f1.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-f113c49f850d4d74b9a1665a10c57352: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-f113c49f850d4d74b9a1665a10c57352.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-53289bc46809468c8f41823de6df122b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-53289bc46809468c8f41823de6df122b.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-659a712b7c9d4d8b99e9c67d1ac862ef: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-5cf704d0f5bb4bedba26aab705b0949b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-cb5ec18a7ef94518a1ca89744af5f3f1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-659a712b7c9d4d8b99e9c67d1ac862ef.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-9e5c56a352b4487cb82c8417be16d004: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-5cf704d0f5bb4bedba26aab705b0949b.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-4bcbbb50c5914267be350a6ae101b3d6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-cb5ec18a7ef94518a1ca89744af5f3f1.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-cfdd52ba31c94b3f9be784610073b9d9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-9e5c56a352b4487cb82c8417be16d004.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59610 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-4bcbbb50c5914267be350a6ae101b3d6.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-cfdd52ba31c94b3f9be784610073b9d9.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-9b595b1f235b46f48c28033abad7edd2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-9b595b1f235b46f48c28033abad7edd2.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [loggers.py:118] Engine 000: Avg prompt throughput: 564.0 tokens/s, Avg generation throughput: 3556.1 tokens/s, Running: 254 reqs, Waiting: 336 reqs, GPU KV cache usage: 81.4%, Prefix cache hit rate: 84.9%
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-48531a7d97934109827035bdd057917a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-48531a7d97934109827035bdd057917a.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-a9edd32a039046d1b1cb54e34b993d03: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-a9edd32a039046d1b1cb54e34b993d03.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-9de0e765d9ab41caa7d38399cbece7b9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-9de0e765d9ab41caa7d38399cbece7b9.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-3cfcfc808d0e4cb9833d401d10e477db: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59666 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-3cfcfc808d0e4cb9833d401d10e477db.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-9772d850e9944797aeeded7f74f4a015: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-9772d850e9944797aeeded7f74f4a015.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-8b514dedccd4468cb1b4e6d5738b4a34: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-8b514dedccd4468cb1b4e6d5738b4a34.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-a4cd1cbcecc54feba94152debc0e0913: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-a4cd1cbcecc54feba94152debc0e0913.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-81d8ceba8e1641c39e61e300063a6f98: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-81d8ceba8e1641c39e61e300063a6f98.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-c7d0ebc2f68e4d9ba02234fa595f44d5: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-c7d0ebc2f68e4d9ba02234fa595f44d5.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-98d262f28ebc4aabb200498b026f5e84: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-98d262f28ebc4aabb200498b026f5e84.
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [logger.py:43] Received request chatcmpl-444f272fac1d4fa68c56e7ef4ddc9512: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:57 [async_llm.py:270] Added request chatcmpl-444f272fac1d4fa68c56e7ef4ddc9512.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-1bfc100d859345b884e3d41595082e45: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-1bfc100d859345b884e3d41595082e45.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-43de5d304ca8489b93d4bdd7f7ba0444: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-6127711062bd451da092aa11efa9c859: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-43de5d304ca8489b93d4bdd7f7ba0444.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-6127711062bd451da092aa11efa9c859.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-29e58f4c84bf46a983603a44627f3f07: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-29e58f4c84bf46a983603a44627f3f07.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-9b7e77a6cc4749999b9bbce5b6e984ef: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-9b7e77a6cc4749999b9bbce5b6e984ef.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-a414ecb6f1c5460eb5ffc5af589f0c22: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-a414ecb6f1c5460eb5ffc5af589f0c22.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-ec488cb9342e48509c42c477b12044fa: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-ec488cb9342e48509c42c477b12044fa.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-038b02aedb4d4d159f1731ffef33a7dd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-038b02aedb4d4d159f1731ffef33a7dd.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-71cab18aaf334fb3959d95ca1a3cf4ac: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-71cab18aaf334fb3959d95ca1a3cf4ac.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-0b8933a4024c49d881230001256c95fe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-0b8933a4024c49d881230001256c95fe.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-822975d6b0d2499bb81be1ba35e1ae25: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-822975d6b0d2499bb81be1ba35e1ae25.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-89bcdb920eb74be3963e356167b25041: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-89bcdb920eb74be3963e356167b25041.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-8c9f3b1c7adb4b909be96a53f8c97a9a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-8c9f3b1c7adb4b909be96a53f8c97a9a.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-6002c3ac48e748ceb64d0ae95c823915: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-6002c3ac48e748ceb64d0ae95c823915.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-21065fc9e06940e4b6a6fd657191e63a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-21065fc9e06940e4b6a6fd657191e63a.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-8cecb43d3fb54f73a4a095cb1b70c1cf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-8cecb43d3fb54f73a4a095cb1b70c1cf.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-f45cae21020e4f45944e03a7fbfca591: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-f45cae21020e4f45944e03a7fbfca591.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-a020e195fb0e41759dfda719a0c4d057: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-4a93161cb36a46fa97c8974771f882be: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-a020e195fb0e41759dfda719a0c4d057.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-4a93161cb36a46fa97c8974771f882be.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-1f837e56856142508c40f98d27578795: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-d6454cf22b744279921d95029fa9f503: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-1f837e56856142508c40f98d27578795.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-d6454cf22b744279921d95029fa9f503.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-c644fbb51af04cc8a04e932f79b3cbb4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-6c3c298c1c444437a7035f155630ce28: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-b5e60f0fe2f54c9e9a69b640946c0638: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-c644fbb51af04cc8a04e932f79b3cbb4.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-6c3c298c1c444437a7035f155630ce28.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-b5e60f0fe2f54c9e9a69b640946c0638.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-8bf2f60a70994cf6b499794b143551b9: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-9183f57645ad4cc1b075e5d472579fa9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-8bf2f60a70994cf6b499794b143551b9.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-9183f57645ad4cc1b075e5d472579fa9.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-b0781308e9e44840940574a7f5a3bbc6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-b0781308e9e44840940574a7f5a3bbc6.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-a23571903da34d80b00d69e6c8435618: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:59998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-a23571903da34d80b00d69e6c8435618.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-bd84b85c0f3543699284fb2d63fa5c55: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-bd84b85c0f3543699284fb2d63fa5c55.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-7e2847c97b27492ebeadaea7570134a3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-7e2847c97b27492ebeadaea7570134a3.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-6551a03bee2546179e8fc573fae74b0f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-6551a03bee2546179e8fc573fae74b0f.
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [logger.py:43] Received request chatcmpl-afb0fc4fc0f34e25884c1313e0074320: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:58 [async_llm.py:270] Added request chatcmpl-afb0fc4fc0f34e25884c1313e0074320.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-9295a8af0c9f4243806c0983bd1bb5f9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-9295a8af0c9f4243806c0983bd1bb5f9.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-377523db4fce4160b7ba508c2f2f2e44: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-377523db4fce4160b7ba508c2f2f2e44.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-2d4ad74c46b44d49b92ff3758bb33ba3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-2d4ad74c46b44d49b92ff3758bb33ba3.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-1322190e1d8b40549daa3da6f4860e71: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60060 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-1322190e1d8b40549daa3da6f4860e71.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-bbb3d85bd28c4ec7bd863ce571ffa9cb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-bbb3d85bd28c4ec7bd863ce571ffa9cb.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-46e150330adc417e863e368bd3cf0397: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-421494da0b01410f9697f35bf5cb540b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-46e150330adc417e863e368bd3cf0397.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60090 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-421494da0b01410f9697f35bf5cb540b.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-1ec77a8a00b14d56ad72c916f7011e45: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60102 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-1ec77a8a00b14d56ad72c916f7011e45.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-ef88899da598428192d4938ea7c7560c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-1a5a324d30574553a402d512944466e6: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-ef88899da598428192d4938ea7c7560c.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-1a5a324d30574553a402d512944466e6.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-9cc784d03b8b41f8bc15c14546bf4978: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-9cc784d03b8b41f8bc15c14546bf4978.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-b15481f8566f4a6ea1c32a718bf5eba0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-b15481f8566f4a6ea1c32a718bf5eba0.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-0f595fd6a6264c9e8bb704985c1cbb8c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-0f595fd6a6264c9e8bb704985c1cbb8c.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-5ef6dfa42ca64856b03fe5929a088844: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-5ef6dfa42ca64856b03fe5929a088844.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-a066ecd8ceb1499486c5c3c98017d3d8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-d63895eadf7a4086a98d161d2113cd47: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-a066ecd8ceb1499486c5c3c98017d3d8.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-d63895eadf7a4086a98d161d2113cd47.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-6ce1d74cc877459bbc72cfaece53360a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-6ce1d74cc877459bbc72cfaece53360a.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-51929e8a11124a69b5511e7520809ceb: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-51929e8a11124a69b5511e7520809ceb.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-e3cf701f77ee484980dc5e10b4d0c63f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-e3cf701f77ee484980dc5e10b4d0c63f.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-6f1fef5ef5194577a42fb257cace11d8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-8741461025bf41cc8efa60441fc15670: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-6f1fef5ef5194577a42fb257cace11d8.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-8741461025bf41cc8efa60441fc15670.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-d780af786c67406b91138a87d66aee09: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-d780af786c67406b91138a87d66aee09.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-21034c5a8ee64745b9bfebbaf462422f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-21034c5a8ee64745b9bfebbaf462422f.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-4187ddeddfb9491fb3e6f212ef6db46d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-4187ddeddfb9491fb3e6f212ef6db46d.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-b68a0315b9fe4aeaa65194da39a70254: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-b68a0315b9fe4aeaa65194da39a70254.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-f3c4bb8184714079a9c6ab0edce27195: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-f3c4bb8184714079a9c6ab0edce27195.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-d260f7fc6cd64ccf868442a06e3ec91b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-d260f7fc6cd64ccf868442a06e3ec91b.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-d3fb65a86f0e46b79ba0a931b89bd8d9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60280 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-d3fb65a86f0e46b79ba0a931b89bd8d9.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-72705a36626e435d9e8081afad006406: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-72705a36626e435d9e8081afad006406.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-70459483e2304212935972be76c2ab82: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-70459483e2304212935972be76c2ab82.
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [logger.py:43] Received request chatcmpl-c304629b87754ef1b9da32e13a21e435: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:04:59 [async_llm.py:270] Added request chatcmpl-c304629b87754ef1b9da32e13a21e435.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-798bcc0f0b334a749f67f41ab1e6bd68: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-798bcc0f0b334a749f67f41ab1e6bd68.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-d94126c8a9db4e8eaeca89e4cc1a9a6c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-d94126c8a9db4e8eaeca89e4cc1a9a6c.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-7793f525b5d84ab3943307fd65bada29: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-7793f525b5d84ab3943307fd65bada29.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-25d60bd5e8974832b03b98e5526cc2d0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-25d60bd5e8974832b03b98e5526cc2d0.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-f5afd397196d457ca63caab60b46fb7e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-f5afd397196d457ca63caab60b46fb7e.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-cf41545715594c5bbadf9ddf9741f274: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-cf41545715594c5bbadf9ddf9741f274.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-04e3279f4eef47feb61975aed11287b6: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-04e3279f4eef47feb61975aed11287b6.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-de26a9712ac94d6685cae4186632bd28: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-de26a9712ac94d6685cae4186632bd28.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-ece8a549077c40968719cd4af7be6274: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-ece8a549077c40968719cd4af7be6274.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-d02180f569804e778e3496f1b918d8f3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-d02180f569804e778e3496f1b918d8f3.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-76fed8d695f54bf890de81aebc0a5fb9: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-76fed8d695f54bf890de81aebc0a5fb9.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-3f1909d747004de4b59788b368a761e1: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-3f1909d747004de4b59788b368a761e1.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-77e07baad3894fe2856ed44886b81a6f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-77e07baad3894fe2856ed44886b81a6f.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-b2f04d23a8784940bf507a45f666369e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-b2f04d23a8784940bf507a45f666369e.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-74ecf72889074909a97e64d8fafdb4a0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-7860bcdaea91418fb312c205c1cad898: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-74ecf72889074909a97e64d8fafdb4a0.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-7860bcdaea91418fb312c205c1cad898.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-9dd7c5b92d8d4c1ebd6eaab40f52a119: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-9dd7c5b92d8d4c1ebd6eaab40f52a119.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-a9c3991a04e24ff5b4e6f1ec30d90f79: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-a9c3991a04e24ff5b4e6f1ec30d90f79.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-91ef06fd8cb34f2cb916d9a228991b88: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60478 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-91ef06fd8cb34f2cb916d9a228991b88.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-67f4636ff67040aab51a2a5d31e07e26: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-67f4636ff67040aab51a2a5d31e07e26.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-2f084bdf07414f4a912cfaa372137192: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-2f084bdf07414f4a912cfaa372137192.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-bddb938aa1714aa29900e44870a865da: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-bddb938aa1714aa29900e44870a865da.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-7db78764c0b14ff5b7a0a23bc6fb7e35: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-7db78764c0b14ff5b7a0a23bc6fb7e35.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-74efd80ecc114706a11968c5d2419a9a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-74efd80ecc114706a11968c5d2419a9a.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-b02340c717ca45c3a06372a76bf01e3e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-b02340c717ca45c3a06372a76bf01e3e.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-9e7da0846cb54ce1aa820ca9ec33b8e9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-9e7da0846cb54ce1aa820ca9ec33b8e9.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-c2f8810dc6c14da6a5b788f88d4c3da5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-c2f8810dc6c14da6a5b788f88d4c3da5.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-473b3c2ef9994ff2aa4e13f00ce1b441: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-473b3c2ef9994ff2aa4e13f00ce1b441.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-922d6904af634876a62dc0aa70010db6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-922d6904af634876a62dc0aa70010db6.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-00b6d942d88b4563a962ce75a8ff9dc5: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-00b6d942d88b4563a962ce75a8ff9dc5.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-b8c86e425bb84598961d9eae74f82a8b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-9202c5a1434a42a4851ed33c5a8617f9: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-b8c86e425bb84598961d9eae74f82a8b.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-9202c5a1434a42a4851ed33c5a8617f9.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-952f134175fb44ff928c2464dd31c742: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-952f134175fb44ff928c2464dd31c742.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-2261641a7d4e4292b840d4be97e3bdac: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-2261641a7d4e4292b840d4be97e3bdac.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-66fcf7e44e2f4097accbaaad09518815: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-66fcf7e44e2f4097accbaaad09518815.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-4443538247ab4c98b3ea52c3cb56db25: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60656 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-4443538247ab4c98b3ea52c3cb56db25.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-160fbcfa95614dee9291ed82b57d7017: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-160fbcfa95614dee9291ed82b57d7017.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-93db0042627b4064992b4a7682b1d90c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-c99f183a7a2a4254833e486d7692508a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-93db0042627b4064992b4a7682b1d90c.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-c99f183a7a2a4254833e486d7692508a.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-f53399257e3d41fcbed4da7e5b47232a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-ae7e183474a2448981e6fde66471998b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [logger.py:43] Received request chatcmpl-e08f1c0c89934077b651ff2431859098: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-f53399257e3d41fcbed4da7e5b47232a.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-ae7e183474a2448981e6fde66471998b.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:00 [async_llm.py:270] Added request chatcmpl-e08f1c0c89934077b651ff2431859098.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-2c1c39e4879346c7aaf7f91029107d22: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-2c1c39e4879346c7aaf7f91029107d22.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-9673c41f26e04302ad8278413ba163ba: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-9673c41f26e04302ad8278413ba163ba.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-305e88f29a4e47d7aa15146bea02ea86: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-305e88f29a4e47d7aa15146bea02ea86.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-a5200586412c4cdab697f364c463649c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-a5200586412c4cdab697f364c463649c.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-1b7a665c6bb34e3da01de8608941bade: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-1b7a665c6bb34e3da01de8608941bade.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-fe054b0c693e42e0b3ef1944fd67e1c6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60778 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-fe054b0c693e42e0b3ef1944fd67e1c6.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-e5a2a3928de84562836fb999c8bcedc4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-e5a2a3928de84562836fb999c8bcedc4.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-984d22260a02496a8e1a6d8d896f681c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-984d22260a02496a8e1a6d8d896f681c.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-97680cedba4e4da8a3fd9a20528abd6e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-97680cedba4e4da8a3fd9a20528abd6e.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-530b4ad1e13b4797a7c177e87f0f7201: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-530b4ad1e13b4797a7c177e87f0f7201.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-aa482004ae644224a16775973b555949: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-aa482004ae644224a16775973b555949.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-927beaf030c9435cbfbbfdec63aaf33a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-927beaf030c9435cbfbbfdec63aaf33a.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-ce57769d98014525b6fedd1aa90de55f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-ce57769d98014525b6fedd1aa90de55f.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-10da24d0e6694857870555bcc7bf8893: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-10da24d0e6694857870555bcc7bf8893.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-c23f7e30ce59485785d3e35a1ea0b1fe: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60868 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-c23f7e30ce59485785d3e35a1ea0b1fe.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-d20b53b0ff404fc0884f476b074ba81d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-d20b53b0ff404fc0884f476b074ba81d.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-bae5de4f342e48368f4ca1e18680fe40: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-bae5de4f342e48368f4ca1e18680fe40.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-74700336c83645e48459ef42507d47fd: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-74700336c83645e48459ef42507d47fd.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-a997ccd78ef54977a2f95c71dc55edf2: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-a997ccd78ef54977a2f95c71dc55edf2.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-083bf3fc61af4712b701f93686128abb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-083bf3fc61af4712b701f93686128abb.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-dd45a6cb7a6e418ab0dd51c7376dfe4c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-dd45a6cb7a6e418ab0dd51c7376dfe4c.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-08aef9e7c9dc4d32bca61dab9a4ffaaf: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-08aef9e7c9dc4d32bca61dab9a4ffaaf.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-7f46b175bd994f45b0017f1946f1ea21: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-7f46b175bd994f45b0017f1946f1ea21.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-b8d24f594b5a40a38b7e4dba1372c2fd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-b8d24f594b5a40a38b7e4dba1372c2fd.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-f0ecf1e04c99418980707961bdaf7ddd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-f0ecf1e04c99418980707961bdaf7ddd.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-16cbf4221d4c4d0c934fcafb8c09834c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60980 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-16cbf4221d4c4d0c934fcafb8c09834c.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-06ee18896a7044b69774296a6b29746c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-06ee18896a7044b69774296a6b29746c.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-bd404cbeeb874bb7a40ecda521bf989a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-bd404cbeeb874bb7a40ecda521bf989a.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-7fedb153853048a9a39dcbada1949678: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-7fedb153853048a9a39dcbada1949678.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-80a54855ebef4e8797673dbf065864e0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:60998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-80a54855ebef4e8797673dbf065864e0.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-2df122dced5445f883b98e6a6a32b286: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-2df122dced5445f883b98e6a6a32b286.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-c167f5a7d30649488c8f1a5710c60c57: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-c167f5a7d30649488c8f1a5710c60c57.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-1877e9f33b0d4ef58892b432f0356ac8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-1877e9f33b0d4ef58892b432f0356ac8.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-6895b481179043b28923b2c52b56fb55: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-6895b481179043b28923b2c52b56fb55.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-bc70e9ac484c4f3c86b6f82d617111d5: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-bc70e9ac484c4f3c86b6f82d617111d5.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-5f60d93b436d4872b754b968def2608d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-5f60d93b436d4872b754b968def2608d.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-abbf47393ae04d25a24d6cc16dfc1d5c: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-abbf47393ae04d25a24d6cc16dfc1d5c.
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [logger.py:43] Received request chatcmpl-f05503b7a642457bb441fe789eaf927f: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:01 [async_llm.py:270] Added request chatcmpl-f05503b7a642457bb441fe789eaf927f.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-4d60ea68ffab4af9b222b896eeba2e3e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-4d60ea68ffab4af9b222b896eeba2e3e.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-1e4a4685a22f42468a63d80f4ee6cafd: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-1e4a4685a22f42468a63d80f4ee6cafd.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-e07e89e2ca294b2e92cbd006eb1857b4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-e07e89e2ca294b2e92cbd006eb1857b4.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-029e54c0c1af4a418453cf5f833cab1d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-029e54c0c1af4a418453cf5f833cab1d.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-babde1d0dab9486ab02a3db037cdd886: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-babde1d0dab9486ab02a3db037cdd886.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-69d0db5ef55346cea70a0472e971979c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-69d0db5ef55346cea70a0472e971979c.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-4ac88284f7854b9d9b7cb139a4258025: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-4ac88284f7854b9d9b7cb139a4258025.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-ec5ff6bcac4f4236b0ea008f934eb4e1: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-ec5ff6bcac4f4236b0ea008f934eb4e1.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-60c03671d8404cd197c78537e06a249e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-60c03671d8404cd197c78537e06a249e.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-f7a332c7e2d749b0b8332785415c8c1d: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-f7a332c7e2d749b0b8332785415c8c1d.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-f70d583b1c7748e89468ab9cc209340f: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-f70d583b1c7748e89468ab9cc209340f.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-63ab8379aa9949af97e5fadf871d3359: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-2109fce3baf84dcf9e3f3f99ca0ebda0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-63ab8379aa9949af97e5fadf871d3359.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-2109fce3baf84dcf9e3f3f99ca0ebda0.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-beeb79c9b40a4f17a16ff8682a1eb23b: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-beeb79c9b40a4f17a16ff8682a1eb23b.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-5e2c751e741f468c99ea7a9ba2090d7b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:32998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-5e2c751e741f468c99ea7a9ba2090d7b.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-5a9927b3554a44c3906376aadc4412b8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-5a9927b3554a44c3906376aadc4412b8.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-3e424a13af844444b6e49a2d29110a47: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-3e424a13af844444b6e49a2d29110a47.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-80bc5c93629149cead7ceb91c7b872eb: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-80bc5c93629149cead7ceb91c7b872eb.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-a404028131fc48bc9a0027fd06bd7672: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-a404028131fc48bc9a0027fd06bd7672.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-872ccbeac2094081b1753a1ffe681384: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-872ccbeac2094081b1753a1ffe681384.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-5bd7842af69a4064a8ec6a0fe64eeaf3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-5bd7842af69a4064a8ec6a0fe64eeaf3.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-6f3d2817a4fb40b19329205e2a784b12: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-6f3d2817a4fb40b19329205e2a784b12.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-39d7d681729b4fa3bba69085e972392b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-39d7d681729b4fa3bba69085e972392b.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-ca2dc1a974dc4e2eb9231a1da2b51675: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-ca2dc1a974dc4e2eb9231a1da2b51675.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-7fb30e943e78459d857ecffec1fd0a4d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-7fb30e943e78459d857ecffec1fd0a4d.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-9f17c807cc384086b2743ce4de5e6840: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33088 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-9f17c807cc384086b2743ce4de5e6840.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-3e1a5a3a594f4f3e8d8af01cab76d28a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-3e1a5a3a594f4f3e8d8af01cab76d28a.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-8a726bf01d094eb6a12a38b9a86eae04: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33108 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-8a726bf01d094eb6a12a38b9a86eae04.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-ec660959fe89459788cf6f21d530ea24: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-ec660959fe89459788cf6f21d530ea24.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-6929ab70117f417c983a58c9b69ed214: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-6929ab70117f417c983a58c9b69ed214.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-5f053381c96f4344931d5d26272ed228: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-5f053381c96f4344931d5d26272ed228.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-5228ad216ef84ea5861708c1d6d5033a: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-5228ad216ef84ea5861708c1d6d5033a.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-475cdf3ebe974b4da286149b9d9c04a8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:33160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-475cdf3ebe974b4da286149b9d9c04a8.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-e76ecbfcc0df42b39eb058746cf053ed: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-e76ecbfcc0df42b39eb058746cf053ed.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-2ec2da28ad9847eb8ba4a7a613ac1c8a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-2ec2da28ad9847eb8ba4a7a613ac1c8a.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-5e0a54b4d2f74770b75f9a693188e283: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-5e0a54b4d2f74770b75f9a693188e283.
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [logger.py:43] Received request chatcmpl-3a29d57c104443bd9ddfe10894c94a3a: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:02 [async_llm.py:270] Added request chatcmpl-3a29d57c104443bd9ddfe10894c94a3a.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-1143be76624a4de59c6ca3690a9f6d40: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-1143be76624a4de59c6ca3690a9f6d40.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-1a9c2f972e6848f5b7eff8d4684d2171: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-1a9c2f972e6848f5b7eff8d4684d2171.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-2a568815c03d4986bea0ab4aac329c4b: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-2a568815c03d4986bea0ab4aac329c4b.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-fb4454e85dd64e00b6ba67124bc195c8: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-fb4454e85dd64e00b6ba67124bc195c8.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-0f5dc7b582754b01bc75dd4248f52be2: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-0f5dc7b582754b01bc75dd4248f52be2.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-2f4ace4628f44c258323c1e3f612174c: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-2f4ace4628f44c258323c1e3f612174c.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-580991c04dfc41b5b3e46a952c6944ae: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57888 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-580991c04dfc41b5b3e46a952c6944ae.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-27772bdb1ba2424db4ec5b07a1d077cf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-27772bdb1ba2424db4ec5b07a1d077cf.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-c51415021d0c4802b664aee71b918e44: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-c51415021d0c4802b664aee71b918e44.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-ed659d4ed88d47ac90ae1813975be3ff: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-ed659d4ed88d47ac90ae1813975be3ff.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-2baf346a7ebb4cd6a66bc110117a2b5f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-2baf346a7ebb4cd6a66bc110117a2b5f.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-cf217eec7e714a29a5d0b18d68c259d0: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-cf217eec7e714a29a5d0b18d68c259d0.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-219063f222b749d7934466755918d153: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-219063f222b749d7934466755918d153.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-a3adedb6dfef4fb7b9fc016d1cf65696: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-a3adedb6dfef4fb7b9fc016d1cf65696.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-105e44e0b9c1458293d81ef9cae7712e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-105e44e0b9c1458293d81ef9cae7712e.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-2ed8a3cf954d42c897956934d414cec4: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-2ed8a3cf954d42c897956934d414cec4.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-dde099580bce420aa4115979b7de8de6: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:57990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-dde099580bce420aa4115979b7de8de6.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-bc13ae5bc60443d6a5346b2d7a14e85e: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-bc13ae5bc60443d6a5346b2d7a14e85e.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-434ae6729644473da2562c44cb28a409: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58016 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-434ae6729644473da2562c44cb28a409.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-cb2bdcc516224c34b4977832c5fc86b4: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-cb2bdcc516224c34b4977832c5fc86b4.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-0f866c05595948119f6a82289ae447b4: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-0f866c05595948119f6a82289ae447b4.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-2e118ca7cf6541c2a421ab399ff457f0: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-2e118ca7cf6541c2a421ab399ff457f0.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-bed252e9dd88476caacf07af85736a2a: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-bed252e9dd88476caacf07af85736a2a.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-35d50eb99e5842628961eb7953e27596: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-35d50eb99e5842628961eb7953e27596.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-60e86322809c4507b5819f2e1d27b0c8: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-60e86322809c4507b5819f2e1d27b0c8.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-135ab27f27b54594889b1e743c111d6d: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-135ab27f27b54594889b1e743c111d6d.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-e4beb2522b894b82ad15c3cc8496e542: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58074 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-e4beb2522b894b82ad15c3cc8496e542.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-0cedfa48d60840ffbd3e97ea7d6ab304: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-0cedfa48d60840ffbd3e97ea7d6ab304.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-be7bedcfb02f4bffafcf57b605d11bb6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-be7bedcfb02f4bffafcf57b605d11bb6.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-0bc7c1cc227645f7bf3807af8cadbb25: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-0bc7c1cc227645f7bf3807af8cadbb25.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-24ef96dab0b4443cbb110e2f55a2096f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-24ef96dab0b4443cbb110e2f55a2096f.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-1f2a74215f3f459396a072b66ee32404: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-1f2a74215f3f459396a072b66ee32404.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-df4a9d9cc9a949a8a67785ec99bbdcad: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-df4a9d9cc9a949a8a67785ec99bbdcad.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-3904409c0c8440b8bfe4d8f0111677b0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-ec530133814f40e9a02373c44c3e396e: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-3904409c0c8440b8bfe4d8f0111677b0.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-ec530133814f40e9a02373c44c3e396e.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-4b144c2e29b44d7ca45845c55d422e1e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-4b144c2e29b44d7ca45845c55d422e1e.
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [logger.py:43] Received request chatcmpl-ffb4dc435acd4e0d9d574114c309aae8: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:03 [async_llm.py:270] Added request chatcmpl-ffb4dc435acd4e0d9d574114c309aae8.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-47ce2ec862f5497aa5e7d7852a18ec1b: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-47ce2ec862f5497aa5e7d7852a18ec1b.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-7550ef9b42a744f386ea12b6bbd64441: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-7550ef9b42a744f386ea12b6bbd64441.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-d5defe87dc154a3b8d16b4e620ac9ec0: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-d5defe87dc154a3b8d16b4e620ac9ec0.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-4b36941901bd4118a3aa17d0e052f6b3: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-4b36941901bd4118a3aa17d0e052f6b3.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-26e54c0a25bb4a319d2951145adcf084: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-26e54c0a25bb4a319d2951145adcf084.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-7f216225b2764825806f3f068f991f62: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-7f216225b2764825806f3f068f991f62.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-f6f024c0ac904ce7bb080621b3fe3452: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-f6f024c0ac904ce7bb080621b3fe3452.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-ef277838e0e346a7ba5ea55d60d4c9c8: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-ef277838e0e346a7ba5ea55d60d4c9c8.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-ea531476dae046219d05fafe14fadcf3: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-ea531476dae046219d05fafe14fadcf3.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-121c12c192e5423c975c644eb5d9cc5e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-121c12c192e5423c975c644eb5d9cc5e.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-d2902169aa7e4d40b6c8f19d95bcca6e: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-d2902169aa7e4d40b6c8f19d95bcca6e.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-1157c04bcb8d495d871e4f42658c0eda: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-1157c04bcb8d495d871e4f42658c0eda.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-3465ce5917e8403bb04ceec660f3055d: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-3465ce5917e8403bb04ceec660f3055d.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-9b6cf822ffa14b5d8c80fa366b4270d6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-9b6cf822ffa14b5d8c80fa366b4270d6.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-c400e0cbbcc745dbbcf13ba25336d151: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-2e65db3cbb3045a893f7f78c45aaa346: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-c400e0cbbcc745dbbcf13ba25336d151.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-2e65db3cbb3045a893f7f78c45aaa346.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-e05454249d7047f2bb562c9c94534fb0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-e05454249d7047f2bb562c9c94534fb0.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-54b0559c07a44819b4ba41dab0aea61d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-54b0559c07a44819b4ba41dab0aea61d.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-e927ce53e6234e4aa2d5af77360a23d0: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-e927ce53e6234e4aa2d5af77360a23d0.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-776548d33e134cb184bd800eadbc2152: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-776548d33e134cb184bd800eadbc2152.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-2a84710878924af4b1136227bb7bf13f: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-2a84710878924af4b1136227bb7bf13f.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-3661bf1e5a5b4abfa80333791641d20d: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-3661bf1e5a5b4abfa80333791641d20d.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-77d2c115848242389317f6ce2b68cfc7: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-77d2c115848242389317f6ce2b68cfc7.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-2e6312afa2d14a458a712908c2d0def7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-2e6312afa2d14a458a712908c2d0def7.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-b7541ce3c250411a9a9deeb5be022bcc: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-b7541ce3c250411a9a9deeb5be022bcc.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-c9d4128f152f4cd7b282af48e1bd6097: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-c9d4128f152f4cd7b282af48e1bd6097.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-b593d2ed0828449aaa768e1c3e1291ba: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-b593d2ed0828449aaa768e1c3e1291ba.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-228f894bca944381b060e670cfdf4bb9: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-228f894bca944381b060e670cfdf4bb9.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-23acafbd26474ef6aef05550abddb2bb: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-23acafbd26474ef6aef05550abddb2bb.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-3863ecb3d20e4c688779870ced7b7837: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-3863ecb3d20e4c688779870ced7b7837.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-b32e3fc87557499f9fd7a25dbb71afb1: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58478 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-b32e3fc87557499f9fd7a25dbb71afb1.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-5fd9a1050bf740369d4ccc8b83984f19: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-5fd9a1050bf740369d4ccc8b83984f19.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-56e61247a680433f89fbb6f91e9b8140: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-56e61247a680433f89fbb6f91e9b8140.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-76b2ea2a5fff41f89c3bf219f6431060: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-76b2ea2a5fff41f89c3bf219f6431060.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-f27304c6aaeb40d19666ad326cebc280: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-f27304c6aaeb40d19666ad326cebc280.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-4b254c0bdb244d36bc65139127f0f2d2: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-4b254c0bdb244d36bc65139127f0f2d2.
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [logger.py:43] Received request chatcmpl-02bde4724f5849fb8d35330b37a54270: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58536 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:04 [async_llm.py:270] Added request chatcmpl-02bde4724f5849fb8d35330b37a54270.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-b4745adb165c4a56b37d0be391fdcaa3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-b4745adb165c4a56b37d0be391fdcaa3.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-2ff41930909b47d19d3bf11c7a69c654: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-2ff41930909b47d19d3bf11c7a69c654.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-048681d1b8204dfa80a352ed98df1aea: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-048681d1b8204dfa80a352ed98df1aea.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-d6c87707ee0446648252ca2fd210e444: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-d6c87707ee0446648252ca2fd210e444.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-f7bfde9c5dd04b67a8ed280ec5365c8a: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-f7bfde9c5dd04b67a8ed280ec5365c8a.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-7fa8eba6b8a1418a8e538c81c5d0a597: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-7fa8eba6b8a1418a8e538c81c5d0a597.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-e61ee12346c746879a1f52cae9e700e3: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-e61ee12346c746879a1f52cae9e700e3.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-05e498f92374444a834ffe959c39ccbf: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-05e498f92374444a834ffe959c39ccbf.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-8db507d168a944fabc946898306474fc: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-8db507d168a944fabc946898306474fc.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-91ab78fe9c104991b91e3569d6f78411: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-91ab78fe9c104991b91e3569d6f78411.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-ee98d2caf13d4d50aef3ea7a7c136b98: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-ee98d2caf13d4d50aef3ea7a7c136b98.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-382be8e658974967b2fdbc48e84049b6: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-382be8e658974967b2fdbc48e84049b6.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-db75e5d8afea480c819823c18eb4d756: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-db75e5d8afea480c819823c18eb4d756.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-5de1c0c5db6943a7b470ffff1f84c855: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-5de1c0c5db6943a7b470ffff1f84c855.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-329eb6b9750642f3ada98582c47ba432: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-329eb6b9750642f3ada98582c47ba432.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-1516509dd861462b97f308c852377512: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-1516509dd861462b97f308c852377512.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-944d89b379244c5b8e57e178630f3c84: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-944d89b379244c5b8e57e178630f3c84.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-ba2c30a77b0040ebb21ca9e73509af17: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-ba2c30a77b0040ebb21ca9e73509af17.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-d057640b37b045e5b7ad1feed59bb77c: prompt: "<|im_start|>user\n<|im_start|>user\nCan you provide a detailed explanation of how a transformer model works, including the roles of self-attention, multi-head attention, and positional encodings? Please also include a brief history of the model's development.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-d057640b37b045e5b7ad1feed59bb77c.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-872c453cce48416d88b3323b6832d67b: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-872c453cce48416d88b3323b6832d67b.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-107478606b8c4ca2b9dc42c80f8f5280: prompt: '<|im_start|>user\n<|im_start|>user\nDescribe the process of photosynthesis in a way that a high school student could understand. Include the chemical equation and explain the significance of the light-dependent and light-independent reactions.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-107478606b8c4ca2b9dc42c80f8f5280.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-4bf4b187015a410abe4ec6938d08596c: prompt: "<|im_start|>user\n<|im_start|>user\nGenerate a comprehensive investment plan for a young professional with a moderate risk tolerance. The plan should include a mix of stocks, bonds, and real estate, and should take into account the individual's long-term financial goals.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-4bf4b187015a410abe4ec6938d08596c.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-1cbcdf7bb9d542959a515801b14a9de7: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-1cbcdf7bb9d542959a515801b14a9de7.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-55d034aa2eda4e5bbe4aa37156208a6f: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-55d034aa2eda4e5bbe4aa37156208a6f.
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [logger.py:43] Received request chatcmpl-330b282ed7a04c4fb7bfa918ee6c6a8e: prompt: '<|im_start|>user\n<|im_start|>user\nWrite a short story in the style of Edgar Allan Poe about a man who is haunted by a mysterious sound.<|im_end|>\n<|im_start|>assistant\n<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
[36mllm_server_1  |[0m INFO:     172.28.0.1:58806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[36mllm_server_1  |[0m INFO 07-20 23:05:05 [async_llm.py:270] Added request chatcmpl-330b282ed7a04c4fb7bfa918ee6c6a8e.
[36mllm_server_1  |[0m INFO 07-20 23:05:07 [loggers.py:118] Engine 000: Avg prompt throughput: 749.0 tokens/s, Avg generation throughput: 3660.4 tokens/s, Running: 256 reqs, Waiting: 488 reqs, GPU KV cache usage: 81.0%, Prefix cache hit rate: 85.1%
[36mllm_server_1  |[0m INFO 07-20 23:05:17 [loggers.py:118] Engine 000: Avg prompt throughput: 706.6 tokens/s, Avg generation throughput: 3319.6 tokens/s, Running: 253 reqs, Waiting: 350 reqs, GPU KV cache usage: 74.9%, Prefix cache hit rate: 85.2%
[36mllm_server_1  |[0m INFO 07-20 23:05:27 [loggers.py:118] Engine 000: Avg prompt throughput: 593.0 tokens/s, Avg generation throughput: 3887.0 tokens/s, Running: 256 reqs, Waiting: 232 reqs, GPU KV cache usage: 94.8%, Prefix cache hit rate: 85.3%
[36mllm_server_1  |[0m INFO 07-20 23:05:37 [loggers.py:118] Engine 000: Avg prompt throughput: 828.3 tokens/s, Avg generation throughput: 3095.3 tokens/s, Running: 255 reqs, Waiting: 68 reqs, GPU KV cache usage: 71.1%, Prefix cache hit rate: 85.5%
[36mllm_server_1  |[0m INFO 07-20 23:05:47 [loggers.py:118] Engine 000: Avg prompt throughput: 354.0 tokens/s, Avg generation throughput: 3886.8 tokens/s, Running: 206 reqs, Waiting: 0 reqs, GPU KV cache usage: 88.1%, Prefix cache hit rate: 85.6%
